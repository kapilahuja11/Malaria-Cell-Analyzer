<!DOCTYPE html>
<!-- saved from url=(0077)https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style id="react-tooltip">.__react_component_tooltip{border-radius:3px;display:inline-block;font-size:13px;left:-999em;opacity:0;padding:8px 21px;position:fixed;pointer-events:none;transition:opacity 0.3s ease-out;top:-999em;visibility:hidden;z-index:999}.__react_component_tooltip.allow_hover,.__react_component_tooltip.allow_click{pointer-events:auto}.__react_component_tooltip:before,.__react_component_tooltip:after{content:"";width:0;height:0;position:absolute}.__react_component_tooltip.show{opacity:0.9;margin-top:0px;margin-left:0px;visibility:visible}.__react_component_tooltip.type-dark{color:#fff;background-color:#222}.__react_component_tooltip.type-dark.place-top:after{border-top-color:#222;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-dark.place-bottom:after{border-bottom-color:#222;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-dark.place-left:after{border-left-color:#222;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-dark.place-right:after{border-right-color:#222;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-dark.border{border:1px solid #fff}.__react_component_tooltip.type-dark.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-dark.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-dark.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-dark.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-success{color:#fff;background-color:#8DC572}.__react_component_tooltip.type-success.place-top:after{border-top-color:#8DC572;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-success.place-bottom:after{border-bottom-color:#8DC572;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-success.place-left:after{border-left-color:#8DC572;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-success.place-right:after{border-right-color:#8DC572;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-success.border{border:1px solid #fff}.__react_component_tooltip.type-success.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-success.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-success.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-success.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-warning{color:#fff;background-color:#F0AD4E}.__react_component_tooltip.type-warning.place-top:after{border-top-color:#F0AD4E;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-warning.place-bottom:after{border-bottom-color:#F0AD4E;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-warning.place-left:after{border-left-color:#F0AD4E;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-warning.place-right:after{border-right-color:#F0AD4E;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-warning.border{border:1px solid #fff}.__react_component_tooltip.type-warning.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-warning.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-warning.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-warning.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-error{color:#fff;background-color:#BE6464}.__react_component_tooltip.type-error.place-top:after{border-top-color:#BE6464;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-error.place-bottom:after{border-bottom-color:#BE6464;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-error.place-left:after{border-left-color:#BE6464;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-error.place-right:after{border-right-color:#BE6464;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-error.border{border:1px solid #fff}.__react_component_tooltip.type-error.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-error.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-error.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-error.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-info{color:#fff;background-color:#337AB7}.__react_component_tooltip.type-info.place-top:after{border-top-color:#337AB7;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-info.place-bottom:after{border-bottom-color:#337AB7;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-info.place-left:after{border-left-color:#337AB7;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-info.place-right:after{border-right-color:#337AB7;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-info.border{border:1px solid #fff}.__react_component_tooltip.type-info.border.place-top:before{border-top:8px solid #fff}.__react_component_tooltip.type-info.border.place-bottom:before{border-bottom:8px solid #fff}.__react_component_tooltip.type-info.border.place-left:before{border-left:8px solid #fff}.__react_component_tooltip.type-info.border.place-right:before{border-right:8px solid #fff}.__react_component_tooltip.type-light{color:#222;background-color:#fff}.__react_component_tooltip.type-light.place-top:after{border-top-color:#fff;border-top-style:solid;border-top-width:6px}.__react_component_tooltip.type-light.place-bottom:after{border-bottom-color:#fff;border-bottom-style:solid;border-bottom-width:6px}.__react_component_tooltip.type-light.place-left:after{border-left-color:#fff;border-left-style:solid;border-left-width:6px}.__react_component_tooltip.type-light.place-right:after{border-right-color:#fff;border-right-style:solid;border-right-width:6px}.__react_component_tooltip.type-light.border{border:1px solid #222}.__react_component_tooltip.type-light.border.place-top:before{border-top:8px solid #222}.__react_component_tooltip.type-light.border.place-bottom:before{border-bottom:8px solid #222}.__react_component_tooltip.type-light.border.place-left:before{border-left:8px solid #222}.__react_component_tooltip.type-light.border.place-right:before{border-right:8px solid #222}.__react_component_tooltip.place-top{margin-top:-10px}.__react_component_tooltip.place-top:before{border-left:10px solid transparent;border-right:10px solid transparent;bottom:-8px;left:50%;margin-left:-10px}.__react_component_tooltip.place-top:after{border-left:8px solid transparent;border-right:8px solid transparent;bottom:-6px;left:50%;margin-left:-8px}.__react_component_tooltip.place-bottom{margin-top:10px}.__react_component_tooltip.place-bottom:before{border-left:10px solid transparent;border-right:10px solid transparent;top:-8px;left:50%;margin-left:-10px}.__react_component_tooltip.place-bottom:after{border-left:8px solid transparent;border-right:8px solid transparent;top:-6px;left:50%;margin-left:-8px}.__react_component_tooltip.place-left{margin-left:-10px}.__react_component_tooltip.place-left:before{border-top:6px solid transparent;border-bottom:6px solid transparent;right:-8px;top:50%;margin-top:-5px}.__react_component_tooltip.place-left:after{border-top:5px solid transparent;border-bottom:5px solid transparent;right:-6px;top:50%;margin-top:-4px}.__react_component_tooltip.place-right{margin-left:10px}.__react_component_tooltip.place-right:before{border-top:6px solid transparent;border-bottom:6px solid transparent;left:-8px;top:50%;margin-top:-5px}.__react_component_tooltip.place-right:after{border-top:5px solid transparent;border-bottom:5px solid transparent;left:-6px;top:50%;margin-top:-4px}.__react_component_tooltip .multi-line{display:block;padding:2px 0px;text-align:center}</style>
    <title>Malaria Cell Analyzer + Tensorflow.js Web App | Kaggle</title>
    
    <meta name="robots" content="index, follow">
    <meta name="description" content="Explore and run machine learning code with Kaggle Notebooks | Using data from Malaria Cell Images Dataset">
    <meta name="turbolinks-cache-control" content="no-cache">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">
    <meta name="theme-color" content="#008ABC">
    <script src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/cb=gapi.loaded_0" nonce="" async=""></script><script type="text/javascript" async="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/js" nonce=""></script><script type="text/javascript" async="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/analytics.js" nonce=""></script><script nonce="" type="text/javascript">
        if ('serviceWorker' in navigator) {
            navigator.serviceWorker.getRegistrations()
                .then(function(registrations) {
                    for (let registration of registrations) {
                        registration.unregister();
                    }
                })
                .catch(function(err) {
                    console.error("Service worker unregister failed: ", err);
                });
        }
    </script>
    <script nonce="" type="text/javascript">
        window["pageRequestStartTime"] = 1601042753189;
        window["pageRequestEndTime"] = 1601042753545;
        window["initialPageLoadStartTime"] = new Date().getTime();
    </script>
    <link rel="preconnect" href="https://www.google-analytics.com/" crossorigin="anonymous"><link rel="preconnect" href="https://stats.g.doubleclick.net/"><link rel="preconnect" href="https://storage.googleapis.com/"><link rel="preconnect" href="https://apis.google.com/">
    <link href="https://www.kaggle.com/static/images/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link rel="manifest" href="https://www.kaggle.com/static/json/manifest.json" crossorigin="use-credentials">
    <link href="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/css" rel="stylesheet" type="text/css">
    <link href="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/icon" rel="stylesheet" type="text/css">
        <link rel="canonical" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app">
        <link rel="stylesheet" type="text/css" href="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/vendor.css">
        <link rel="stylesheet" type="text/css" href="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/app.css">
    
    
 
    
    <script nonce="">
        try{(function(a,s,y,n,c,h,i,d,e){d=s.createElement("style");
        d.appendChild(s.createTextNode(""));s.head.appendChild(d);d=d.sheet;
        y=y.map(x => d.insertRule(x + "{ opacity: 0 !important }"));
        h.start=1*new Date;h.end=i=function(){y.forEach(x => x<d.cssRules.length ? d.deleteRule(x) : {})};
        (a[n]=a[n]||[]).hide=h;setTimeout(function(){i();h.end=null},c);h.timeout=c;
        })(window,document,['.site-header-react__nav'],'dataLayer',2000,{'GTM-52LNT9S':true});}catch(ex){}
    </script><style></style>
    <script nonce="">
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-12629138-1', {
            'optimize_id': 'GTM-52LNT9S',
            'displayFeaturesTask': null,
            'send_page_view': false,
            'content_group1': 'Notebooks'
        });
    </script>
    <script nonce="" async="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/js(1)"></script>

    
    
    <meta name="og:url" content="https://kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app">
    <meta property="og:title" content="Malaria Cell Analyzer + Tensorflow.js Web App">
    <meta property="og:description" content="Explore and run machine learning code with Kaggle Notebooks | Using data from Malaria Cell Images Dataset">
    <meta property="og:type" content="website">
    <meta name="og:image" content="https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@kaggledatasets">


    
    
    <script nonce="" type="application/ld+json">{"@context":"http://schema.org/","@type":"Article","author":{"@type":"Person","name":"vbookshelf","sameAs":"/vbookshelf"},"headline":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app","dateModified":"2020-08-17T07:50:16.94","datePublished":"2020-08-17T07:50:16.94","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app"},"publisher":{"@type":"Organization","name":"Kaggle","sameAs":"https://www.kaggle.com","logo":{"type":"ImageObject","url":"https://www.kaggle.com/static/images/site-logo.png"}},"image":["https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg"]}</script>


    
    
    
<script nonce="" type="text/javascript">
    var Kaggle = window.Kaggle || {};

    Kaggle.Current = {
        antiForgeryToken: 'CfDJ8LdUzqlsSWBPr4Ce3rb9VL-B4q8D_q6fu9C_LYNMSW1bYgcEPrerIZa4lO4t6ZewXZ6_6Y3JCWdOo58BU5o7ekgX8aozyw_ETIpMvQtGqS-AsO-HnMiRqC8t4Y8_p18JokTI9qczXXCFnLPvb9aCpfg',
        isAnonymous: true,
        analyticsToken: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MDEwNDM2NTMsIlVzZXJJZCI6MH0.QemkY9jqbyK6orBOMES2IHoc6ek9tv1NGiZPbM6gKG8',
        analyticsTokenExpiry: 15,
        
        
        
        
        
        
        mdeImageUploader: true,
        
        enableRapidash: true, 
    }
        Kaggle.Current.log = function(){};
        Kaggle.Current.warn = function(){};

    var decodeUserDisplayName = function () {
        var escapedUserDisplayName = Kaggle.Current.userDisplayNameEscaped || "";
        try {
            var textVersion = new DOMParser().parseFromString(escapedUserDisplayName, "text/html").documentElement.textContent;
            if (textVersion) {
                return textVersion;
            }
        } catch(ex) {}
        return escapedUserDisplayName;
    }
    Kaggle.Current.userDisplayName = decodeUserDisplayName();
</script>

    

<script nonce="" type="text/javascript">
    var Kaggle = window.Kaggle || {};
    Kaggle.PageMessages = [];
</script>

        <script nonce="" type="text/javascript">
/* <![CDATA[ */
goog_snippet_vars = function() {
    var w = window;
    w.google_conversion_id = 955616553;
    w.google_conversion_label = "QSjvCKDksHMQqZrWxwM";
    w.google_conversion_value = 0.00;
    w.google_conversion_currency = "USD";
    w.google_remarketing_only = false;
    w.google_conversion_language = "en";
    w.google_conversion_format = "3";
    w.google_conversion_color = "ffffff";
}
// DO NOT CHANGE THE CODE BELOW.
goog_report_conversion = function(url) {
    goog_snippet_vars();
    window.google_conversion_format = "3";
    var opt = new Object();
    opt.onload_callback = function() {
        if (typeof(url) != 'undefined') {
            window.location = url;
        }
    }
    var conv_handler = window['google_trackConversion'];
    if (typeof(conv_handler) == 'function') {
        conv_handler(opt);
    }
}
/* ]]> */
    </script>
    <script nonce="" type="text/javascript" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/f.txt">
    </script>



        <script nonce="">window['useKaggleAnalytics'] = true;</script>

    <script id="gapi-target" nonce="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/api.js" defer="" async="" gapi_processed="true"></script>
    <script nonce="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/runtime.js" data-turbolinks-track="reload"></script>
    <script nonce="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/vendor.js" data-turbolinks-track="reload"></script>
    <script nonce="" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/app.js" data-turbolinks-track="reload"></script>
        <script nonce="" type="text/javascript">
            window.kaggleStackdriverConfig = {
                key: 'AIzaSyDANGXFHtSIVc51MIdGwg4mQFgm3oNrKoo',
                projectId: 'kaggle-161607',
                service: 'web-fe',
                version: 'ci',
                context: {
                    user: '0',
                },
            };
        </script>
<style data-styled="active" data-styled-version="5.1.0"></style><script charset="utf-8" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/1.dd5a306d90619456bea0.js"></script><script charset="utf-8" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/3.00f0628c5c47cbc2ac42.js"></script><script charset="utf-8" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/4.e53ce4017abda33554bb.js"></script><script charset="utf-8" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/15.dba91dfc95430d157c11.js"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.react-json-view .copy-to-clipboard-container{vertical-align:top;display:none}.react-json-view .click-to-add,.react-json-view .click-to-edit,.react-json-view .click-to-remove{display:none}.react-json-view .object-content .variable-row:hover .click-to-edit,.react-json-view .object-content .variable-row:hover .click-to-remove,.react-json-view .object-key-val:hover>span>.object-meta-data>.click-to-add,.react-json-view .object-key-val:hover>span>.object-meta-data>.click-to-remove,.react-json-view .object-key-val:hover>span>.object-meta-data>.copy-to-clipboard-container,.react-json-view .variable-row:hover .copy-to-clipboard-container{display:inline-block}</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>
<body data-turbolinks="false"><div id="MathJax_Message" style="display: none;"></div>
    <main>
        






<div id="site-container"><div style="--mdc-theme-on-primary:#fff; --mdc-theme-on-surface:rgba(0, 0, 0, 0.87); --mdc-theme-text-primary-on-background:rgba(0, 0, 0, 0.87); --mdc-theme-text-secondary-on-background:rgba(0, 0, 0, 0.54); --mdc-theme-text-hint-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-disabled-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-text-icon-on-background:rgba(0, 0, 0, 0.38); --mdc-theme-primary:#20BEFF; --mdc-theme-error:#F58B8A; --mdc-theme-background:#F8F8F8; --mdc-theme-surface:#fff; --mdc-theme-primary-bg:#20BEFF; --mdc-theme-secondary-bg:#919294;"><div class="sc-qQKBW fHcRgw"><div class="sc-qYGzz hZIRis"><div class="sc-pkfsj eWAAem"><div class="sc-psAte gMVDjT"><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx google-material-icons">menu</button></div><div class="sc-qQAox boezIO"><a href="https://www.kaggle.com/"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/site-logo.png" class="sc-pbKro bjcxym"></a></div></div></div><div class="sc-pBAKv bbJvDc"><div class="sc-pJVLq bIdQJv"><div class="sc-pbvBv jsONvj"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pjuKs jdzzxM">search</i><p class="sc-fzqARJ sc-fzqNqU sc-prqHV dloUTY">Search</p></div></div><div class="sc-pRRIT cPqVsD"><div class="sc-qamJO bjTpYM"><a href="https://www.kaggle.com/account/login?phase=startSignInTab&amp;returnUrl=%2Fvbookshelf%2Fmalaria-cell-analyzer-tensorflow-js-web-app" class="sc-oTpqt bnxBWg"><button class="sc-fzpkqZ jjUXyG" type="submit"><span class="sc-fzoxKX sc-fzoKki cyTXUp">Sign In</span></button></a></div><div class="sc-qamJO bjTpYM"><a href="https://www.kaggle.com/account/login?phase=startRegisterTab&amp;returnUrl=%2Fvbookshelf%2Fmalaria-cell-analyzer-tensorflow-js-web-app" class="sc-oTpqt bnxBWg"><button class="sc-fzpkqZ jvlJco" type="submit"><span class="sc-fzoxKX sc-fzoKki cyTXUp">Register</span></button></a></div></div></div></div><div class="sc-psrQp dNlktM"><div class="sc-pkfsj eWAAem"><div class="sc-psAte gMVDjT"><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx google-material-icons">menu</button></div><div class="sc-qQAox boezIO"><a href="https://www.kaggle.com/"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/site-logo.png" class="sc-pbKro bjcxym"></a></div></div><div class="sc-pZlBu dEvZvv"><div class="sc-oUqyN eIYhBz"><div class="sc-pbvBv jsONvj"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pjuKs jdzzxM">search</i><p class="sc-fzqARJ sc-fzqNqU sc-prqHV dloUTY">Search</p></div></div><div class="km-list"><a href="https://www.kaggle.com/" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">explore</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Home</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/competitions" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/ic_comps_deselected.svg" class="sc-pQQAz gpsSCH"><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Compete</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/datasets" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">table_chart</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Data</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/notebooks" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ hycqTh" role="listitem" tabindex="0"><i color="#202124" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl ddcgsN">code</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hzMfxM">Notebooks</p><div class="sc-qXgsJ bgVgeZ"></div></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/discussion" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">comment</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Discuss</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/learn" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">school</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Courses</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/jobs" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">business_center</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Jobs</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">expand_more</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">More</p></li><div class="sc-qQMSE jyKJRd"></div></a></div></div><div class="sc-pkvSM gpmTzN"></div></div><div class="sc-qPJtC cKarFc"><div class="sc-qXFrf fBislN"><div class="sc-pkfsj eWAAem"><div class="sc-psAte gMVDjT"><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx google-material-icons">menu</button></div><div class="sc-qQAox boezIO"><a href="https://www.kaggle.com/"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/site-logo.png" class="sc-pbKro bjcxym"></a></div></div></div><div class="sc-pZlBu dEvZvv"><div class="sc-oUqyN eIYhBz"><div class="sc-pbvBv jsONvj"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pjuKs jdzzxM">search</i><p class="sc-fzqARJ sc-fzqNqU sc-prqHV dloUTY">Search</p></div></div><div class="km-list"><a href="https://www.kaggle.com/" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">explore</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Home</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/competitions" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/ic_comps_deselected.svg" class="sc-pQQAz gpsSCH"><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Compete</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/datasets" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">table_chart</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Data</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/notebooks" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ hycqTh" role="listitem" tabindex="0"><i color="#202124" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl ddcgsN">code</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hzMfxM">Notebooks</p><div class="sc-qXgsJ bgVgeZ"></div></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/discussion" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">comment</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Discuss</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/learn" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">school</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Courses</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/jobs" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">business_center</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">Jobs</p></li><div class="sc-qQMSE jyKJRd"></div></a><a href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#" class="sc-qPkqk kuHLLu"><li class="sc-pRtAn dlRVIf sc-pAayJ pbbcw" role="listitem" tabindex="0"><i color="#80868B" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl cbuTye">expand_more</i><p class="sc-fzqARJ sc-fzqNqU sc-pIvzE hSNpBX">More</p></li><div class="sc-qQMSE jyKJRd"></div></a></div></div></div><div data-testid="searchContainer" class="sc-oVcyR jQLpAz"><div class="sc-qbELi hCxzEv"><div class="sc-qPIWj iRKpJi"><div class="sc-pQrUA keqDxd"><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx google-material-icons">arrow_back</button><form data-testid="searchInputBarFormElement" class="sc-pAwOa cQVMki"><input data-testid="searchInputBarInputElement" class="sc-pIUfD jbZJoo searchTarget" type="text" size="1" value=""></form></div><div class="sc-pYPmd fqlqii"><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx sc-qXFOy bJZCQt google-material-icons">search</button><button aria-hidden="true" role="button" tabindex="0" class="sc-fznLPX hTnWXx google-material-icons">close</button></div></div></div><div class="sc-pcZmk eKKLWt"><div class="sc-qWfCM eskDZY"><div data-testid="searchInitialPage" class="sc-pQfvp hBInHv"><div class="sc-pkvvt fbnKIp"><p class="sc-fzoyTs sc-fzoNJl sc-pYcnE hwrRzi">Trending Searches</p><div class="sc-qPjXN eYzxUD"><li class="mdc-list-item sc-qXgLg kKFcHj searchTarget" role="listitem"><div class="sc-pzXPE lhhXeF"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pIvhh bPcmWg">trending_up</i></div><div class="sc-oUnPI fMZnGz"><h6 class="sc-fznJRM sc-fznWqX sc-pZopv IkinA">house</h6></div></li></div><div class="sc-qPjXN eYzxUD"><li class="mdc-list-item sc-qXgLg kKFcHj searchTarget" role="listitem"><div class="sc-pzXPE lhhXeF"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pIvhh bPcmWg">trending_up</i></div><div class="sc-oUnPI fMZnGz"><h6 class="sc-fznJRM sc-fznWqX sc-pZopv IkinA">logistic regression</h6></div></li></div><div class="sc-qPjXN eYzxUD"><li class="mdc-list-item sc-qXgLg kKFcHj searchTarget" role="listitem"><div class="sc-pzXPE lhhXeF"><i sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl bSGqIJ sc-pIvhh bPcmWg">trending_up</i></div><div class="sc-oUnPI fMZnGz"><h6 class="sc-fznJRM sc-fznWqX sc-pZopv IkinA">mental health</h6></div></li></div></div><div class="sc-pkvvt fbnKIp"><p class="sc-fzoyTs sc-fzoNJl sc-pYcnE hwrRzi">Popular Tags</p><div class="sc-pssnI dTOZkD"><div class="sc-ptEpz elHUAl mdc-chip-set"><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">gpu</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">1694730</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">tpu</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">73884</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">beginner</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">21971</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">data visualization</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">15076</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">business</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">13493</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">exploratory data analysis</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">10450</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">deep learning</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">9897</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">classification</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">8382</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">utility script</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">8199</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">earth and nature</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">7765</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">arts and entertainment</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">7593</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">computer science</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">6078</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">feature engineering</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">4615</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">internet</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">4420</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">data cleaning</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">4367</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">education</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">3978</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">clothing and accessories</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">3497</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">online communities</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">3279</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">nlp</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">2999</span></div></div></button><button tabindex="0" customsize="medium" class="sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-upgraded sc-fzqKVi blNcET sc-pIhhe eubWuu mdc-ripple-surface mdc-chip"><div class="mdc-chip__text"><div class="sc-qPwwY aevsi"><p class="sc-fzoyTs sc-fzoNJl sc-qXTOB gva-DJg">health</p><span class="sc-fznzOf sc-fznMnq sc-pAkoP jAWugF">2684</span></div></div></button></div></div></div></div></div></div></div><div id="site-content" class="sc-pAzCb kgzgdZ"><div class="sc-pIUCW kFpHGq"><div class="sc-AxjAm dVDPh"><div class="sc-AxirZ ezTeoc">We use cookies on Kaggle to deliver our services, analyze web traffic, and improve your experience on the site. By using Kaggle, you agree to our use of cookies.</div><div class="sc-AxiKw kOAUSS"><div class="sc-AxhCb gsXzyw">Got it</div><a href="https://www.kaggle.com/cookies" class="sc-AxhUy fxWvvr"><div class="sc-AxhCb gsXzyw">Learn more</div></a></div></div></div><div id="kernel-container" class="KernelViewer_KernelContainer-sc-pqp17m ghCHNB"><div id="kernel-header-wrapper" class="KernelViewer_HeaderWrapper-sc-8tipmk hKKZMm" style="top: 0px;"><div class="KernelViewerContext_ViewerHeader-sc-1yl2fcg dAVsfH"><span class="KernelViewerContext_PrimaryInfoWrapper-sc-1y8qdg5 yIJPU"><span class="KernelViewerContext_AuthorThumbnailWrapper-sc-jgjas1 ivsRNo"><a class="avatar" href="https://www.kaggle.com/vbookshelf" style="width: 37px;"><img class="avatar__thumbnail" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/1086574-kg.jpg" alt="Marsh" width="37" height="37" style="border-radius: 3.7px;"><img class="avatar__tier" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/avatier-expert@2x.png" alt="expert tier" width="37" style="margin-top: 1.85px;"></a><span class="KernelViewerContext_TruncatedCollaboratorsWrapper-sc-1a47ru8 gCKHMa"><span><span class="TruncatedCollaborators_CollaboratorThumbnailWrapper-sc-soe92h dSUapk"></span></span></span></span><span class="KernelViewerContext_TitleBlock-sc-1g6h0vy iBZjQs"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/bronzel@1x.png" alt="bronze medal" class="KernelViewerContext_KernelMedal-sc-11qb0f7 dMKIji"><a class="KernelViewerContext_KernelTitle-sc-rdaqnd chqxNN">Malaria Cell Analyzer + Tensorflow.js Web App</a><br><span class="KernelViewerContext_KernelSubtitle-sc-rltxca esPWpV"><span class="KernelViewerContext_KernelTypeInfo-sc-58vmvn fbzaIp">Python notebook using data from</span><a href="https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria" class="KernelViewerContext_DataSourceUrl-sc-q3np7t cQzqHz"> Malaria Cell Images Dataset</a><span>  <span>974</span> views</span><span>  <span title="Tue May 28 2019 10:08:48 GMT+0530 (India Standard Time)">1y ago</span></span><span class="KernelViewerContext_CategoriesWrapper-sc-1ivezpk duFlJV"><span><span class="Categories_CategorySeparator-sc-b5hptt jgGkHQ category_separator"></span><span><span class="Categories_TagIcon-sc-1xao8k8 eWyhOz fa fa-tag"></span><span><a href="https://www.kaggle.com/tags/gpu" class="Categories_CategoryLink-sc-qnv6v2 choYYp">gpu</a>, </span><span><a href="https://www.kaggle.com/tags/cnn" class="Categories_CategoryLink-sc-qnv6v2 choYYp">cnn</a></span></span></span></span></span></span></span><span class="HeaderOptionsContext_CommunityWrapper-sc-1xsfcpi bmgjpa"><span class="KernelVoteButton_VoteButtonWrapper-sc-2bmt8m jkdDhc"><div class="vote-button__container "><div class="vote-button vote-button--compact vote-button--enabled"><div class="vote-button__button vote-button__button--up vb-upvote"><span class="fa fa-angle-up"></span></div><div class="vote-button__button vote-button__button--up vote-button__vote-count-container"><span class="vote-button__vote-count">8</span></div><div class="vote-button__button-placeholder"></div></div></div></span><div class="ForkButton_Wrapper-sc-1t7l405 hBXvia"><span class="ForkButton_JoinedCopyAndEditContainer-sc-lcxjzz gTTnvZ fork-button"><div class="ToolTip_ToolTipContainer-sc-f0vhmk hZctOZ"><div data-tip="true" data-for="tooltip_0" currentitem="false"><a class="button__anchor-wrapper" data-disable-rapidash="false"><div class="button button--disabled ForkButton_CreateCopyAndEditButton-sc-1dnwfvb jWSuEU"><span><span class="ForkButton_CopyAndEditTextContainer-sc-m90zur sPdHz"><span class="ForkButton_CodeForkIcon-sc-5oak8 cFOFOc fa fa-clone"></span> Copy and Edit</span></span></div></a></div><div class="__react_component_tooltip place-left type-dark " id="tooltip_0" data-id="tooltip" style="left: 664px; top: 82px;"><div class="ToolTip_ToolTipView-sc-1ci7zcv kYjOWe">This notebook uses GPU. Please sign in to enable copying.</div></div></div><a class="button__anchor-wrapper" data-disable-rapidash="false"><div class="button ForkButton_CreateCopyAndEditButton-sc-1dnwfvb ForkButton_CopyAndEditCountButton-sc-2wqg9l eOytFf"><span><div>7</div></span></div></a></span></div><span class="HeaderOptionsContext_InlineDropdown-sc-183b1ux eMugJq"><div class="NewsfeedDropdown_DropdownContainer-sc-1t1dgyy kMBvdR"><div class="NewsfeedDropdown_DropdownIconContainer-sc-vzs67v ivhIzp"><span name="ellipsis-h" class="fa fa-ellipsis-h"></span> </div></div></span></span></div><div class="sc-jXyOrd cHHTHg"></div></div><div class="KernelViewer_ViewerContent-sc-8uapew krPmHG"><div class="kernel-viewer"><span class="KernelViewer_NavigationSidebarWrapper-sc-m3cp4m elKRpW"><div class="KernelViewerContext_FullNavigationMenu-sc-ht8mmc gkAIMn"><span><div class="VersionsInfoBox_VersionTitle-sc-1earxha hAguIn">Version 3 of 3</div></span><div class="navbox__container"><div class="navbox__nav-wrapper"><nav class="navbox__nav"><a class="navbox__nav-item--selected" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/notebook" title="Notebook"><span class="navbox__nav-item-content">Notebook</span></a><div class="navbox__page"><span class="KernelViewerContext_ContentsMenu-sc-1mdi3gb bEocnw"><div class="navbox__container"><div class="navbox__nav-wrapper"><nav class="navbox__nav"><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#Malaria-Cell-Analyzer" title=""><span class="navbox__nav-item-content">Malaria Cell Analyzer</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#Introduction" title=""><span class="navbox__nav-item-content">Introduction</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#Contents" title=""><span class="navbox__nav-item-content">Contents</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#1.-Domain-Knowledge" title=""><span class="navbox__nav-item-content">1. Domain Knowledge</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#2.-EDA" title=""><span class="navbox__nav-item-content">2. EDA</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#3.-Create-a-Holdout-Set" title=""><span class="navbox__nav-item-content">3. Create a Holdout Set</span></a><a class="navbox__nav-item--selected-with-bar" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#4.-Train-Test-Split-Model" title=""><span class="navbox__nav-item-content">4. Train-Test-Split Model</span></a><div class="navbox__page"></div><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#5.-Error-Analysis" title=""><span class="navbox__nav-item-content">5. Error Analysis</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#5-Fold-Cross-Validation" title=""><span class="navbox__nav-item-content">5 Fold Cross Validation</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#7.-Train-the-Final-Model-using-all-the-data" title=""><span class="navbox__nav-item-content">7. Train the Final Model using all the data</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#8.-Evaluate-the-Final-Model-on-the-Holdout-Set" title=""><span class="navbox__nav-item-content">8. Evaluate the Final Model on the Holdout Set</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app#9.-Convert-the-final-model-from-Keras-to-Tensorflow.js" title=""><span class="navbox__nav-item-content">9. Convert the final model from Keras to Tensorflow.js</span></a></nav></div></div></span></div><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/data" title="Input"><span class="navbox__nav-item-content">Input (1)</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/output" title="Output"><span class="navbox__nav-item-content">Output</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/execution" title="Execution Info"><span class="navbox__nav-item-content">Execution Info</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/log" title="Log"><span class="navbox__nav-item-content">Log</span></a><a class="navbox__nav-item" href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/comments" title="Comments"><span class="navbox__nav-item-content">Comments (3)</span></a></nav></div></div></div></span><div class="kernel-viewer__container"><div class="kernel-viewer__pane-container" id="kernel-viewer__pane-container"><div class="KernelViewer_SubmissionWrapper-sc-13mikru ekeSWA"></div><div id="notebook"><div><div style="display: block;"><div class="sc-jWRPOI fpCyIS sc-kIFxrv iHkFNK"><div class="sc-kNYnxC ckmtjn"><iframe id="rendered-kernel-content" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/__results__.html" scrolling="no" title="Main Kernel Content" class="sc-jYqlGV elfyZR" style="height: 36798px; display: block;"></iframe><div class="sc-jTooJc cahHVZ" style="display: none;"><div _size="medium" class="rmwc-circular-progress rmwc-circular-progress--size-medium rmwc-circular-progress--indeterminate"><svg class="rmwc-circular-progress__circle" viewBox="0 0 24 24"><circle class="rmwc-circular-progress__path" cx="12" cy="12" r="10"></circle></svg></div></div></div></div></div><div class="kernel-code-pane__subtitle" id="notebook-pane-license-info">This Notebook has been released under the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache 2.0</a> open source license.</div><div class="upvote-suggestion upvote-suggestion__kernel"><div class="upvote-suggestion__text"><div class="upvote-suggestion__main-text">Did you find this Notebook useful?</div><div class="upvote-suggestion__sub-text">Show your appreciation with an upvote</div></div><div class="upvote-suggestion__button"><div class="vote-button__container "><div class="vote-button vote-button--enabled"><div class="vote-button__button vote-button__button--up vb-upvote"><span class="fa fa-caret-up"></span></div><div class="vote-button__button vote-button__button--up vote-button__vote-count-container"><span class="vote-button__vote-count">8</span></div><div class="vote-button__button-placeholder"></div></div></div></div><div class="upvote-suggestion__voters"><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="Sayantan Das" data-tooltip-size="small"><a href="https://www.kaggle.com/sayantandas30011998" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/734966-kg.jpg" alt="Sayantan Das" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="Larxel" data-tooltip-size="small"><a href="https://www.kaggle.com/andrewmvd" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/793761-kg.jpg" alt="Larxel" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="Ekrem Bayar" data-tooltip-size="small"><a href="https://www.kaggle.com/ekrembayar" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/1056223-kg.jpg" alt="Ekrem Bayar" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="xrvf" data-tooltip-size="small"><a href="https://www.kaggle.com/aquibjkhan" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/1552901-kg.jpg" alt="xrvf" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="sjb17" data-tooltip-size="small"><a href="https://www.kaggle.com/jainsarika04" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/2106122-fb.jpg" alt="sjb17" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="Vipul Gandhi" data-tooltip-size="small"><a href="https://www.kaggle.com/vipulgandhi" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/2554700-kg.jpg" alt="Vipul Gandhi" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="arif" data-tooltip-size="small"><a href="https://www.kaggle.com/arifzizi" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/default-thumb.png" alt="arif" class="upvote-suggestion__voter-img"></a></span></div><div class="upvote-suggestion__voter"><span class="tooltip-container" data-tooltip="Ramon M. Ferreira" data-tooltip-size="small"><a href="https://www.kaggle.com/ramonmf" class="upvote-suggestion__voter-link"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/5138170-kg.JPG" alt="Ramon M. Ferreira" class="upvote-suggestion__voter-img"></a></span></div></div></div></div></div><div id="data"><div id="data" class="sc-jMWPSv RmUey"><div class="sc-kIhKUI kKxjwl"><div class="sc-jWtaIR fVfaUv"><div class="sc-kcaKSa cBdCIj"><div class="sc-jLZncB ilfWpF"><div class="sc-kJfnKC fhSxgF"><h6 class="sc-fznJRM sc-fznWqX sc-kOhnOT iMpPMS">Input</h6><div class="sc-Axmtr sc-fzoiQi sc-jSwCyv cCkeqw">334.69 MB</div></div><div class="sc-jBEVQz iSaZJQ"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">folder</i></div><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">Data Sources</p></div><ul class="sc-jXPsEC cNLfVg"><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-jMpEfV kXvCLJ"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">arrow_drop_down</i></div><div class="sc-kIOWHi ibJcNY"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/dataset-thumbnail.jpg" class="sc-kcQTKx jjbzIv"></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_0" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">Malaria Cell Images Dataset</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_0" data-id="tooltip"><div class="sc-pzMyG ejxVHF">Malaria Cell Images Dataset</div></div></div></div></div><ul class="sc-kcAzFB dJDCsP"><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc ePQctj"><div class="sc-jMpEfV prrTt"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">arrow_drop_down</i></div><div class="sc-kIOWHi bGaEKU"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">folder</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_1" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX gZjoYR">cell_images</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_1" data-id="tooltip"><div class="sc-pzMyG ejxVHF">cell_images</div></div></div></div></div><ul class="sc-kcAzFB dJDCsP"><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-jMpEfV kXvCLJ"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">arrow_right</i></div><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">folder</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_10" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">Parasitized</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_10" data-id="tooltip"><div class="sc-pzMyG ejxVHF">Parasitized</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-jMpEfV kXvCLJ"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">arrow_right</i></div><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">folder</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_11" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">Uninfected</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_11" data-id="tooltip"><div class="sc-pzMyG ejxVHF">Uninfected</div></div></div></div></div></ul></ul></ul></ul></ul></ul></div></div></div><div class="sc-kNjHSB clSFlc"><div class="sc-jMpdBL cDCgpr"><div class="sc-kIPxls bBDgkQ"><h5 class="sc-fzpjYC sc-fznxsB sc-kNRujl pGDwH">cell_images<span class="sc-kcQtlj dCeAhU">(2 directories)</span></h5></div><div class="sc-jxBCLY kxAPof"></div><div class="sc-jxJyOx gcztTL"><div class="sc-jDcoUE hHwBiB"><div class="sc-kIybTg kYFEIJ"><div class="sc-jRPqLV gbsul"><i color="#919294" sizevalue="48px" class="rmwc-icon google-material-icons sc-AxgMl dABvVq">folder</i></div><div class="sc-kNAbXx deyDWZ"><p class="sc-fzoXWK sc-fzpmMD sc-jXigSc eKCRlv">Parasitized</p><span class="sc-fznzOf sc-fznMnq sc-kbSmKH kTIgxU">13.8k files</span></div></div><div class="sc-kIybTg kYFEIJ"><div class="sc-jRPqLV gbsul"><i color="#919294" sizevalue="48px" class="rmwc-icon google-material-icons sc-AxgMl dABvVq">folder</i></div><div class="sc-kNAbXx deyDWZ"><p class="sc-fzoXWK sc-fzpmMD sc-jXigSc eKCRlv">Uninfected</p><span class="sc-fznzOf sc-fznMnq sc-kbSmKH kTIgxU">13.8k files</span></div></div></div></div></div></div></div></div><div id="output" class="sc-jMWPSv RmUey"><div class="sc-kIhKUI kKxjwl"><div class="sc-jWtaIR fVfaUv"><div class="sc-kcaKSa cBdCIj"><div class="sc-jLZncB ilfWpF"><div class="sc-kJfnKC fhSxgF"><h6 class="sc-fznJRM sc-fznWqX sc-kOhnOT iMpPMS">Output</h6><div class="sc-Axmtr sc-fzoiQi sc-jSwCyv cCkeqw">84.17 MB</div></div><ul class="sc-jXPsEC cNLfVg"><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc ePQctj"><div class="sc-kIOWHi bGaEKU"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_2" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX gZjoYR">df_val_preds.pickle</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_2" data-id="tooltip"><div class="sc-pzMyG ejxVHF">df_val_preds.pickle</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_3" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">final_model.h5</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_3" data-id="tooltip"><div class="sc-pzMyG ejxVHF">final_model.h5</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_4" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">model.h5</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_4" data-id="tooltip"><div class="sc-pzMyG ejxVHF">model.h5</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_5" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">tfjs/model/group1-shard1of3.bin</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_5" data-id="tooltip"><div class="sc-pzMyG ejxVHF">tfjs/model/group1-shard1of3.bin</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_6" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">tfjs/model/group1-shard2of3.bin</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_6" data-id="tooltip"><div class="sc-pzMyG ejxVHF">tfjs/model/group1-shard2of3.bin</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">insert_drive_file</i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_7" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">tfjs/model/group1-shard3of3.bin</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_7" data-id="tooltip"><div class="sc-pzMyG ejxVHF">tfjs/model/group1-shard3of3.bin</div></div></div></div></div></ul><ul class="sc-khTpLI jYmywc"><div role="button" class="sc-jBEVQz sc-kcsDDc jAaZSE"><div class="sc-kIOWHi ibJcNY"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe"><svg x="0px" y="0px" viewBox="0 0 24 24" class="sc-kijJQE gwuApG" style="fill: currentcolor;"><g><rect id="bounds" width="24" height="24" style="fill: none;"></rect><path id="Shape" d="M11,7h2v2h-2V7z M11,11h2v7h-2V11z"></path><path d="M2,12v-1c0,0,0,0,0,0c1.1,0,2-0.9,2-2l0,0V4c0,0,0,0,0,0c0-1.1,0.9-2,2-2c0,0,0,0,0,0h3v2H6
 c0,0,0,2,0,6c0,0,0,0,0,0h0c0,1.1-0.9,2-2,2c0,0,0,0,0,0C4,12,3.3,12,2,12z"></path><path d="M22,12v-1c0,0,0,0,0,0c-1.1,0-2-0.9-2-2l0,0V4c0,0,0,0,0,0c0-1.1-0.9-2-2-2c0,0,0,0,0,0h-3v2h3
 c0,0,0,2,0,6c0,0,0,0,0,0h0c0,1.1,0.9,2,2,2c0,0,0,0,0,0C20,12,20.7,12,22,12z"></path><path d="M2,12v1c0,0,0,0,0,0c1.1,0,2,0.9,2,2l0,0v5c0,0,0,0,0,0c0,1.1,0.9,2,2,2c0,0,0,0,0,0h3v-2H6
 c0,0,0-2,0-6c0,0,0,0,0,0h0c0-1.1-0.9-2-2-2c0,0,0,0,0,0C4,12,3.3,12,2,12z"></path><path d="M22,12v1c0,0,0,0,0,0c-1.1,0-2,0.9-2,2l0,0v5c0,0,0,0,0,0c0,1.1-0.9,2-2,2c0,0,0,0,0,0h-3v-2h3
 c0,0,0-2,0-6c0,0,0,0,0,0h0c0-1.1,0.9-2,2-2c0,0,0,0,0,0C20,12,20.7,12,22,12z"></path></g></svg></i></div><div class="sc-khuDHt dNUvqM"><div class="sc-qZtCU gYnjXV sc-jXzbGe hUurmq" data-tip="true" data-for="tooltip_components_8" currentitem="false"><p class="sc-fzoyTs sc-fzoNJl sc-jSglzX uuNaJ">tfjs/model/model.json</p><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_8" data-id="tooltip"><div class="sc-pzMyG ejxVHF">tfjs/model/model.json</div></div></div></div></div></ul></ul></div></div><div class="sc-jHUSUC ihCUq"><button class="sc-fzpkqZ cwYqgr sc-khcHPT ecUxDH" type="submit"><i sizevalue="18px" class="rmwc-icon google-material-icons sc-AxgMl bSGqGe">get_app</i><span class="sc-fzoxKX sc-fzoKki cyTXUp">Download All</span></button></div></div><div class="sc-kNjHSB clSFlc"><div class="sc-jMpdBL cDCgpr"><div class="sc-kIPxls bBDgkQ"><h5 class="sc-fzpjYC sc-fznxsB sc-kNRujl pGDwH">df_val_preds.pickle<span class="sc-kcQtlj dCeAhU">(267.62 KB)</span></h5><div class="sc-qZtCU gYnjXV" data-tip="true" data-for="tooltip_components_9" currentitem="false"><i color="rgba(0,0,0,0.7)" sizevalue="24px" class="rmwc-icon google-material-icons sc-AxgMl dhwzZD sc-kijjrq kRpYTx">get_app</i><div class="__react_component_tooltip place-top type-dark " id="tooltip_components_9" data-id="tooltip"><div class="sc-pzMyG ejxVHF">Download</div></div></div></div><div class="sc-kOpIPO gYTKnb"><h6 class="sc-fznJRM sc-fznWqX sc-jSphag byzoAi">Unable to show preview</h6><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/error.png" class="sc-jXrhex gMtyaZ"><p class="sc-fzoyTs sc-fzoNJl sc-kcsddO hOnhnD">Previews for binary data are not supported</p></div></div></div></div></div><div aria-live="assertive" aria-atomic="true" aria-hidden="true" class="sc-jwCYSG cgbVsE mdc-snackbar mdc-snackbar--leading"><div class="mdc-snackbar__surface"><div role="status" aria-live="polite" class="mdc-snackbar__label"><div style="display: none;"></div></div><div class="mdc-snackbar__actions"></div></div></div></div><div id="output"><div></div></div><div id="execution"><div class="kernel-execution-pane"><div class="content-box"><div><div class="content-box__title-bar"><div class="ContentBox_Title-sc-6fbrxj hMwprG" style="line-height: 46px;">Execution Info</div></div></div><div class="content-box__content-section"><div class="kernel-execution-pane__execution-info"><div class="kernel-execution-pane__execution-info-top"><div class="kernel-execution-pane__execution-info-column"><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Succeeded</div><div class="kernel-execution-pane__execution-info-right">True</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Exit Code</div><div class="kernel-execution-pane__execution-info-right">0</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Used All Space</div><div class="kernel-execution-pane__execution-info-right">False</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Environment</div><div class="kernel-execution-pane__execution-info-right"><a href="https://gcr.io/kaggle-gpu-images/python" target="_blank" class="ExecutionPane_DockerLink-sc-fmfkhb jwkRpE">Container Image</a> (<a href="https://github.com/Kaggle/docker-python/blob/master/gpu.Dockerfile" target="_blank">Dockerfile</a>)</div></div></div><div class="kernel-execution-pane__execution-info-column"><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Run Time</div><div class="kernel-execution-pane__execution-info-right">11065.3 seconds</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Timeout Exceeded</div><div class="kernel-execution-pane__execution-info-right">False</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Output Size</div><div class="kernel-execution-pane__execution-info-right">0</div></div><div class="kernel-execution-pane__execution-info-row"><div class="kernel-execution-pane__execution-info-left">Accelerator</div><div class="kernel-execution-pane__execution-info-right">GPU</div></div></div></div></div></div></div></div></div><div id="log"><div class="kernel-log-pane"><div class="content-box"><div><div class="content-box__title-bar"><div class="ContentBox_Title-sc-6fbrxj hMwprG" style="line-height: 46px;">Log</div><div class="content-box__right-side"><div class="kernel-code-pane__right-section"><a href="data:text/plain;charset=utf-8,%5BNbConvertApp%5D%20Converting%20notebook%20__notebook__.ipynb%20to%20notebook%0A%0A%5BNbConvertApp%5D%20Executing%20notebook%20with%20kernel%3A%20python3%0A%0A2019-05-28%2004%3A42%3A10.301794%3A%20I%20tensorflow%2Fcore%2Fplatform%2Fprofile_utils%2Fcpu_utils.cc%3A94%5D%20CPU%20Frequency%3A%202200000000%20Hz%0A%0A2019-05-28%2004%3A42%3A10.304988%3A%20I%20tensorflow%2Fcompiler%2Fxla%2Fservice%2Fservice.cc%3A150%5D%20XLA%20service%200x55bc427a7020%20executing%20computations%20on%20platform%20Host.%20Devices%3A%0A2019-05-28%2004%3A42%3A10.305032%3A%20I%20tensorflow%2Fcompiler%2Fxla%2Fservice%2Fservice.cc%3A158%5D%20%20%20StreamExecutor%20device%20(0)%3A%20%3Cundefined%3E%2C%20%3Cundefined%3E%0A%0A2019-05-28%2004%3A42%3A10.476988%3A%20I%20tensorflow%2Fstream_executor%2Fcuda%2Fcuda_gpu_executor.cc%3A998%5D%20successful%20NUMA%20node%20read%20from%20SysFS%20had%20negative%20value%20(-1)%2C%20but%20there%20must%20be%20at%20least%20one%20NUMA%20node%2C%20so%20returning%20NUMA%20node%20zero%0A%0A2019-05-28%2004%3A42%3A10.478124%3A%20I%20tensorflow%2Fcompiler%2Fxla%2Fservice%2Fservice.cc%3A150%5D%20XLA%20service%200x55bc4285b760%20executing%20computations%20on%20platform%20CUDA.%20Devices%3A%0A2019-05-28%2004%3A42%3A10.478440%3A%20I%20tensorflow%2Fcompiler%2Fxla%2Fservice%2Fservice.cc%3A158%5D%20%20%20StreamExecutor%20device%20(0)%3A%20Tesla%20P100-PCIE-16GB%2C%20Compute%20Capability%206.0%0A%0A2019-05-28%2004%3A42%3A10.485884%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A1433%5D%20Found%20device%200%20with%20properties%3A%20%0Aname%3A%20Tesla%20P100-PCIE-16GB%20major%3A%206%20minor%3A%200%20memoryClockRate(GHz)%3A%201.3285%0ApciBusID%3A%200000%3A00%3A04.0%0AtotalMemory%3A%2015.90GiB%20freeMemory%3A%2015.61GiB%0A2019-05-28%2004%3A42%3A10.485930%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A1512%5D%20Adding%20visible%20gpu%20devices%3A%200%0A%0A2019-05-28%2004%3A42%3A11.211375%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A984%5D%20Device%20interconnect%20StreamExecutor%20with%20strength%201%20edge%20matrix%3A%0A2019-05-28%2004%3A42%3A11.211458%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A990%5D%20%20%20%20%20%200%20%0A2019-05-28%2004%3A42%3A11.211476%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A1003%5D%200%3A%20%20%20N%20%0A%0A2019-05-28%2004%3A42%3A11.218924%3A%20I%20tensorflow%2Fcore%2Fcommon_runtime%2Fgpu%2Fgpu_device.cc%3A1115%5D%20Created%20TensorFlow%20device%20(%2Fjob%3Alocalhost%2Freplica%3A0%2Ftask%3A0%2Fdevice%3AGPU%3A0%20with%2015121%20MB%20memory)%20-%3E%20physical%20GPU%20(device%3A%200%2C%20name%3A%20Tesla%20P100-PCIE-16GB%2C%20pci%20bus%20id%3A%200000%3A00%3A04.0%2C%20compute%20capability%3A%206.0)%0A%0A2019-05-28%2004%3A42%3A12.999853%3A%20I%20tensorflow%2Fstream_executor%2Fdso_loader.cc%3A152%5D%20successfully%20opened%20CUDA%20library%20libcublas.so.10.0%20locally%0A%0A%5BNbConvertApp%5D%20WARNING%20%7C%20Timeout%20waiting%20for%20IOPub%20output%0A%0A%5BNbConvertApp%5D%20WARNING%20%7C%20Timeout%20waiting%20for%20IOPub%20output%0A%0A%5BNbConvertApp%5D%20WARNING%20%7C%20Timeout%20waiting%20for%20IOPub%20output%0A%0A%5BNbConvertApp%5D%20Writing%201968811%20bytes%20to%20__notebook__.ipynb%0A%0A%5BNbConvertApp%5D%20Converting%20notebook%20__notebook__.ipynb%20to%20html%0A%0A%5BNbConvertApp%5D%20Support%20files%20will%20be%20in%20__results___files%2F%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Making%20directory%20__results___files%0A%5BNbConvertApp%5D%20Writing%20555278%20bytes%20to%20__results__.html%0A" class="kernel-code-pane__download" download="malaria-cell-analyzer-tensorflow-js-web-app.log">Download Log</a></div></div></div></div><div class="content-box__content-section"><div class="kernel-log-pane__log"><div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column kernel-log-pane__time-column--header">Time</span><span class="kernel-log-pane__line-no-column kernel-log-pane__line-no-column--header">Line #</span><span class="kernel-log-pane__data-column kernel-log-pane__data-column--header">Log Message</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">5.7s</span><span class="kernel-log-pane__line-no-column">1</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Converting notebook __notebook__.ipynb to notebook
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">5.9s</span><span class="kernel-log-pane__line-no-column">2</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Executing notebook with kernel: python3
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">200.9s</span><span class="kernel-log-pane__line-no-column">3</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:10.301794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">200.9s</span><span class="kernel-log-pane__line-no-column">4</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:10.304988: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc427a7020 executing computations on platform Host. Devices:
2019-05-28 04:42:10.305032: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">201.0s</span><span class="kernel-log-pane__line-no-column">5</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:10.476988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">201.0s</span><span class="kernel-log-pane__line-no-column">6</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:10.478124: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc4285b760 executing computations on platform CUDA. Devices:
2019-05-28 04:42:10.478440: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">201.0s</span><span class="kernel-log-pane__line-no-column">7</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:10.485884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:00:04.0
totalMemory: 15.90GiB freeMemory: 15.61GiB
2019-05-28 04:42:10.485930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">201.8s</span><span class="kernel-log-pane__line-no-column">8</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:11.211375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-05-28 04:42:11.211458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-05-28 04:42:11.211476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">201.8s</span><span class="kernel-log-pane__line-no-column">9</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:11.218924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15121 MB memory) -&gt; physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">203.6s</span><span class="kernel-log-pane__line-no-column">10</span><span class="kernel-log-pane__data-column--stderr">2019-05-28 04:42:12.999853: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">1821.7s</span><span class="kernel-log-pane__line-no-column">11</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] WARNING | Timeout waiting for IOPub output
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">9013.7s</span><span class="kernel-log-pane__line-no-column">12</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] WARNING | Timeout waiting for IOPub output
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">10980.9s</span><span class="kernel-log-pane__line-no-column">13</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] WARNING | Timeout waiting for IOPub output
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11060.6s</span><span class="kernel-log-pane__line-no-column">14</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Writing 1968811 bytes to __notebook__.ipynb
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11063.3s</span><span class="kernel-log-pane__line-no-column">15</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Converting notebook __notebook__.ipynb to html
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11064.7s</span><span class="kernel-log-pane__line-no-column">16</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Support files will be in __results___files/
[NbConvertApp] Making directory __results___files
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11064.7s</span><span class="kernel-log-pane__line-no-column">17</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
</span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11064.7s</span><span class="kernel-log-pane__line-no-column">18</span><span class="kernel-log-pane__data-column--stderr">[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Making directory __results___files
[NbConvertApp] Writing 555278 bytes to __results__.html
</span></div><div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11064.7s</span><span class="kernel-log-pane__line-no-column">19</span><span class="kernel-log-pane__data-column"></span></div><div class="kernel-log-pane__line"><span class="kernel-log-pane__time-column">11064.7s</span><span class="kernel-log-pane__line-no-column">21</span><span class="kernel-log-pane__data-column--success">Complete. Exited with code 0.</span></div></div></div></div></div></div></div></div><div id="comments"><div class="sc-pkTlu fWKsqo"><div class="sc-psPnT kKrOoz"><div class="sc-qPlVY ciPYeQ"><p class="sc-fzqARJ sc-fzqNqU lcuOYI">Comments <span class="sc-qXhYx htegr">(3)</span></p></div><div class="sc-pAcex hIsIVl"><p class="sc-fzoXWK sc-fzpmMD sc-pIxfs iCtRcy">Sort by </p><div class="sc-ptbNe fRblot"><div class="sc-qPYZt fCfzpT"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Hotness</p><i color="rgba(0,0,0,0.5)" sizevalue="20px" class="rmwc-icon google-material-icons sc-AxgMl fJpgWV">arrow_drop_down</i></div><div class="sc-qYuao iiiEcc"><a class="sc-pcxuJ EElru"><div class="sc-pCOsa iDlSoE"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Hotness</p></div></a><a class="sc-pcxuJ EElru"><div class="sc-pCOsa iDlSoE"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Most Votes</p></div></a><a class="sc-pcxuJ EElru"><div class="sc-pCOsa iDlSoE"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Newest</p></div></a><a class="sc-pcxuJ EElru"><div class="sc-pCOsa iDlSoE"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Oldest</p></div></a><a class="sc-pcxuJ EElru"><div class="sc-pCOsa iDlSoE"><p class="sc-fzoyTs sc-fzoNJl sc-pKKuz gPVmWL">Chronological</p></div></a></div></div></div></div><div class="sc-pQSgn eLSRLP"><div id="540417" class="sc-pRreJ ktsOWN"><div class="sc-pAytO fbvEUZ"><div class="sc-pYQRR fjCNyU"><a class="sc-oVfmS hrQHij" href="https://www.kaggle.com/aquibjkhan"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/1552901-kg.jpg" class="sc-pdbpr icVbui"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/avatier-contributor@2x.png" class="sc-plWPA iKeZgx"></a><div class="sc-oULiq izrAPy"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/bronzel@1x(1).png" class="sc-qXHum kKDeXp"></div></div><div class="sc-pIVLr iiTJYa"><div class="sc-pQtAo hGjRFy"></div><div class="sc-pZMfE hCQsRh"><div class="sc-oTPZz duiGAL"><div class="sc-pclau hoCIAk"><p class="sc-fzoXWK sc-fzpmMD sc-pjHZB bCZMKc"><a href="https://www.kaggle.com/aquibjkhan" class="sc-qbBXh sc-oUa-dg jquLaK">xrvf</a>    <span>Posted on Version 3 of 3    </span><span title="Fri May 31 2019 19:01:09 GMT+0530 (India Standard Time)">a year ago</span>    <a color="grey" class="sc-qbBXh flQLOk">Options</a>    <a color="grey" class="sc-qbBXh flQLOk">Reply</a></p></div><div><div class="sc-qcpLw sc-pIHGs goiKsp"><div class="sc-oVqWn sc-pZAOG hCKUUY"><div class="sc-pREbn fFwgCU"><i sizevalue="20px" class="rmwc-icon google-material-icons sc-AxgMl bSGqLl">keyboard_arrow_up</i></div><div class="sc-paYBk jrsLBv"><p class="sc-fzoXWK sc-fzpmMD iffvTX">1</p></div></div></div></div></div><div class="sc-qWRat eBfncH"><div class="sc-qbDfu dWREo"><p>Great kernel, Amazed to see the results after padding. </p></div><div class="sc-oTcgo HNTRh"></div></div></div></div></div></div></div><div class="sc-pQSgn eLSRLP"><div id="536543" class="sc-pRreJ ktsOWN"><div class="sc-pAytO fbvEUZ"><div class="sc-pYQRR fjCNyU"><a class="sc-oVfmS hrQHij" href="https://www.kaggle.com/haeyzl"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/default-thumb.png" class="sc-pdbpr icVbui"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/avatier-novice@2x.png" class="sc-plWPA iKeZgx"></a><div class="sc-oULiq izrAPy"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/bronzel@1x(1).png" class="sc-qXHum kKDeXp"></div></div><div class="sc-pIVLr iiTJYa"><div class="sc-pQtAo hGjRFy"></div><div class="sc-pZMfE hCQsRh"><div class="sc-oTPZz duiGAL"><div class="sc-pclau hoCIAk"><p class="sc-fzoXWK sc-fzpmMD sc-pjHZB bCZMKc"><a href="https://www.kaggle.com/haeyzl" class="sc-qbBXh sc-oUa-dg jquLaK">Chris C</a>    <span>Posted on Version 1 of 3    </span><span title="Fri May 24 2019 22:03:24 GMT+0530 (India Standard Time)">a year ago</span>    <a color="grey" class="sc-qbBXh flQLOk">Options</a>    <a color="grey" class="sc-qbBXh flQLOk">Reply</a></p></div><div><div class="sc-qcpLw sc-pIHGs goiKsp"><div class="sc-oVqWn sc-pZAOG hCKUUY"><div class="sc-pREbn fFwgCU"><i sizevalue="20px" class="rmwc-icon google-material-icons sc-AxgMl bSGqLl">keyboard_arrow_up</i></div><div class="sc-paYBk jrsLBv"><p class="sc-fzoXWK sc-fzpmMD iffvTX">1</p></div></div></div></div></div><div class="sc-qWRat eBfncH"><div class="sc-qbDfu dWREo"><p>Really nice kernel, very thoroughly explained throughout!</p></div><div class="sc-oTcgo HNTRh"></div></div></div></div></div></div></div><div class="sc-pQSgn eLSRLP"><div id="872697" class="sc-pRreJ ktsOWN"><div class="sc-pAytO fbvEUZ"><div class="sc-pYQRR fjCNyU"><a class="sc-oVfmS hrQHij" href="https://www.kaggle.com/asma123"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/default-thumb.png" class="sc-pdbpr icVbui"><img src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/avatier-novice@2x.png" class="sc-plWPA iKeZgx"></a><div class="sc-oULiq izrAPy"></div></div><div class="sc-pIVLr iiTJYa"><div class="sc-pQtAo hGjRFy"></div><div class="sc-pZMfE hCQsRh"><div class="sc-oTPZz duiGAL"><div class="sc-pclau hoCIAk"><p class="sc-fzoXWK sc-fzpmMD sc-pjHZB bCZMKc"><a href="https://www.kaggle.com/asma123" class="sc-qbBXh sc-oUa-dg jquLaK">asma</a>    <span>Posted on Version 3 of 3    </span><span title="Wed Jun 03 2020 18:04:44 GMT+0530 (India Standard Time)">4 months ago</span>    <a color="grey" class="sc-qbBXh flQLOk">Options</a>    <a color="grey" class="sc-qbBXh flQLOk">Reply</a></p></div><div><div class="sc-qcpLw sc-pIHGs goiKsp"><div class="sc-oVqWn sc-pZAOG hCKUUY"><div class="sc-pREbn fFwgCU"><i sizevalue="20px" class="rmwc-icon google-material-icons sc-AxgMl bSGqLl">keyboard_arrow_up</i></div><div class="sc-paYBk jrsLBv"><p class="sc-fzoXWK sc-fzpmMD iffvTX">0</p></div></div></div></div></div><div class="sc-qWRat eBfncH"><div class="sc-qbDfu dWREo"><p>Hi,<br>
I have a working code in python, i do not know how to get this to tensorflow.js, from what i have understood i have created .h5 and [.json+.bin] files and after this i do not know what to do next.<br>
Can someone please help.</p></div><div class="sc-oTcgo HNTRh"></div></div></div></div></div></div></div><div class="sc-pleUP ljmSTG"><div class="sc-psnTT eKlXqL"><div class="sc-qPNpY gUSggg"></div></div></div></div></div></div></div><div class="KernelViewer_EmptyColumn-sc-1ubq290 hTXhVJ"></div></div></div><span class="KernelViewer_NavigationFooterWrapper-sc-wrdi08 igLevt"><div class="StickyFooter-sc-17zvw1m fxFNDA"><nav class="NavBoxFooter_NavigationBar-sc-fpq4iw eBMKZz"><a href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/notebook" title="Notebook" class="NavBoxFooter_NavigationElement-sc-1a4m5yz NavBoxFooter_SelectedNavigationElement-sc-128c1nx fhmScz"><div><span name="book" size="20" class="fa fa-book fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv eYbBJp">Notebook</div></a><a href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/data" title="Input" class="NavBoxFooter_NavigationElement-sc-1a4m5yz gLiAfK"><div><span name="table" size="20" class="fa fa-table fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv eYbBJp">Input</div></a><a href="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/comments" title="Comments" class="NavBoxFooter_NavigationElement-sc-1a4m5yz gLiAfK"><div><span name="comment" size="20" class="fa fa-comment fa-stack-20x"></span></div><div class="NavBoxFooter_NavigationText-sc-p8ascv eYbBJp">Comments</div></a></nav><div class="navbox__page"></div></div></span></div><div class="sc-pYMCY kvKFJc"></div><div id="site-body" class="">
    



<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script class="kaggle-component" nonce="">var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":4009178,"title":"Malaria Cell Analyzer + Tensorflow.js Web App","forkParent":null,"currentRunId":14798337,"mostRecentRunId":40892475,"url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app","tags":[{"name":"cnn","slug":"cnn","url":"/tags/cnn"},{"name":"gpu","slug":"gpu","url":"/tags/gpu"}],"commentCount":0,"upvoteCount":8,"viewCount":974,"forkCount":7,"bestPublicScore":null,"author":{"id":1086574,"displayName":"Marsh","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","profileUrl":"/vbookshelf","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":2,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isKaggleBot":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"isPhoneVerified":false},"isPrivate":false,"updatedTime":"2020-08-17T07:50:16.94Z","selfLink":"/kernels/4009178","pinnedDockerImageVersionId":null,"dockerImagePinningType":"original","originalDockerImageId":29507,"isLanguageTemplate":false,"medal":"bronze","topicId":93251,"readGroupId":null,"writeGroupId":null,"slug":"malaria-cell-analyzer-tensorflow-js-web-app","hasUsedAccelerator":false,"pinnedSessionId":null,"disableComments":false,"hasLinkedSubmission":false},"kernelBlob":{"id":331614577,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","accelerator":"gpu","isInternetEnabled":true},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Malaria Cell Analyzer\\nby Marsh [ @vbookshelf ] \u003cbr\u003e\\n24 May 2019\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/children.jpg\\\u0022 width=\\\u0022600\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eMalaria killed more than 261,000 children in 2017\u003c/h5\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003e *Malaria exacts a massive toll on human health and imposes a heavy social\\nand economic burden in low and middle income countries, particularly in Sub-Saharan Africa and South Asia. An estimated 219 million people suffered from the disease in 2017 and about 435,000 died. More than 90 percent of the deaths were in Africa, and over 60 percent were among children under 5.\u003cbr\u003e*\\n ~ [gatesfoundation.org](https://www.gatesfoundation.org/What-We-Do/Global-Health/Malaria)\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Introduction\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The goal of this kernel is to build a model that can detect malaria parasites in a cell image. The model will analyze a segmented red blood cell image and classify it as either \\\u0022uninfected\\\u0022 or \\\u0022parasitized\\\u0022. Once trained the model will be deployed online as a Tensorflow.js web app. \\n\\nMalaria diagnosis is currently a manual process. This prototype app is capable of automatically batch analyzing cell images. Therefore, it can help speed up a doctor\u0027s diagnostic workflow and reduce diagnostic errors. It can also help medical staff to triage patients by allowing them to quickly assess the severity of each patient\u0027s infection.\\n\\nWe will do basic EDA, build a Keras cnn model, do 5 fold cross validation, train the model using all the data and finally evaluate the model on a holdout set.\\n\\n\\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message saying that the model is loading but the app may actually be frozen. To see the app in action simply feed it some images from this dataset.\\n\\n\u003e Web App\u003cbr\u003e\\n\u003e http://malaria.test.woza.work/\u003cbr\u003e\\n\u003e \\n\u003e Github\u003cbr\u003e\\n\u003e https://github.com/vbookshelf/Malaria-Cell-Analyzer\\n\\n\\nBecause this app relies on segmented images (Fig. 3 below) this isn\u0027t a true end to end solution. In practice a glass slide image has many cells (see Fig. 2 below). A segmentation algorithm will need to be used to isolate (segment out) each cell before it can be fed into this app.\\n\\nThe app is based on Tensorflow.js, a technology that Google developed that allows machine learning models to run in a web browser. You\u0027ll notice that the app can process multiple images in about one second. It\u0027s fast because the analysis is done locally - on the user\u0027s pc or mobile phone. There\u0027s no need to upload images to an external server thus ensuring patient privacy and data security.\\n\\nI hope that this project will help you see how easy it is to build and deploy an Ai based solution.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Contents\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ca href=\u0027#Domain_Knowledge\u0027\u003e1. Domain Knowledge\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#EDA\u0027\u003e2. EDA\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Create_a_Holdout-Set\u0027\u003e3. Create a Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train-Test-Split_Model\u0027\u003e4. Train-Test-Split Model\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Error_Analysis\u0027\u003e5. Error Analysis\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Cross_Validation\u0027\u003e6. 5 Fold Cross Validation\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train\u0027\u003e7. Train the Final Model on all data\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Evaluate\u0027\u003e8. Evaluate the Final Model on the Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Convert\u0027\u003e9. Convert the Final Model to Tensorflow.js\u003c/a\u003e\u003cbr\u003e\\n\\n\u003ca href=\u0027#Citations\u0027\u003eCitations\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Reference_Kernels\u0027\u003eReference Kernels\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Helpful_Resources\u0027\u003eHelpful Resources\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Conclusion\u0027\u003eConclusion\u003c/a\u003e\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# set seeds to ensure repeatability of results\\nfrom numpy.random import seed\\nseed(101)\\nfrom tensorflow import set_random_seed\\nset_random_seed(101)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport os\\n\\nimport cv2\\nimport tensorflow\\n\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nimport itertools\\nimport shutil\\nimport pickle\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nfrom sklearn.model_selection import KFold, StratifiedKFold\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import classification_report\\n\\n\\nimport tensorflow\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.metrics import categorical_crossentropy\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\\nfrom tensorflow.keras.metrics import binary_accuracy\\n\\n\\n# Don\u0027t Show Warning Messages\\nimport warnings\\nwarnings.filterwarnings(\u0027ignore\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\nIMAGE_HEIGHT = 96\\nIMAGE_WIDTH = 96\\n\\nNUM_HOLDOUT_IMAGES = 200\\n\\nNUM_EPOCHS = 10\\nNUM_FOLDS = 5\\n\\nPADDING = 10\\nBATCH_SIZE = 10\\n\\n\\nNUM_FINAL_MODEL_EPOCHS = 10\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Domain_Knowledge\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 1. Domain Knowledge\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ciframe width=\\\u0022560\\\u0022 height=\\\u0022315\\\u0022 src=\\\u0022https://www.youtube.com/embed/2O3YrdUZQ5U?rel=0\\\u0022 frameborder=\\\u00220\\\u0022 allow=\\\u0022accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\u0022 allowfullscreen\u003e\u003c/iframe\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n\\n### Quick Facts\\n\\n- Malaria is a disease that infects and destroys red blood cells. \\n- Doctors analyze thick and thin blood smears to diagnose malaria and determine how severe it is.\\n- The parasite that causes malaria is called Plasmodium.\\n- The female Anopheles mosquito is the only mosquito that transmits malaria.\\n\\n\\n### Background\\n\\nMicroscopic examination of thick and thin blood smears is the easiest and most reliable test for malaria. Blood smears are often taken from a finger prick.\\n\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/glassslide.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 1. Microscope and glass slide\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/raw_image.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 2. Cells seen under a microscope. \u003cbr\u003e\\nThin blood smear.\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/parasitized.png\\\u0022 width=\\\u0022200\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 3. Segmented parasitized cell\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n*Malaria diagnosis involves the following*:\u003cbr\u003e\\n\u003e - Determine if the malaria parasite is present.\\n\u003e - Determine the parasite species.\\n\u003e - Determine parasite density by counting the number of cells that are infected.\\n\\n\u003cbr\u003e\\n\\n*Thick Blood Smear*\u003cbr\u003e\\n\\nA thick blood smear is a drop of blood on a glass slide. It\u0027s used to determine if there are any parasites in the blood. It\u0027s a larger blood sample because there could only be a few parasites present so a larger sample is needed to detect them. If no parasites are found the patient will have repeated blood smears every 8 hours for a few of days to confirm that there\u0027s no malaria infection.\\n\\n\u003cbr\u003e\\n\\n*Thin Blood Smear*\u003cbr\u003e\\n\\nA thin blood smear is a drop of blood that\u0027s spread over a large area of the glass slide. This blood smear helps doctor\u0027s discover what species of parasite is causing the infection.\\n\\n\u003cbr\u003e\\n\\n*Plasmodium*\u003cbr\u003e\\n\\nThis is the parasite that causes malaria. There are many species of Plasmodium. Five species cause malaria - P. vivax, P. ovale, P. malariae, P. knowlesi and P. falciparum. Most deaths are caused by P. falciparum. The other species usually cause a milder form of malaria.\\n\\n\u003cbr\u003e\\n\\n*Parasitemia*\u003cbr\u003e\\n\\nThis is the percentage of red blood cells that are infected by malaria (parasite density). This is computed by counting the number of infected cells visible under a microscope. This number helps doctors determine how severe the disease is. They then prescribe a treatment based on the severity. For example, if a large percentage of blood cells is infected medicine may be given directly into a vein instead of by mouth.\\n\\n\u003cbr\u003e\\n**Sources**\\n\\n- uofmhealth.org\u003cbr\u003e\\nhttps://www.uofmhealth.org/health-library/hw118744\\n\\n- Wikipedia Malaria\u003cbr\u003e\\nhttps://en.wikipedia.org/wiki/Malaria\\n\\n- Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027EDA\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 2. EDA\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### How many files are in each folder?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nuninfected_list = os.listdir(path_uninfected)\\nparasitized_list = os.listdir(path_parasitized)\\n\\nprint(\u0027Uninfected: \u0027, len(uninfected_list))\\nprint(\u0027Parasitized: \u0027, len(parasitized_list))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The data description says there should be 27,558 cell images i.e. 13779 files per folder. We see that each folder has an extra file. Later we\u0027ll check what kind of file this is.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Let\u0027s take a quick look at some images from each class\\n\\nThere are two classes\\n- uninfected\\n- parasitized.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = uninfected_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = parasitized_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Check if any non image files are in the folders\\n\\nWe see that there are actually 13780 files in each folder and not 13779 as expected. Folders can sometimes contain files that are automatically created during processing. These files can cause errors in the code if we don\u0027t know they exist - like the code we wrote above to display the images. Let\u0027s check if there are any non image files in the folders. \u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\n# sample image name: C140P101ThinF_IMG_20151005_211735_cell_159.png\\n\\nfor item in uninfected_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Uninfected folder: \u0027, item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\nfor item in parasitized_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Parasitized folder: \u0027,item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We see that each folder has one non image file called Thumbs.db. Now that we know that these non image files exist we will be sure exclude them later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Put the image names into dataframes\\n\\nHere we will create a dataframe called df_combined that includes both uninfected and parasitized images. This new dataframe will have a column showing the target class of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# create the dataframe\\ndf_uninfected = pd.DataFrame(uninfected_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_uninfected = df_uninfected[df_uninfected[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_uninfected[\u0027target\u0027] = 0\\n\\n\\n# create the dataframe\\ndf_parasitized = pd.DataFrame(parasitized_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_parasitized = df_parasitized[df_parasitized[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_parasitized[\u0027target\u0027] = 1\\n\\n#print(df_uninfected.shape)\\n#print(df_parasitized.shape)\\n\\n# Combine the two dataframes\\n\\ndf_combined = pd.concat([df_uninfected, df_parasitized], axis=0).reset_index(drop=True)\\n\\n#df_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_combined.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shape.\\n# There should be 27558 rows.\\n\\ndf_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if the image names are unique.\\n# The output should be 27558\\n\\ndf_combined[\u0027image_id\u0027].nunique()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The output above matches the number of rows. This confirms that each image has a unique name. This is important to verify because duplicate image file names will cause errors in the processing code that we will write later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### What are the image sizes and how many channels does each have?\\n\\nHere we will add the following info on each image to the df_combined dataframe:\\n\\nw = width\u003cbr\u003e\\nh = height\u003cbr\u003e\\nc = number of channels\u003cbr\u003e\\nmax_pixel_value\u003cbr\u003e\\nmin_pixel_value\u003cbr\u003e\\nimage_format\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndef read_image_sizes(file_name):\\n    \\\u0022\\\u0022\\\u0022\\n    1. Get the shape of the image\\n    2. Get the min and max pixel values in the image.\\n    Getting pixel values will tell if any pre-processing has been done.\\n    3. This info will be added to the original dataframe.\\n    \\\u0022\\\u0022\\\u0022\\n    \\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n     \\n    if file_name in uninfected_list:\\n        \\n        path = path_uninfected\\n        \\n    else:\\n        path = path_parasitized\\n    \\n    \\n    image = cv2.imread(path + file_name)\\n    max_pixel_val = image.max()\\n    min_pixel_val = image.min()\\n    img_format = file_name.split(\u0027.\u0027)[1]\\n    output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val, img_format]\\n    return output\\n\\nm = np.stack(df_combined[\u0027image_id\u0027].apply(read_image_sizes))\\ndf = pd.DataFrame(m,columns=[\u0027w\u0027,\u0027h\u0027,\u0027c\u0027,\u0027max_pixel_val\u0027,\u0027min_pixel_val\u0027, \u0027image_format\u0027])\\n\\ndf_combined = pd.concat([df_combined,df],axis=1, sort=False)\\n\\ndf_combined.head(10)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images have 3 channels\\ndf_combined[\u0027c\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images are in png format\\ndf_combined[\u0027image_format\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all black images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 0) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 0)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all white images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 255) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 255)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Display random images from each of the target classes\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The images are randomly selected. Therefore, different images will be dispayed each time the code is run. Also you\u0027ll see that parasitized images seem to have a blue-ish area that\u0027s not as common in uninfected images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_uninfected[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_parasitized[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### EDA Summary\\n\\n- There are 27,558 segmented cell images.\\n- Each folder contains 13,779 images.\\n- The images are of various sizes.\\n- All images are in png format.\\n- All images have 3 channels i.e. all are colour images.\\n- There are no all black or all white images.\\n- The target distribution is balanced i.e. each target class has the same number of images.\\n- All cells were segmented from thin blood smear slides.\\n- The parasite specie on all parasitized images is P. falciparum.\\n- Each folder contains a non image file called Thumbs.db.\\n- The presence of a blue-ish area inside the cell appears to be a common (but not definitive) indicator that a cell is parasitized.\\n\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Create_a_Holdout-Set\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 3. Create a Holdout Set\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As we continue to train, validate and modify a model we could over time start to overfit the validation data without realizing it. This means that the model will perform well during training but it will perform poorly on unseen data i.e. the app will perform badly in production.\\n\\nHere we will create a holdout set containing 200 images. We will keep this holdout set aside and only use it at the end to check how the final model performs on unseen data.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# shuffle\\ndf_combined = shuffle(df_combined, random_state=101)\\n\\n# create a holdout set with 200 samples\\ndf_holdout = df_combined.sample(NUM_HOLDOUT_IMAGES, random_state=101)\\n\\n# create a list of holdout images\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_holdout.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shapes.\\n# The ouput should be:\\n# (200, 8)\\n# (27358, 8)\\n\\nprint(df_holdout.shape)\\nprint(df_data.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution in the holdout set.\\n# 0 = uninfected\\n# 1 = parasitized\\n\\ndf_holdout[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a holdout set containing 200 images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train-Test-Split_Model\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 4. Train-Test-Split Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Train-test-split is not the ideal way to assess model perfromance. However, it\u0027s a good starting point because it\u0027s simple to set up and runs 5 times faster than 5 fold cross validation. This helps us to:\\n\\n- Quickly get a feel for what kind of performance we can expect from this data.\\n- Quickly test different architectures and parameters.\\n- Establish the workflow that will later be used in cross validation.\\n- Check the training curves to see if the model is overfitting.\\n- Check the training curves to establish the number of epochs we will use when training the final model on all data.\\n- Perform error analysis.\\n\\nA train-test-split model is a rough guide. When deciding if a particular change actually improved model performance we will base that decision on cross validation results. This is important. If we don\u0027t use cross validation we may think we are improving but in reality we may be getting nowhere.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Directory Structure\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022To reduce RAM use and prevent this kernel from crashing we will batch feed the images when training the model. We will use generators to do this. In order to use this approach Keras requires that a particular directory structure be set up. Keras uses this structure to automatically infer the class (target) of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check if base_dir has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# see what\u0027s inside base_dir\\nos.listdir(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Train and Val Sets\\n\\nWe will create a stratified val set. This means that the val set will have the same target distribution as the train set. Actually doing stratification isn\u0027t essential here because the target is balanced. Because of this there\u0027s a good chance that randomly selecting rows will still result in the val set having a fairly balanced target distribution. Stratification is more applicable for data where the target is unbalanced.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# select the column that we will use for stratification\\ny = df_data[\u0027target\u0027]\\n\\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\\n\\nprint(df_train.shape)\\nprint(df_val.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution of the val set.\\n# The target should be approx balanced.\\n\\ndf_val[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Transfer the images into the folders\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n# Get a list of images in each of the two folders\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n        \\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n        \\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the number of images in each folder\\n\\n# train\\nprint(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\nprint(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Set Up the Generators\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Note that here we are normalizing the images inside the generator.\\n# If you wanted to add some data augmentation you could do it here.\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\nval_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Model Architecture\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022I found this cnn architecture a while ago in a kernel created by @fmarazzi. I\u0027ve since used this on several projects. It really is a good multi-purpose architecture. Here I\u0027ve modified it slightly by adding a ZeroPadding layer. Later, in the error analysis section, I\u0027ll explain why I added this layer.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\nmodel.summary()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Train the Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Please note that we\u0027ve set the learning rate to reduce (decay) at each epoch.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022filepath = \\\u0022model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n                              \\n                              \\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            validation_data=val_gen,\\n                            validation_steps=val_steps,\\n                            epochs=NUM_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Evaluate the model using the val set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# get the metric names so we can use evaulate_generator\\nmodel.metrics_names\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Here the best epoch will be used.\\n\\nmodel.load_weights(\u0027model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\nprint(\u0027val_loss:\u0027, val_loss)\\nprint(\u0027val_acc:\u0027, val_acc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Plot the Training Curves\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# display the loss and accuracy curves\\n\\nimport matplotlib.pyplot as plt\\n\\nacc = history.history[\u0027acc\u0027]\\nval_acc = history.history[\u0027val_acc\u0027]\\nloss = history.history[\u0027loss\u0027]\\nval_loss = history.history[\u0027val_loss\u0027]\\n\\nepochs = range(1, len(acc) + 1)\\n\\nplt.plot(epochs, loss, \u0027bo\u0027, label=\u0027Training loss\u0027)\\nplt.plot(epochs, val_loss, \u0027b\u0027, label=\u0027Validation loss\u0027)\\nplt.title(\u0027Training and validation loss\u0027)\\nplt.legend()\\nplt.figure()\\n\\nplt.plot(epochs, acc, \u0027bo\u0027, label=\u0027Training acc\u0027)\\nplt.plot(epochs, val_acc, \u0027b\u0027, label=\u0027Validation acc\u0027)\\nplt.title(\u0027Training and validation accuracy\u0027)\\nplt.legend()\\nplt.figure()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the labels of the test images.\\n\\ntest_labels = test_gen.classes\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# We need these to plot the confusion matrix.\\ntest_labels\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the label associated with each class\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# If you wanted to get the image_id\u0027s to match them to predictions this\\n# is how to do it.\\n\\n# test_gen.filenames\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check the number of predictions\\npredictions.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Source: Scikit Learn website\\n# http://scikit-learn.org/stable/auto_examples/\\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\\n# selection-plot-confusion-matrix-py\\n\\n\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\u0027Confusion matrix\u0027,\\n                          cmap=plt.cm.Blues):\\n    \\\u0022\\\u0022\\\u0022\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\u0022\\\u0022\\\u0022\\n    if normalize:\\n        cm = cm.astype(\u0027float\u0027) / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\u0022Normalized confusion matrix\\\u0022)\\n    else:\\n        print(\u0027Confusion matrix, without normalization\u0027)\\n\\n    print(cm)\\n\\n    plt.imshow(cm, interpolation=\u0027nearest\u0027, cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n\\n    fmt = \u0027.2f\u0027 if normalize else \u0027d\u0027\\n    thresh = cm.max() / 2.\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\n        plt.text(j, i, format(cm[i, j], fmt),\\n                 horizontalalignment=\\\u0022center\\\u0022,\\n                 color=\\\u0022white\\\u0022 if cm[i, j] \u003e thresh else \\\u0022black\\\u0022)\\n\\n    plt.ylabel(\u0027True label\u0027)\\n    plt.xlabel(\u0027Predicted label\u0027)\\n    plt.tight_layout()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_labels.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Error_Analysis\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5. Error Analysis\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022In this section we\u0027re not going to use any fancy statistical gymnastics. We\u0027re simply going to look at the images that the model predicted correctly and those it got wrong. We want to see if there are any patterns or issues that could\u0027ve caused the model to make mistakes.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Put the val image_id, labels and predictions into a dataframe.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# put the val image_id, labels and predictions into a dataframe\\n\\nval_pred_dict = {\\n    \u0027image_id\u0027: test_gen.filenames,\\n    \u0027val_labels\u0027: test_gen.classes,\\n    \u0027val_preds\u0027: predictions.argmax(axis=1)\\n}\\n\\ndf_val_preds = pd.DataFrame(val_pred_dict)\\n\\n\\n# Adjust the file names\\n\\n# sample image name: a_uninfected/C100P61ThinF_IMG_20150918_144104_...\\n# we want just this part: C100P61ThinF_IMG_20150918_144104_...\\n\\ndef adjust_file_names(x):\\n    # split into a list based on \u0027/\u0027\\n    fname = x.split(\u0027/\u0027)\\n    # chose the second item in the list which is the image name\\n    fname = fname[1]\\n    \\n    return fname\\n\\ndf_val_preds[\u0027image_id\u0027] = df_val_preds[\u0027image_id\u0027].apply(adjust_file_names)\\n\\n\\n# savedf_val_preds so we can analyze the results later\\npickle.dump(df_val_preds,open(\u0027df_val_preds.pickle\u0027,\u0027wb\u0027))\\n\\n# code to load the dataframe\\n# df_val_preds = pickle.load(open(\u0027df_val_preds\u0027,\u0027rb\u0027))\\n\\n\\n#df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# filter out those rows where the model made correct predictions\\ndf_correct = df_val_preds[df_val_preds[\u0027val_labels\u0027] == df_val_preds[\u0027val_preds\u0027]]\\n\\n# filter out those rows where the model made wrong predictions\\ndf_wrong = df_val_preds[df_val_preds[\u0027val_labels\u0027] != df_val_preds[\u0027val_preds\u0027]]\\n\\nprint(df_correct.shape)\\nprint(df_wrong.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the correct predictions\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_correct.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_correct[df_correct[\u0027val_labels\u0027] == 1]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 1\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_correct[df_correct[\u0027val_labels\u0027] == 0]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 0\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the wrong predictions\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized but the model predicted uninfected\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_wrong[df_wrong[\u0027val_labels\u0027] == 1]\\n\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 0\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected but the model predicted parasitized\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_wrong[df_wrong[\u0027val_labels\u0027] == 0]\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 1\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Observations and Comments\\n\\nIn cases where the label was parasitized but the model predicted uninfected I noticed that the blue-ish area was located close to the edge of the image. Could it be that the model was not detecting important patterns that are located at the edge of images? To address this issue I decided to add a padding layer to the model arcitecture. This layer added a 10 pixel zero padding around each image. I found that the cross validation scores improved after this change.\\n\\n\\nWhen doing this error analysis our premise has been that if there is a mismatch between the label and the prediction then the model has made a mistake. However, there\u0027s another possibility - the model prediction is correct. There are 27,558 images in this dataset. All images were examined and labeled by the same expert. This must\u0027ve been a tedious and time consuming process so it\u0027s possible that some images were incorrectly labeled. \\n\\nAt this point it would be helpful to collaborate with a domain expert in order to discuss possible reasons for the mistakes this model is making. Are they the result of model weakness, incorrect labels or maybe damaged images? Is the correct label easy for a human to classify or would a human struggle to make a correct diagnosis? What are the main indicators that an expert looks for when examining cell images? Is the model seeing things that an expert didn\u0027t notice?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Cross_Validation\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5 Fold Cross Validation\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Here we will simply apply the same workflow that we used for the train-test-split model to 5 folds. For each fold we\u0027ll get the loss, accuracy and auc. Then we\u0027ll average the results of the 5 folds to get the final scores.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# ==============================\\n# Create the 5 Folds\\n# ==============================\\n\\n# shuffle df_combined and change the name to df_data\\ndf_data = shuffle(df_combined.copy())\\n\\n# train_test_split\\ny = df_data[\u0027target\u0027]\\n\\n# initialize kfold\\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=101)\\n\\n# define y for stratification\\ny = df_data[\u0027target\u0027]\\n\\n# Note:\\n# Each fold is a tuple ([train_index_values], [val_index_values])\\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df_train, y)\\n\\n# Put the folds into a list. This is a list of tuples.\\n# y was set above.\\nfold_list = list(kf.split(df_data, y))\\n\\n\\n# ==============================\\n# Loop Through the Folds\\n# ==============================\\n\\n# create a list to store the predictions\\nval_pred_list = []\\n\\n# create a list to store the scores\\nval_acc_list = []\\nval_loss_list = []\\nval_auc_list = []\\n\\n\\nfor i, fold in enumerate(fold_list):\\n\\n    # Delete the image data directory we created to prevent a Kaggle error.\\n    # Kaggle allows a max of 500 files to be saved.\\n    \\n    if os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n        \\n        \\n    \\n    # set df_data\\n    df_data = df_combined.copy()\\n    \\n    print(\u0027=== Fold_\u0027 + str(i) + \u0027 ===\u0027)\\n    print(\u0027\\\\n\u0027)\\n\\n    # map the train and val index values to dataframe rows\\n    df_train = df_data[df_data.index.isin(fold[0])]\\n    df_val = df_data[df_data.index.isin(fold[1])]\\n    \\n\\n\\n\\n    # ==============================\\n    # Create a Directory Structure\\n    # ==============================\\n\\n    # Create a new directory\\n    base_dir = \u0027base_dir\u0027\\n    os.mkdir(base_dir)\\n\\n\\n    #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n    # now we create 2 folders inside \u0027base_dir\u0027:\\n\\n    # train\\n        # a_uninfected\\n        # b_parasitized\\n\\n    # val\\n        # a_uninfected\\n        # b_parasitized\\n\\n\\n    # create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n    # train_dir\\n    train_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\n    os.mkdir(train_dir)\\n\\n    # val_dir\\n    val_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\n    os.mkdir(val_dir)\\n\\n\\n    # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n    # Inside each folder we create seperate folders for each class\\n\\n    # create new folders inside train_dir\\n    a_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n    # create new folders inside val_dir\\n    a_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n\\n    # =================================\\n    # Transfer the Images into Folders\\n    # =================================\\n\\n    # Set the image_id as the index in df_data\\n    df_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n    # Get a list of images in each of the two folders\\n\\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n    folder_1 = os.listdir(path_uninfected)\\n    folder_2 = os.listdir(path_parasitized)\\n\\n    # Get a list of train and val images\\n    train_list = list(df_train[\u0027image_id\u0027])\\n    val_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n    # Transfer the train images\\n\\n    for image in train_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n\\n\\n    # Transfer the val images\\n\\n    for image in val_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n    # Print the number of images in each folder\\n\\n    # train\\n    #print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n    # val\\n    #print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n    #print(\u0027\\\\n\u0027)\\n\\n\\n    # ==============================\\n    # Set Up the Generators\\n    # ==============================\\n\\n    train_path = \u0027base_dir/train_dir\u0027\\n    valid_path = \u0027base_dir/val_dir\u0027\\n\\n    num_train_samples = len(df_train)\\n    num_val_samples = len(df_val)\\n    train_batch_size = BATCH_SIZE\\n    val_batch_size = BATCH_SIZE\\n\\n\\n    train_steps = np.ceil(num_train_samples / train_batch_size)\\n    val_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\n    datagen = ImageDataGenerator(rescale=1.0/255)\\n\\n    train_gen = datagen.flow_from_directory(train_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=train_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    val_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    # Note: shuffle=False causes the test dataset to not be shuffled\\n    test_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027,\\n                                            shuffle=False)\\n    \\n    print(\u0027\\\\n\u0027)\\n\\n    # ==============================\\n    # Set Up the Model Architecture\\n    # ==============================\\n\\n\\n\\n    kernel_size = (3,3)\\n    pool_size= (2,2)\\n    first_filters = 32\\n    second_filters = 64\\n    third_filters = 128\\n\\n    dropout_conv = 0.3\\n    dropout_dense = 0.3\\n\\n\\n    model = Sequential()\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                     input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n    \\n    model.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n    \\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size)) \\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Flatten())\\n    model.add(Dense(256, activation = \\\u0022relu\\\u0022))\\n    model.add(Dropout(dropout_dense))\\n    model.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n    #model.summary()\\n\\n\\n\\n    # ==============================\\n    # Train the Model\\n    # ==============================\\n\\n\\n    model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n                  metrics=[\u0027accuracy\u0027])\\n\\n    filepath = \\\u0022model.h5\\\u0022\\n    checkpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                                 save_best_only=True, mode=\u0027max\u0027)\\n\\n    reduce_lr = ReduceLROnPlateau(monitor=\u0027val_acc\u0027, factor=0.5, patience=2, \\n                                       verbose=1, mode=\u0027max\u0027, min_lr=0.00001)\\n\\n\\n    callbacks_list = [checkpoint, reduce_lr]\\n\\n    history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                                validation_data=val_gen,\\n                                validation_steps=val_steps,\\n                                epochs=NUM_EPOCHS, verbose=1,\\n                               callbacks=callbacks_list)\\n\\n\\n\\n    # ==================================\\n    # Evaluate the Model on the Val Set\\n    # ==================================\\n\\n    model.load_weights(\u0027model.h5\u0027)\\n\\n    val_loss, val_acc = \\\\\\n    model.evaluate_generator(test_gen, \\n                            steps=val_steps)\\n    \\n    # append the acc score val_scores_list\\n    val_acc_list.append(val_acc)\\n    val_loss_list.append(val_loss)\\n    \\n    \\n    # ==================================\\n    # Calculate the AUC Score\\n    # ==================================\\n\\n    test_labels = test_gen.classes\\n\\n    # make a prediction\\n    predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n    \\n    # append the predictions to a list\\n    val_pred_list.append(predictions)\\n    \\n    val_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n    \\n    val_auc_list.append(val_auc)\\n    \\n    \\n    \\n    # ==================================\\n    # Print the Fold Scores\\n    # ==================================\\n    \\n    \\n    #print(\u0027\\\\n\u0027)\\n    #print(\u0027Fold_\u0027 + str(i) + \u0027 scores:\\\\n\u0027)\\n    #print(\u0027val_loss:\u0027, val_loss)\\n    #print(\u0027val_acc:\u0027, val_acc)\\n    #print(\u0027val_auc:\u0027, val_auc)\\n    #print(\u0027\\\\n\u0027)\\n\\n    \\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Average for of all 5 folds:\\\\n\u0027)\\n#print(\u0027Average Accuracy: \u0027, avg_acc)\\n#print(\u0027Average Loss: \u0027, avg_loss)\\n#print(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Print the scores for each fold and the average scores\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Print the scores\\n\\nprint(\u0027Val Acc\u0027)\\nfor item in val_acc_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val Loss\u0027)\\nfor item in val_loss_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val AUC\u0027)\\nfor item in val_auc_list:\\n    print(item)\\n\\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Average for of all 5 folds:\\\\n\u0027)\\nprint(\u0027Average Accuracy: \u0027, avg_acc)\\nprint(\u0027Average Loss: \u0027, avg_loss)\\nprint(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 7. Train the Final Model using all the data\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We\u0027re not using validation data in this final model. Therefore, we won\u0027t be able to set up the model to save the best epoch. We\u0027ll need to determine the number of training epochs before starting.\\n\\nTo determine this number I would normally look at the training curves (see train-test-split model) and determine the epoch at which the model started to overfit i.e. the training and validation accuracy curves start to diverge. However, with this data and architecture it appears that overfitting is not a huge problem because the training and validation curves do not diverge significantly. Therefore choosing 10 epochs seems to give a good balance between training time and model quality.\\n\\nIt\u0027s also important to keep in mind that we can train on all data because we set the learning rate to decay at each epoch i.e. the learning rate was scheduled. If we had used a dynamic learning rate (e.g. ReduceLROnPlateau) then we\u0027ll need to have some way of replicating the learning rate changes that happened automatically during 5 fold cross validation. Therefore, using a scheduled learning rate keeps things simple.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n\\n\\n\\n# ==============================\\n# Set df_train\\n# ==============================\\n\\n# This variable was set above. Just setting it here again for clarity.\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\\ndf_train = df_data.copy()\\ndf_val = df_holdout.copy()\\n\\n\\n\\n# ==============================\\n# Create a Directory Structure\\n# ==============================\\n\\n# Create a new directory\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n\\n# =================================\\n# Transfer the Images into Folders\\n# =================================\\n\\n# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n# Set the image_id as the index in df_data\\ndf_holdout.set_index(\u0027image_id\u0027, inplace=True)\\n\\n\\n\\n# Get a list of images in each of the two folders\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n\\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n\\n    fname = image\\n    target = df_holdout.loc[image,\u0027target\u0027]\\n\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n# Print the number of images in each folder\\n\\n# train\\n#print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\n#print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n#print(\u0027\\\\n\u0027)\\n\\n\\n# ==============================\\n# Set Up the Generators\\n# ==============================\\n\\ntrain_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\\n\\nprint(\u0027\\\\n\u0027)\\n\\n# ==============================\\n# Set Up the Model Architecture\\n# ==============================\\n\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n#model.summary()\\n\\n\\n\\n# ==============================\\n# Train the Model\\n# ==============================\\n\\n\\nmodel.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\\n\\n# we are saving the model based on training accuracy\\nfilepath = \\\u0022final_model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n\\n\\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            epochs=NUM_FINAL_MODEL_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\\n\\n\\n# ==================================\\n# Evaluate the Model on the Val Set\\n# ==================================\\n\\nmodel.load_weights(\u0027final_model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\n\\n# ==================================\\n# Calculate the AUC Score\\n# ==================================\\n\\ntest_labels = test_gen.classes\\n\\n# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n\\n\\nval_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n\\n\\n\\n\\n# ==================================\\n# Print the Scores\\n# ==================================\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Accuracy: \u0027, val_acc)\\n#print(\u0027Loss: \u0027, val_loss)\\n#print(\u0027AUC: \u0027, val_auc)\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Evaluate\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 8. Evaluate the Final Model on the Holdout Set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# ==================================\\n# Print the Scores\\n# ==================================\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Accuracy: \u0027, val_acc)\\nprint(\u0027Loss: \u0027, val_loss)\\nprint(\u0027AUC: \u0027, val_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\\n\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\\n\\n\\n# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a trained model that can be incorporated into the web app. The metrics we calculated above all look very good meaning that the model should perform well on unseen data.\\n\\nAlso, because we\u0027ve used a simple architecture the model size will be less than 10 MB. This means that it will download quickly. Therefore, the web page will load fast and the overall user experience will be good.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Convert\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 9. Convert the final model from Keras to Tensorflow.js\\n\\nThis conversion needs to be done so that the model can be loaded into the web app.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# --ignore-installed is added to fix an error.\\n\\n# https://stackoverflow.com/questions/49932759/pip-10-and-apt-how-to-avoid-cannot-uninstall\\n# -x-errors-for-distutils-packages\\n\\n!pip install tensorflowjs --ignore-installed\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Use the command line conversion tool to convert the model\\n\\n!tensorflowjs_converter --input_format keras final_model.h5 tfjs/model\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the folder containing the tfjs model files has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the tfjs files exist\\nos.listdir(\u0027tfjs/model\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Delete the images that were moved around\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Citations\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Citations\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n- Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images. PeerJ6:e4568\u003cbr\u003e\\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude, RJ, Jaeger S, Thoma GR. (2018)\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Reference_Kernels\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Reference Kernels\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Gabriel Preda, Honey Bee Subspecies Classification\u003cbr\u003e\\nhttps://www.kaggle.com/gpreda/honey-bee-subspecies-classification\\n\\n- Francesco Marazzi, Baseline Keras CNN\u003cbr\u003e\\nhttps://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\\n\\n- Kimmo Sskilahti, Image processing with scikit-image\u003cbr\u003e\\nhttps://www.kaggle.com/ksaaskil/image-processing-with-scikit-image\\n\\n- Marsh, Skin Lesion Analyzer\u003cbr\u003e\\nhttps://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Helpful_Resources\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Helpful Resources\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Excellent tutorial series by deeplizard on how to use Tensorflow.js to build a web app.\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\\n\\n- Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\\n\\n- Tutorial by Data School on how to evaluate a classifier\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\\n\\n- Tensorflow.js gallery of projects\u003cbr\u003e\\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Conclusion\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Conclusion\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This is the Foldscope Origami Microscope.\u003cbr\u003e\\nhttps://youtu.be/ky-cqSI5mwE\\n\\nThe microscope image could perhaps be photographed using phone camera and then processed using a web app.\\n\\n\\nThis was a software solution. However, there could be some who are reading this who may be interested in combining the power of Ai with electronics - by building robots, analyzers and other devices. You probably don\u0027t know where to start. The good news is that if you can code then you can also build electronic devices. The principles are the same. It\u0027s just that one uses software and the other uses physical components. \\n\\nThese two practical Udemy courses are a great place to start your journey towards world domination. They are geared towards young students and, most importantly, the instructor is an excellent teacher. The courses were developed with deaf students in mind.\\n\\nElectronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/analog-electronics-robotics-learn-by-building/\\n\\nDigital Electronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/digital-electronics-robotics-learn-by-building-module-ii/\\n\\n\\nMany thanks to Arunava for making this interesting dataset available on Kaggle.\\n\\nThank you for reading.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022Python 3\u0022,\u0022language\u0022:\u0022python\u0022,\u0022name\u0022:\u0022python3\u0022},\u0022language_info\u0022:{\u0022codemirror_mode\u0022:{\u0022name\u0022:\u0022ipython\u0022,\u0022version\u0022:3},\u0022file_extension\u0022:\u0022.py\u0022,\u0022mimetype\u0022:\u0022text/x-python\u0022,\u0022name\u0022:\u0022python\u0022,\u0022nbconvert_exporter\u0022:\u0022python\u0022,\u0022pygments_lexer\u0022:\u0022ipython3\u0022,\u0022version\u0022:\u00223.6.4\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2019-05-28T04:38:47.9095886Z"},"kernelRun":{"id":14798337,"kernelId":4009178,"status":"complete","type":"batch","sourceType":"notebook","language":"python","title":"Malaria Cell Analyzer + Tensorflow.js Web App","dateCreated":"2019-05-28T04:38:48.057Z","dateEvaluated":"2019-05-28T04:38:48.57Z","dateCancelled":null,"workerContainerPort":null,"workerUptimeSeconds":79407,"workerIPAddress":"172.21.129.78  ","workerIPAddressExternal":null,"scriptLanguageId":9,"scriptLanguageName":"IPython Notebook HTML","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TgZ-UlZOZpZxMgd6TNI8-g.n9OqO8yFIi_BtBqpqyAbcKDSddpgU0jsW42KOm3R-ccCWUs5Lhd-6fRgZHLfyOhtGplPcXuyaEPbdHaQYRyNVgDoC9ml4j2y2O3PiNvXJcgdWwptldsHwKBA416ycyZj07R8poTCf1ekx8Nk15-Ze80EndoHnKslyvyo4cXnrYEocaXikjuBVVfmtdk11ykxyDAhkx2HWXpo7rX5GJBdGGqMaaODtK2SBsZsRNcpclixq85z2qnM_PYMIaodsRsqa6cQ3gbBHVZyi4itN8bYv4NLafVjtijiTaInOGot6sxB-Q9QtQZugpkw84FHvbFm7JfPmxjGOR4K8x4hi0qHTNVgX_7iWxizk5z7eVEsieShp-GvNWgDtqCIcBuTmLPcWJF0m8-nGOPDYH3KdCbTm0a8zGObQC3X1I011JDRY5Q6DjejmmKGXut8BwysVMe0S2GOXjBRT2YGomfjRMR0wHpidXBZyzV8OpI5I2SoNN5nHgmadz1lzDlgzYoBXKBv9AUuSA2RBvn_unjnXtiwG-jiZQ9AzkthDXJuSZ1W-fV4j6a1XJjBfIfm6PKzaz8NX8T-dR8WlFf_oeRnhvZAOxsz0VQJxuKaE1ZvEjtaHDCQG-MQuh2kSJj3qE-WCAjZnd4HUXNJ3gAMQf2Bz7qjKkKF4pqXX8iRZGXYeH1GtMU.7aYWiJATsRDNrRJBtGX4KA/__results__.html","commit":{"id":331614577,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","accelerator":"gpu","isInternetEnabled":true},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Malaria Cell Analyzer\\nby Marsh [ @vbookshelf ] \u003cbr\u003e\\n24 May 2019\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/children.jpg\\\u0022 width=\\\u0022600\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eMalaria killed more than 261,000 children in 2017\u003c/h5\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003e *Malaria exacts a massive toll on human health and imposes a heavy social\\nand economic burden in low and middle income countries, particularly in Sub-Saharan Africa and South Asia. An estimated 219 million people suffered from the disease in 2017 and about 435,000 died. More than 90 percent of the deaths were in Africa, and over 60 percent were among children under 5.\u003cbr\u003e*\\n ~ [gatesfoundation.org](https://www.gatesfoundation.org/What-We-Do/Global-Health/Malaria)\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Introduction\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The goal of this kernel is to build a model that can detect malaria parasites in a cell image. The model will analyze a segmented red blood cell image and classify it as either \\\u0022uninfected\\\u0022 or \\\u0022parasitized\\\u0022. Once trained the model will be deployed online as a Tensorflow.js web app. \\n\\nMalaria diagnosis is currently a manual process. This prototype app is capable of automatically batch analyzing cell images. Therefore, it can help speed up a doctor\u0027s diagnostic workflow and reduce diagnostic errors. It can also help medical staff to triage patients by allowing them to quickly assess the severity of each patient\u0027s infection.\\n\\nWe will do basic EDA, build a Keras cnn model, do 5 fold cross validation, train the model using all the data and finally evaluate the model on a holdout set.\\n\\n\\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message saying that the model is loading but the app may actually be frozen. To see the app in action simply feed it some images from this dataset.\\n\\n\u003e Web App\u003cbr\u003e\\n\u003e http://malaria.test.woza.work/\u003cbr\u003e\\n\u003e \\n\u003e Github\u003cbr\u003e\\n\u003e https://github.com/vbookshelf/Malaria-Cell-Analyzer\\n\\n\\nBecause this app relies on segmented images (Fig. 3 below) this isn\u0027t a true end to end solution. In practice a glass slide image has many cells (see Fig. 2 below). A segmentation algorithm will need to be used to isolate (segment out) each cell before it can be fed into this app.\\n\\nThe app is based on Tensorflow.js, a technology that Google developed that allows machine learning models to run in a web browser. You\u0027ll notice that the app can process multiple images in about one second. It\u0027s fast because the analysis is done locally - on the user\u0027s pc or mobile phone. There\u0027s no need to upload images to an external server thus ensuring patient privacy and data security.\\n\\nI hope that this project will help you see how easy it is to build and deploy an Ai based solution.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Contents\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ca href=\u0027#Domain_Knowledge\u0027\u003e1. Domain Knowledge\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#EDA\u0027\u003e2. EDA\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Create_a_Holdout-Set\u0027\u003e3. Create a Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train-Test-Split_Model\u0027\u003e4. Train-Test-Split Model\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Error_Analysis\u0027\u003e5. Error Analysis\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Cross_Validation\u0027\u003e6. 5 Fold Cross Validation\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train\u0027\u003e7. Train the Final Model on all data\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Evaluate\u0027\u003e8. Evaluate the Final Model on the Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Convert\u0027\u003e9. Convert the Final Model to Tensorflow.js\u003c/a\u003e\u003cbr\u003e\\n\\n\u003ca href=\u0027#Citations\u0027\u003eCitations\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Reference_Kernels\u0027\u003eReference Kernels\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Helpful_Resources\u0027\u003eHelpful Resources\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Conclusion\u0027\u003eConclusion\u003c/a\u003e\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# set seeds to ensure repeatability of results\\nfrom numpy.random import seed\\nseed(101)\\nfrom tensorflow import set_random_seed\\nset_random_seed(101)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport os\\n\\nimport cv2\\nimport tensorflow\\n\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nimport itertools\\nimport shutil\\nimport pickle\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nfrom sklearn.model_selection import KFold, StratifiedKFold\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import classification_report\\n\\n\\nimport tensorflow\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.metrics import categorical_crossentropy\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\\nfrom tensorflow.keras.metrics import binary_accuracy\\n\\n\\n# Don\u0027t Show Warning Messages\\nimport warnings\\nwarnings.filterwarnings(\u0027ignore\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\nIMAGE_HEIGHT = 96\\nIMAGE_WIDTH = 96\\n\\nNUM_HOLDOUT_IMAGES = 200\\n\\nNUM_EPOCHS = 10\\nNUM_FOLDS = 5\\n\\nPADDING = 10\\nBATCH_SIZE = 10\\n\\n\\nNUM_FINAL_MODEL_EPOCHS = 10\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Domain_Knowledge\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 1. Domain Knowledge\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ciframe width=\\\u0022560\\\u0022 height=\\\u0022315\\\u0022 src=\\\u0022https://www.youtube.com/embed/2O3YrdUZQ5U?rel=0\\\u0022 frameborder=\\\u00220\\\u0022 allow=\\\u0022accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\u0022 allowfullscreen\u003e\u003c/iframe\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n\\n### Quick Facts\\n\\n- Malaria is a disease that infects and destroys red blood cells. \\n- Doctors analyze thick and thin blood smears to diagnose malaria and determine how severe it is.\\n- The parasite that causes malaria is called Plasmodium.\\n- The female Anopheles mosquito is the only mosquito that transmits malaria.\\n\\n\\n### Background\\n\\nMicroscopic examination of thick and thin blood smears is the easiest and most reliable test for malaria. Blood smears are often taken from a finger prick.\\n\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/glassslide.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 1. Microscope and glass slide\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/raw_image.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 2. Cells seen under a microscope. \u003cbr\u003e\\nThin blood smear.\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/parasitized.png\\\u0022 width=\\\u0022200\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 3. Segmented parasitized cell\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n*Malaria diagnosis involves the following*:\u003cbr\u003e\\n\u003e - Determine if the malaria parasite is present.\\n\u003e - Determine the parasite species.\\n\u003e - Determine parasite density by counting the number of cells that are infected.\\n\\n\u003cbr\u003e\\n\\n*Thick Blood Smear*\u003cbr\u003e\\n\\nA thick blood smear is a drop of blood on a glass slide. It\u0027s used to determine if there are any parasites in the blood. It\u0027s a larger blood sample because there could only be a few parasites present so a larger sample is needed to detect them. If no parasites are found the patient will have repeated blood smears every 8 hours for a few of days to confirm that there\u0027s no malaria infection.\\n\\n\u003cbr\u003e\\n\\n*Thin Blood Smear*\u003cbr\u003e\\n\\nA thin blood smear is a drop of blood that\u0027s spread over a large area of the glass slide. This blood smear helps doctor\u0027s discover what species of parasite is causing the infection.\\n\\n\u003cbr\u003e\\n\\n*Plasmodium*\u003cbr\u003e\\n\\nThis is the parasite that causes malaria. There are many species of Plasmodium. Five species cause malaria - P. vivax, P. ovale, P. malariae, P. knowlesi and P. falciparum. Most deaths are caused by P. falciparum. The other species usually cause a milder form of malaria.\\n\\n\u003cbr\u003e\\n\\n*Parasitemia*\u003cbr\u003e\\n\\nThis is the percentage of red blood cells that are infected by malaria (parasite density). This is computed by counting the number of infected cells visible under a microscope. This number helps doctors determine how severe the disease is. They then prescribe a treatment based on the severity. For example, if a large percentage of blood cells is infected medicine may be given directly into a vein instead of by mouth.\\n\\n\u003cbr\u003e\\n**Sources**\\n\\n- uofmhealth.org\u003cbr\u003e\\nhttps://www.uofmhealth.org/health-library/hw118744\\n\\n- Wikipedia Malaria\u003cbr\u003e\\nhttps://en.wikipedia.org/wiki/Malaria\\n\\n- Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027EDA\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 2. EDA\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### How many files are in each folder?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nuninfected_list = os.listdir(path_uninfected)\\nparasitized_list = os.listdir(path_parasitized)\\n\\nprint(\u0027Uninfected: \u0027, len(uninfected_list))\\nprint(\u0027Parasitized: \u0027, len(parasitized_list))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The data description says there should be 27,558 cell images i.e. 13779 files per folder. We see that each folder has an extra file. Later we\u0027ll check what kind of file this is.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Let\u0027s take a quick look at some images from each class\\n\\nThere are two classes\\n- uninfected\\n- parasitized.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = uninfected_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = parasitized_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Check if any non image files are in the folders\\n\\nWe see that there are actually 13780 files in each folder and not 13779 as expected. Folders can sometimes contain files that are automatically created during processing. These files can cause errors in the code if we don\u0027t know they exist - like the code we wrote above to display the images. Let\u0027s check if there are any non image files in the folders. \u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\n# sample image name: C140P101ThinF_IMG_20151005_211735_cell_159.png\\n\\nfor item in uninfected_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Uninfected folder: \u0027, item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\nfor item in parasitized_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Parasitized folder: \u0027,item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We see that each folder has one non image file called Thumbs.db. Now that we know that these non image files exist we will be sure exclude them later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Put the image names into dataframes\\n\\nHere we will create a dataframe called df_combined that includes both uninfected and parasitized images. This new dataframe will have a column showing the target class of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# create the dataframe\\ndf_uninfected = pd.DataFrame(uninfected_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_uninfected = df_uninfected[df_uninfected[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_uninfected[\u0027target\u0027] = 0\\n\\n\\n# create the dataframe\\ndf_parasitized = pd.DataFrame(parasitized_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_parasitized = df_parasitized[df_parasitized[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_parasitized[\u0027target\u0027] = 1\\n\\n#print(df_uninfected.shape)\\n#print(df_parasitized.shape)\\n\\n# Combine the two dataframes\\n\\ndf_combined = pd.concat([df_uninfected, df_parasitized], axis=0).reset_index(drop=True)\\n\\n#df_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_combined.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shape.\\n# There should be 27558 rows.\\n\\ndf_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if the image names are unique.\\n# The output should be 27558\\n\\ndf_combined[\u0027image_id\u0027].nunique()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The output above matches the number of rows. This confirms that each image has a unique name. This is important to verify because duplicate image file names will cause errors in the processing code that we will write later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### What are the image sizes and how many channels does each have?\\n\\nHere we will add the following info on each image to the df_combined dataframe:\\n\\nw = width\u003cbr\u003e\\nh = height\u003cbr\u003e\\nc = number of channels\u003cbr\u003e\\nmax_pixel_value\u003cbr\u003e\\nmin_pixel_value\u003cbr\u003e\\nimage_format\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndef read_image_sizes(file_name):\\n    \\\u0022\\\u0022\\\u0022\\n    1. Get the shape of the image\\n    2. Get the min and max pixel values in the image.\\n    Getting pixel values will tell if any pre-processing has been done.\\n    3. This info will be added to the original dataframe.\\n    \\\u0022\\\u0022\\\u0022\\n    \\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n     \\n    if file_name in uninfected_list:\\n        \\n        path = path_uninfected\\n        \\n    else:\\n        path = path_parasitized\\n    \\n    \\n    image = cv2.imread(path + file_name)\\n    max_pixel_val = image.max()\\n    min_pixel_val = image.min()\\n    img_format = file_name.split(\u0027.\u0027)[1]\\n    output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val, img_format]\\n    return output\\n\\nm = np.stack(df_combined[\u0027image_id\u0027].apply(read_image_sizes))\\ndf = pd.DataFrame(m,columns=[\u0027w\u0027,\u0027h\u0027,\u0027c\u0027,\u0027max_pixel_val\u0027,\u0027min_pixel_val\u0027, \u0027image_format\u0027])\\n\\ndf_combined = pd.concat([df_combined,df],axis=1, sort=False)\\n\\ndf_combined.head(10)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images have 3 channels\\ndf_combined[\u0027c\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images are in png format\\ndf_combined[\u0027image_format\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all black images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 0) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 0)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all white images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 255) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 255)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Display random images from each of the target classes\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The images are randomly selected. Therefore, different images will be dispayed each time the code is run. Also you\u0027ll see that parasitized images seem to have a blue-ish area that\u0027s not as common in uninfected images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_uninfected[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_parasitized[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### EDA Summary\\n\\n- There are 27,558 segmented cell images.\\n- Each folder contains 13,779 images.\\n- The images are of various sizes.\\n- All images are in png format.\\n- All images have 3 channels i.e. all are colour images.\\n- There are no all black or all white images.\\n- The target distribution is balanced i.e. each target class has the same number of images.\\n- All cells were segmented from thin blood smear slides.\\n- The parasite specie on all parasitized images is P. falciparum.\\n- Each folder contains a non image file called Thumbs.db.\\n- The presence of a blue-ish area inside the cell appears to be a common (but not definitive) indicator that a cell is parasitized.\\n\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Create_a_Holdout-Set\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 3. Create a Holdout Set\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As we continue to train, validate and modify a model we could over time start to overfit the validation data without realizing it. This means that the model will perform well during training but it will perform poorly on unseen data i.e. the app will perform badly in production.\\n\\nHere we will create a holdout set containing 200 images. We will keep this holdout set aside and only use it at the end to check how the final model performs on unseen data.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# shuffle\\ndf_combined = shuffle(df_combined, random_state=101)\\n\\n# create a holdout set with 200 samples\\ndf_holdout = df_combined.sample(NUM_HOLDOUT_IMAGES, random_state=101)\\n\\n# create a list of holdout images\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_holdout.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shapes.\\n# The ouput should be:\\n# (200, 8)\\n# (27358, 8)\\n\\nprint(df_holdout.shape)\\nprint(df_data.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution in the holdout set.\\n# 0 = uninfected\\n# 1 = parasitized\\n\\ndf_holdout[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a holdout set containing 200 images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train-Test-Split_Model\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 4. Train-Test-Split Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Train-test-split is not the ideal way to assess model perfromance. However, it\u0027s a good starting point because it\u0027s simple to set up and runs 5 times faster than 5 fold cross validation. This helps us to:\\n\\n- Quickly get a feel for what kind of performance we can expect from this data.\\n- Quickly test different architectures and parameters.\\n- Establish the workflow that will later be used in cross validation.\\n- Check the training curves to see if the model is overfitting.\\n- Check the training curves to establish the number of epochs we will use when training the final model on all data.\\n- Perform error analysis.\\n\\nA train-test-split model is a rough guide. When deciding if a particular change actually improved model performance we will base that decision on cross validation results. This is important. If we don\u0027t use cross validation we may think we are improving but in reality we may be getting nowhere.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Directory Structure\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022To reduce RAM use and prevent this kernel from crashing we will batch feed the images when training the model. We will use generators to do this. In order to use this approach Keras requires that a particular directory structure be set up. Keras uses this structure to automatically infer the class (target) of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check if base_dir has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# see what\u0027s inside base_dir\\nos.listdir(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Train and Val Sets\\n\\nWe will create a stratified val set. This means that the val set will have the same target distribution as the train set. Actually doing stratification isn\u0027t essential here because the target is balanced. Because of this there\u0027s a good chance that randomly selecting rows will still result in the val set having a fairly balanced target distribution. Stratification is more applicable for data where the target is unbalanced.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# select the column that we will use for stratification\\ny = df_data[\u0027target\u0027]\\n\\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\\n\\nprint(df_train.shape)\\nprint(df_val.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution of the val set.\\n# The target should be approx balanced.\\n\\ndf_val[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Transfer the images into the folders\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n# Get a list of images in each of the two folders\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n        \\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n        \\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the number of images in each folder\\n\\n# train\\nprint(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\nprint(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Set Up the Generators\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Note that here we are normalizing the images inside the generator.\\n# If you wanted to add some data augmentation you could do it here.\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\nval_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Model Architecture\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022I found this cnn architecture a while ago in a kernel created by @fmarazzi. I\u0027ve since used this on several projects. It really is a good multi-purpose architecture. Here I\u0027ve modified it slightly by adding a ZeroPadding layer. Later, in the error analysis section, I\u0027ll explain why I added this layer.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\nmodel.summary()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Train the Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Please note that we\u0027ve set the learning rate to reduce (decay) at each epoch.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022filepath = \\\u0022model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n                              \\n                              \\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            validation_data=val_gen,\\n                            validation_steps=val_steps,\\n                            epochs=NUM_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Evaluate the model using the val set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# get the metric names so we can use evaulate_generator\\nmodel.metrics_names\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Here the best epoch will be used.\\n\\nmodel.load_weights(\u0027model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\nprint(\u0027val_loss:\u0027, val_loss)\\nprint(\u0027val_acc:\u0027, val_acc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Plot the Training Curves\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# display the loss and accuracy curves\\n\\nimport matplotlib.pyplot as plt\\n\\nacc = history.history[\u0027acc\u0027]\\nval_acc = history.history[\u0027val_acc\u0027]\\nloss = history.history[\u0027loss\u0027]\\nval_loss = history.history[\u0027val_loss\u0027]\\n\\nepochs = range(1, len(acc) + 1)\\n\\nplt.plot(epochs, loss, \u0027bo\u0027, label=\u0027Training loss\u0027)\\nplt.plot(epochs, val_loss, \u0027b\u0027, label=\u0027Validation loss\u0027)\\nplt.title(\u0027Training and validation loss\u0027)\\nplt.legend()\\nplt.figure()\\n\\nplt.plot(epochs, acc, \u0027bo\u0027, label=\u0027Training acc\u0027)\\nplt.plot(epochs, val_acc, \u0027b\u0027, label=\u0027Validation acc\u0027)\\nplt.title(\u0027Training and validation accuracy\u0027)\\nplt.legend()\\nplt.figure()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the labels of the test images.\\n\\ntest_labels = test_gen.classes\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# We need these to plot the confusion matrix.\\ntest_labels\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the label associated with each class\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# If you wanted to get the image_id\u0027s to match them to predictions this\\n# is how to do it.\\n\\n# test_gen.filenames\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check the number of predictions\\npredictions.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Source: Scikit Learn website\\n# http://scikit-learn.org/stable/auto_examples/\\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\\n# selection-plot-confusion-matrix-py\\n\\n\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\u0027Confusion matrix\u0027,\\n                          cmap=plt.cm.Blues):\\n    \\\u0022\\\u0022\\\u0022\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\u0022\\\u0022\\\u0022\\n    if normalize:\\n        cm = cm.astype(\u0027float\u0027) / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\u0022Normalized confusion matrix\\\u0022)\\n    else:\\n        print(\u0027Confusion matrix, without normalization\u0027)\\n\\n    print(cm)\\n\\n    plt.imshow(cm, interpolation=\u0027nearest\u0027, cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n\\n    fmt = \u0027.2f\u0027 if normalize else \u0027d\u0027\\n    thresh = cm.max() / 2.\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\n        plt.text(j, i, format(cm[i, j], fmt),\\n                 horizontalalignment=\\\u0022center\\\u0022,\\n                 color=\\\u0022white\\\u0022 if cm[i, j] \u003e thresh else \\\u0022black\\\u0022)\\n\\n    plt.ylabel(\u0027True label\u0027)\\n    plt.xlabel(\u0027Predicted label\u0027)\\n    plt.tight_layout()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_labels.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Error_Analysis\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5. Error Analysis\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022In this section we\u0027re not going to use any fancy statistical gymnastics. We\u0027re simply going to look at the images that the model predicted correctly and those it got wrong. We want to see if there are any patterns or issues that could\u0027ve caused the model to make mistakes.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Put the val image_id, labels and predictions into a dataframe.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# put the val image_id, labels and predictions into a dataframe\\n\\nval_pred_dict = {\\n    \u0027image_id\u0027: test_gen.filenames,\\n    \u0027val_labels\u0027: test_gen.classes,\\n    \u0027val_preds\u0027: predictions.argmax(axis=1)\\n}\\n\\ndf_val_preds = pd.DataFrame(val_pred_dict)\\n\\n\\n# Adjust the file names\\n\\n# sample image name: a_uninfected/C100P61ThinF_IMG_20150918_144104_...\\n# we want just this part: C100P61ThinF_IMG_20150918_144104_...\\n\\ndef adjust_file_names(x):\\n    # split into a list based on \u0027/\u0027\\n    fname = x.split(\u0027/\u0027)\\n    # chose the second item in the list which is the image name\\n    fname = fname[1]\\n    \\n    return fname\\n\\ndf_val_preds[\u0027image_id\u0027] = df_val_preds[\u0027image_id\u0027].apply(adjust_file_names)\\n\\n\\n# savedf_val_preds so we can analyze the results later\\npickle.dump(df_val_preds,open(\u0027df_val_preds.pickle\u0027,\u0027wb\u0027))\\n\\n# code to load the dataframe\\n# df_val_preds = pickle.load(open(\u0027df_val_preds\u0027,\u0027rb\u0027))\\n\\n\\n#df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# filter out those rows where the model made correct predictions\\ndf_correct = df_val_preds[df_val_preds[\u0027val_labels\u0027] == df_val_preds[\u0027val_preds\u0027]]\\n\\n# filter out those rows where the model made wrong predictions\\ndf_wrong = df_val_preds[df_val_preds[\u0027val_labels\u0027] != df_val_preds[\u0027val_preds\u0027]]\\n\\nprint(df_correct.shape)\\nprint(df_wrong.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the correct predictions\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_correct.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_correct[df_correct[\u0027val_labels\u0027] == 1]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 1\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_correct[df_correct[\u0027val_labels\u0027] == 0]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 0\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the wrong predictions\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized but the model predicted uninfected\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_wrong[df_wrong[\u0027val_labels\u0027] == 1]\\n\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 0\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected but the model predicted parasitized\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_wrong[df_wrong[\u0027val_labels\u0027] == 0]\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 1\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Observations and Comments\\n\\nIn cases where the label was parasitized but the model predicted uninfected I noticed that the blue-ish area was located close to the edge of the image. Could it be that the model was not detecting important patterns that are located at the edge of images? To address this issue I decided to add a padding layer to the model arcitecture. This layer added a 10 pixel zero padding around each image. I found that the cross validation scores improved after this change.\\n\\n\\nWhen doing this error analysis our premise has been that if there is a mismatch between the label and the prediction then the model has made a mistake. However, there\u0027s another possibility - the model prediction is correct. There are 27,558 images in this dataset. All images were examined and labeled by the same expert. This must\u0027ve been a tedious and time consuming process so it\u0027s possible that some images were incorrectly labeled. \\n\\nAt this point it would be helpful to collaborate with a domain expert in order to discuss possible reasons for the mistakes this model is making. Are they the result of model weakness, incorrect labels or maybe damaged images? Is the correct label easy for a human to classify or would a human struggle to make a correct diagnosis? What are the main indicators that an expert looks for when examining cell images? Is the model seeing things that an expert didn\u0027t notice?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Cross_Validation\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5 Fold Cross Validation\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Here we will simply apply the same workflow that we used for the train-test-split model to 5 folds. For each fold we\u0027ll get the loss, accuracy and auc. Then we\u0027ll average the results of the 5 folds to get the final scores.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# ==============================\\n# Create the 5 Folds\\n# ==============================\\n\\n# shuffle df_combined and change the name to df_data\\ndf_data = shuffle(df_combined.copy())\\n\\n# train_test_split\\ny = df_data[\u0027target\u0027]\\n\\n# initialize kfold\\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=101)\\n\\n# define y for stratification\\ny = df_data[\u0027target\u0027]\\n\\n# Note:\\n# Each fold is a tuple ([train_index_values], [val_index_values])\\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df_train, y)\\n\\n# Put the folds into a list. This is a list of tuples.\\n# y was set above.\\nfold_list = list(kf.split(df_data, y))\\n\\n\\n# ==============================\\n# Loop Through the Folds\\n# ==============================\\n\\n# create a list to store the predictions\\nval_pred_list = []\\n\\n# create a list to store the scores\\nval_acc_list = []\\nval_loss_list = []\\nval_auc_list = []\\n\\n\\nfor i, fold in enumerate(fold_list):\\n\\n    # Delete the image data directory we created to prevent a Kaggle error.\\n    # Kaggle allows a max of 500 files to be saved.\\n    \\n    if os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n        \\n        \\n    \\n    # set df_data\\n    df_data = df_combined.copy()\\n    \\n    print(\u0027=== Fold_\u0027 + str(i) + \u0027 ===\u0027)\\n    print(\u0027\\\\n\u0027)\\n\\n    # map the train and val index values to dataframe rows\\n    df_train = df_data[df_data.index.isin(fold[0])]\\n    df_val = df_data[df_data.index.isin(fold[1])]\\n    \\n\\n\\n\\n    # ==============================\\n    # Create a Directory Structure\\n    # ==============================\\n\\n    # Create a new directory\\n    base_dir = \u0027base_dir\u0027\\n    os.mkdir(base_dir)\\n\\n\\n    #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n    # now we create 2 folders inside \u0027base_dir\u0027:\\n\\n    # train\\n        # a_uninfected\\n        # b_parasitized\\n\\n    # val\\n        # a_uninfected\\n        # b_parasitized\\n\\n\\n    # create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n    # train_dir\\n    train_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\n    os.mkdir(train_dir)\\n\\n    # val_dir\\n    val_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\n    os.mkdir(val_dir)\\n\\n\\n    # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n    # Inside each folder we create seperate folders for each class\\n\\n    # create new folders inside train_dir\\n    a_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n    # create new folders inside val_dir\\n    a_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n\\n    # =================================\\n    # Transfer the Images into Folders\\n    # =================================\\n\\n    # Set the image_id as the index in df_data\\n    df_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n    # Get a list of images in each of the two folders\\n\\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n    folder_1 = os.listdir(path_uninfected)\\n    folder_2 = os.listdir(path_parasitized)\\n\\n    # Get a list of train and val images\\n    train_list = list(df_train[\u0027image_id\u0027])\\n    val_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n    # Transfer the train images\\n\\n    for image in train_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n\\n\\n    # Transfer the val images\\n\\n    for image in val_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n    # Print the number of images in each folder\\n\\n    # train\\n    #print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n    # val\\n    #print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n    #print(\u0027\\\\n\u0027)\\n\\n\\n    # ==============================\\n    # Set Up the Generators\\n    # ==============================\\n\\n    train_path = \u0027base_dir/train_dir\u0027\\n    valid_path = \u0027base_dir/val_dir\u0027\\n\\n    num_train_samples = len(df_train)\\n    num_val_samples = len(df_val)\\n    train_batch_size = BATCH_SIZE\\n    val_batch_size = BATCH_SIZE\\n\\n\\n    train_steps = np.ceil(num_train_samples / train_batch_size)\\n    val_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\n    datagen = ImageDataGenerator(rescale=1.0/255)\\n\\n    train_gen = datagen.flow_from_directory(train_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=train_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    val_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    # Note: shuffle=False causes the test dataset to not be shuffled\\n    test_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027,\\n                                            shuffle=False)\\n    \\n    print(\u0027\\\\n\u0027)\\n\\n    # ==============================\\n    # Set Up the Model Architecture\\n    # ==============================\\n\\n\\n\\n    kernel_size = (3,3)\\n    pool_size= (2,2)\\n    first_filters = 32\\n    second_filters = 64\\n    third_filters = 128\\n\\n    dropout_conv = 0.3\\n    dropout_dense = 0.3\\n\\n\\n    model = Sequential()\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                     input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n    \\n    model.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n    \\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size)) \\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Flatten())\\n    model.add(Dense(256, activation = \\\u0022relu\\\u0022))\\n    model.add(Dropout(dropout_dense))\\n    model.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n    #model.summary()\\n\\n\\n\\n    # ==============================\\n    # Train the Model\\n    # ==============================\\n\\n\\n    model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n                  metrics=[\u0027accuracy\u0027])\\n\\n    filepath = \\\u0022model.h5\\\u0022\\n    checkpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                                 save_best_only=True, mode=\u0027max\u0027)\\n\\n    reduce_lr = ReduceLROnPlateau(monitor=\u0027val_acc\u0027, factor=0.5, patience=2, \\n                                       verbose=1, mode=\u0027max\u0027, min_lr=0.00001)\\n\\n\\n    callbacks_list = [checkpoint, reduce_lr]\\n\\n    history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                                validation_data=val_gen,\\n                                validation_steps=val_steps,\\n                                epochs=NUM_EPOCHS, verbose=1,\\n                               callbacks=callbacks_list)\\n\\n\\n\\n    # ==================================\\n    # Evaluate the Model on the Val Set\\n    # ==================================\\n\\n    model.load_weights(\u0027model.h5\u0027)\\n\\n    val_loss, val_acc = \\\\\\n    model.evaluate_generator(test_gen, \\n                            steps=val_steps)\\n    \\n    # append the acc score val_scores_list\\n    val_acc_list.append(val_acc)\\n    val_loss_list.append(val_loss)\\n    \\n    \\n    # ==================================\\n    # Calculate the AUC Score\\n    # ==================================\\n\\n    test_labels = test_gen.classes\\n\\n    # make a prediction\\n    predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n    \\n    # append the predictions to a list\\n    val_pred_list.append(predictions)\\n    \\n    val_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n    \\n    val_auc_list.append(val_auc)\\n    \\n    \\n    \\n    # ==================================\\n    # Print the Fold Scores\\n    # ==================================\\n    \\n    \\n    #print(\u0027\\\\n\u0027)\\n    #print(\u0027Fold_\u0027 + str(i) + \u0027 scores:\\\\n\u0027)\\n    #print(\u0027val_loss:\u0027, val_loss)\\n    #print(\u0027val_acc:\u0027, val_acc)\\n    #print(\u0027val_auc:\u0027, val_auc)\\n    #print(\u0027\\\\n\u0027)\\n\\n    \\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Average for of all 5 folds:\\\\n\u0027)\\n#print(\u0027Average Accuracy: \u0027, avg_acc)\\n#print(\u0027Average Loss: \u0027, avg_loss)\\n#print(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Print the scores for each fold and the average scores\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Print the scores\\n\\nprint(\u0027Val Acc\u0027)\\nfor item in val_acc_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val Loss\u0027)\\nfor item in val_loss_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val AUC\u0027)\\nfor item in val_auc_list:\\n    print(item)\\n\\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Average for of all 5 folds:\\\\n\u0027)\\nprint(\u0027Average Accuracy: \u0027, avg_acc)\\nprint(\u0027Average Loss: \u0027, avg_loss)\\nprint(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 7. Train the Final Model using all the data\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We\u0027re not using validation data in this final model. Therefore, we won\u0027t be able to set up the model to save the best epoch. We\u0027ll need to determine the number of training epochs before starting.\\n\\nTo determine this number I would normally look at the training curves (see train-test-split model) and determine the epoch at which the model started to overfit i.e. the training and validation accuracy curves start to diverge. However, with this data and architecture it appears that overfitting is not a huge problem because the training and validation curves do not diverge significantly. Therefore choosing 10 epochs seems to give a good balance between training time and model quality.\\n\\nIt\u0027s also important to keep in mind that we can train on all data because we set the learning rate to decay at each epoch i.e. the learning rate was scheduled. If we had used a dynamic learning rate (e.g. ReduceLROnPlateau) then we\u0027ll need to have some way of replicating the learning rate changes that happened automatically during 5 fold cross validation. Therefore, using a scheduled learning rate keeps things simple.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n\\n\\n\\n# ==============================\\n# Set df_train\\n# ==============================\\n\\n# This variable was set above. Just setting it here again for clarity.\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\\ndf_train = df_data.copy()\\ndf_val = df_holdout.copy()\\n\\n\\n\\n# ==============================\\n# Create a Directory Structure\\n# ==============================\\n\\n# Create a new directory\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n\\n# =================================\\n# Transfer the Images into Folders\\n# =================================\\n\\n# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n# Set the image_id as the index in df_data\\ndf_holdout.set_index(\u0027image_id\u0027, inplace=True)\\n\\n\\n\\n# Get a list of images in each of the two folders\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n\\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n\\n    fname = image\\n    target = df_holdout.loc[image,\u0027target\u0027]\\n\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n# Print the number of images in each folder\\n\\n# train\\n#print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\n#print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n#print(\u0027\\\\n\u0027)\\n\\n\\n# ==============================\\n# Set Up the Generators\\n# ==============================\\n\\ntrain_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\\n\\nprint(\u0027\\\\n\u0027)\\n\\n# ==============================\\n# Set Up the Model Architecture\\n# ==============================\\n\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n#model.summary()\\n\\n\\n\\n# ==============================\\n# Train the Model\\n# ==============================\\n\\n\\nmodel.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\\n\\n# we are saving the model based on training accuracy\\nfilepath = \\\u0022final_model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n\\n\\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            epochs=NUM_FINAL_MODEL_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\\n\\n\\n# ==================================\\n# Evaluate the Model on the Val Set\\n# ==================================\\n\\nmodel.load_weights(\u0027final_model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\n\\n# ==================================\\n# Calculate the AUC Score\\n# ==================================\\n\\ntest_labels = test_gen.classes\\n\\n# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n\\n\\nval_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n\\n\\n\\n\\n# ==================================\\n# Print the Scores\\n# ==================================\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Accuracy: \u0027, val_acc)\\n#print(\u0027Loss: \u0027, val_loss)\\n#print(\u0027AUC: \u0027, val_auc)\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Evaluate\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 8. Evaluate the Final Model on the Holdout Set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# ==================================\\n# Print the Scores\\n# ==================================\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Accuracy: \u0027, val_acc)\\nprint(\u0027Loss: \u0027, val_loss)\\nprint(\u0027AUC: \u0027, val_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\\n\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\\n\\n\\n# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a trained model that can be incorporated into the web app. The metrics we calculated above all look very good meaning that the model should perform well on unseen data.\\n\\nAlso, because we\u0027ve used a simple architecture the model size will be less than 10 MB. This means that it will download quickly. Therefore, the web page will load fast and the overall user experience will be good.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Convert\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 9. Convert the final model from Keras to Tensorflow.js\\n\\nThis conversion needs to be done so that the model can be loaded into the web app.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# --ignore-installed is added to fix an error.\\n\\n# https://stackoverflow.com/questions/49932759/pip-10-and-apt-how-to-avoid-cannot-uninstall\\n# -x-errors-for-distutils-packages\\n\\n!pip install tensorflowjs --ignore-installed\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Use the command line conversion tool to convert the model\\n\\n!tensorflowjs_converter --input_format keras final_model.h5 tfjs/model\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the folder containing the tfjs model files has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the tfjs files exist\\nos.listdir(\u0027tfjs/model\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Delete the images that were moved around\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Citations\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Citations\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n- Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images. PeerJ6:e4568\u003cbr\u003e\\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude, RJ, Jaeger S, Thoma GR. (2018)\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Reference_Kernels\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Reference Kernels\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Gabriel Preda, Honey Bee Subspecies Classification\u003cbr\u003e\\nhttps://www.kaggle.com/gpreda/honey-bee-subspecies-classification\\n\\n- Francesco Marazzi, Baseline Keras CNN\u003cbr\u003e\\nhttps://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\\n\\n- Kimmo Sskilahti, Image processing with scikit-image\u003cbr\u003e\\nhttps://www.kaggle.com/ksaaskil/image-processing-with-scikit-image\\n\\n- Marsh, Skin Lesion Analyzer\u003cbr\u003e\\nhttps://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Helpful_Resources\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Helpful Resources\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Excellent tutorial series by deeplizard on how to use Tensorflow.js to build a web app.\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\\n\\n- Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\\n\\n- Tutorial by Data School on how to evaluate a classifier\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\\n\\n- Tensorflow.js gallery of projects\u003cbr\u003e\\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Conclusion\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Conclusion\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This is the Foldscope Origami Microscope.\u003cbr\u003e\\nhttps://youtu.be/ky-cqSI5mwE\\n\\nThe microscope image could perhaps be photographed using phone camera and then processed using a web app.\\n\\n\\nThis was a software solution. However, there could be some who are reading this who may be interested in combining the power of Ai with electronics - by building robots, analyzers and other devices. You probably don\u0027t know where to start. The good news is that if you can code then you can also build electronic devices. The principles are the same. It\u0027s just that one uses software and the other uses physical components. \\n\\nThese two practical Udemy courses are a great place to start your journey towards world domination. They are geared towards young students and, most importantly, the instructor is an excellent teacher. The courses were developed with deaf students in mind.\\n\\nElectronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/analog-electronics-robotics-learn-by-building/\\n\\nDigital Electronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/digital-electronics-robotics-learn-by-building-module-ii/\\n\\n\\nMany thanks to Arunava for making this interesting dataset available on Kaggle.\\n\\nThank you for reading.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022Python 3\u0022,\u0022language\u0022:\u0022python\u0022,\u0022name\u0022:\u0022python3\u0022},\u0022language_info\u0022:{\u0022codemirror_mode\u0022:{\u0022name\u0022:\u0022ipython\u0022,\u0022version\u0022:3},\u0022file_extension\u0022:\u0022.py\u0022,\u0022mimetype\u0022:\u0022text/x-python\u0022,\u0022name\u0022:\u0022python\u0022,\u0022nbconvert_exporter\u0022:\u0022python\u0022,\u0022pygments_lexer\u0022:\u0022ipython3\u0022,\u0022version\u0022:\u00223.6.4\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2019-05-28T04:38:47.9095886Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e4594240\u003c/disk_kb_free\u003e\u003cdocker_image_digest\u003e742fa26e1d0b8e816011492dfa78fe0213a1f3cb60b57c8b6ef07909925a70b7\u003c/docker_image_digest\u003e\u003cdocker_image_id\u003esha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-private-byod/python\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cinvalid_path_errors\u003eFalse\u003c/invalid_path_errors\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e11065.347707848\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/gpu.Dockerfile","dockerHubUrl":"https://gcr.io/kaggle-gpu-images/python","dockerImageDigest":"742fa26e1d0b8e816011492dfa78fe0213a1f3cb60b57c8b6ef07909925a70b7","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-gpu-images/python","diskKbFree":4594240,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":11065.347707848,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"dockerImageVersionId":25160,"usedCustomDockerImage":false,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":""}],"useNewKernelsBackend":null,"isGpuEnabled":true,"isTpuEnabled":false,"isInternetEnabled":true},"author":{"id":1086574,"displayName":"Marsh","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","profileUrl":"/vbookshelf","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":2,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isKaggleBot":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"isPhoneVerified":false},"baseUrl":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app","collaborators":{"owner":{"userId":1086574,"groupId":null,"groupMemberCount":null,"profileUrl":"/vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","name":"Marsh","slug":"vbookshelf","userTier":2,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook__.ipynb to notebook\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.6876314699911745\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Executing notebook with kernel: python3\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.90076449199114\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.301794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 200.85943224398943\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.304988: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc427a7020 executing computations on platform Host. Devices:\\n2019-05-28 04:42:10.305032: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): \u003cundefined\u003e, \u003cundefined\u003e\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 200.8657584189932\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.476988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.0341933269956\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.478124: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc4285b760 executing computations on platform CUDA. Devices:\\n2019-05-28 04:42:10.478440: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.03845160298806\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.485884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\\npciBusID: 0000:00:04.0\\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\\n2019-05-28 04:42:10.485930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.0499695509934\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:11.211375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\\n2019-05-28 04:42:11.211458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \\n2019-05-28 04:42:11.211476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.76856156899885\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:11.218924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15121 MB memory) -\u003e physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.77562138000212\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:12.999853: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 203.55696834999253\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 1821.7109596949886\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 9013.67850043399\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 10980.866831804\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Writing 1968811 bytes to __notebook__.ipynb\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11060.63353391699\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook__.ipynb to html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11063.305635205994\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Support files will be in __results___files/\\n[NbConvertApp] Making directory __results___files\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.712728618993\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.717234862997\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Writing 555278 bytes to __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.729104304992\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":32169077,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard2of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard2of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lVWW8fJDfDhP_QH84jKWSQ.TRrs1hy06z2HLvHh7NM0JqrKyGaDooL2-PuCY693Q193hKtNhNxg9TyzreybB39bP6gdranC8xQN649iRXv09L51GgUtOWGzUlmOCUFZuU_LyM0oqZDuojLpOuTDLaEd8dRUQXq57sa1MFrSHzpGlDPlS7v4o9vtgwL7ibe1O-ILCSoC7C-ANSo8jEAMU_ky-Tpobc2Utl7J5SWaGMF2Nogxf07rLoDk1uGp2lHWon_c6s5t2FX-MQdOWEqoUG4b11yO5aIaDTVbpPOQzafdh08uEgoHSnTv0FUYVzD_hkecRQHKkG7IMM7ZKZ_mrWaZTebMleOcKLpEnDD5oWXevGZizqdURfGrwtZOQzj1sanTGDEHq05DAid50g_R43TdyxpwqVmnlama4wjR1Z5FRvpSaqGh_RG5xsnIw8lK_NUcjy9zIsIopMIl-mgvRfXj92t2sdqUPnq9W52gfgNd0NOpMZaeOhvmLa12SNRJ9KsrhBUQBRoAsRjImM46OnzGgnh_rLeO0crYRAJ3qxjUYEizSOpx6KZ3ZhwsvUuUzDO6sbTjQzMSJSsLXAM5nf1Rd-J6Ip5ErMLI08u9d1b02iJPuf0oJYvW6eHtMLlij0CIyLtDskTCmVJyzih3kObeLsBw4j7r7A3Fhc5awMnSP_EL_nnRO4F88bXJbKvVx8g.YBRUfgoYYwRSDkfdSk9BVg/tfjs/model/group1-shard2of3.bin","fileType":".bin","contentLength":4194304,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard2of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169078,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard1of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard1of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..WiD7ljU4zJJzCXZmIYID4g.g2GIiZuBUPU7o-xvzarv3qnHNqrixlBP55f99zF_7Y1wWwatx51W-9pxGpjh49XDjwibfiYuLwsMWitwyxeVlYSdwcjrZpXktfJZ3WeTpyZzm1ebMEwkcxjB4Qqe1wdSmlUjvgETkGu-Gwsu9B7Z569OmFBIN87OA4A4oKs11wyb9NakU2FEELpJlFHghwqo9OizC8ogP-qxMHeiQOCJpYobXQKBVAZazcN-sPCmm-X99nMQefdCPztACWHhbsSzXm1r4pnwhKNUsJ6zzL4IhvPP8DOcIeb4jkSA_Wmw-g5z_kTqpVicH6lRlYmtPZdtyGNA2TdtHSgeGH7NjtJToWgbsdtZweru3Ku-x2odNm3eVusytVjH8_RrGFf3bB2FCiZNWv4aYvGswAet5SQG42BUqiJIz95VwOgZMrY9_7koPALSwQyGRfn66RiN_ytIaldRlY2qrQV7Yfjt69W4qtKNJxOqr79OINWVGSk-JxmMRwHV-aG7pxqIs9XWKrj_zR2q1zDOshtfD3KTbIMVW1ExFSfYA_2p9PP9ApaEe86MzutZ8hpnN1oMinYKNDxrSNa2zqKzCfr_nlC7UTQObskL2S0PrxZ6cx5WVOq5bXBRIOdBf2aaps05BntpA6zoQ5n3oM7HnsDtv85MLNjaly8X9pg96V5C2ul0P0dd59I.6ly4S4FsvOjCQpGvrlEM4Q/tfjs/model/group1-shard1of3.bin","fileType":".bin","contentLength":4194304,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard1of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169079,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"model.h5","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/model.h5","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LRVrI4PINN63zY3wCl0PlA.lcxfMM503itGlJCPPw5CRoFLFIbWjINWwElVuu7g0Ga5G-lmywueFpyzYFSC3HE4ax19p2qmVZb8BaFMZziWwhvgITl7EVnCnVflNGmAj61ymj2xudifo5RSNvOp7n5s0-b4UjUB_v1PSLb23KWxiLi4Z0rzzuqGjWeqn86uBWfi35J8DNKmKOoTXaUZ42bzkL6cA24UTLNcvsFptljihhV7pg-taGUv_O5lkuNqM9eyGJTX4xhAI3EwlJluTHHY4q2owc8Ah8XCmWJqNoM6x_E3N5UENEq_Z1ldrcxXLeU-hy9l3wRp5pxU_XpYdQbwyG27Q9lvczeT2QdCzJ2VTxNPmcTUDREj9hDZLyAsbdMhpPg7ym6fbz53l3nCEYbryZshbnM7oYChowZjReYZy0ePm6NurLlYIZW--rMX5bJF8L2wnma4yg0Fjc5ZaeHNnogMvZP_mxqQx67M8ZjXayMRZzvxU0TkmisxWQXh3USFHO62Zx-WLwx41CZCH42aX-hGG_3HRdbsvws69Bxczicd5cURCz8R-X9yZ6yxZmbHa7lr3oA-_cCUFoFSJRUl6hnAB4pTOmq4apYfsgfZMu0AIu3S6bMKTkrAQFgNA9owTqUHjvi34FS7VsnwDeP3-Q1I3VZxoaxdQCRO4AsnmNbszhRA4zCxuUM_G497VNE.3yUfgPWFjIwXnkrC_qlVUQ/model.h5","fileType":".h5","contentLength":37715208,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"model.h5","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169080,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"final_model.h5","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/final_model.h5","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..dACm-iYru0dwus9_VgjV3A.MUmYStVpEgKjPo7iccka0YFL8CEzjtxn03JS6B2XBLsg28W04XXalRZdGWIefJF2Oh20JrGDyMuGanVHQqanRR7lUNMJM4zUMHxoBAdwK8rHtUV6dyrcsAcgmb2th4Vne1KQHX3oZRdEdGboKX1c3vIJe94X4bO_5IJDJ6N0TAN09jphXWhXGJMx6kjWGcMkDUYmKnRDxU4T61go5L-ub4bAUsedohygc_O5VnyIbPypVjwJPQ8Fze16suworvUEGEbkJq9pV6t2xZru80RqLVv7TwLsc66ZIVGwC8Re5ExIsd43Pq7RSkeB1KgLlz3BQ1kJDFbXq3togGq0oHbJcYYwJsjbLET1xvCbr-sZzp3AvGNhnqoHMy0Eas45f9WnLqWdasGmUkjYSoummnbbseK5puZ3uPilHVH5xaMeFHDwpit0FZ-m0lNIJdS3K0AeIUl34sJURjv0npgPvkYHNrTL1KShkvFkjPVvIf2wPROrrTSNCbLtvXqgxBeQanbdNkUjbp73Usfqc0QBCVnEU1kNJsOnRMSV0ybp0WuZwYGz-ij8psVkQuHf8SXfi8O0GxGRcz3XrkMTI9StJxOJbT4EYU3CIse5XTXLDwG1e4h8PAeBgqZENKX-O7pR3vqxDFopi1TNHWiLnSpU-lDBIE278IwbZBZG2XNlaTLSR1c.114t6UIYooOpweIgr1IKvA/final_model.h5","fileType":".h5","contentLength":37715208,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"final_model.h5","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169081,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"df_val_preds.pickle","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/df_val_preds.pickle","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..AqqX0iJHMyn3qHyUZ8WIqg.EbTVxjoImv7BzrgJMcgvKRySlxVF4uthis9atDkUOAi8XgVtNJf005R_S17bpuReiXLtoJpU1aJVUpqOltA5BpjdC2uLjOegUBLx4Fq7N2_r4etFenBpAbYmAdPvuLDK93Y3aRuQMVQ5Hotd1kMFZG7fYRBozMZoMbWP4sNdu_X6_Q8e_GL9VaccJU3pkuLhE43_N7J1DevO0w0OUj525_YlHiKTtqsTbSngqDv5vdMYYGcTqqy28DcoGuEaj0WYye98JbUqyTGGcySitQdWVcAS0oKdy-27FDGfofos0qp0warEA7KGJv55lazMaskiXjSyFTGRzk2v59D7-BGM1Z_0-PzN9rXabAKV1Q-7OOL2N1So6hktMiOse0n1cwwHTrzIK6Xh7UQtE5c0S8lZ9pd1f42494KI1G4PwUh85vHbPdW1RsX8VrKgfw3eVZRwYbK_2ZgIHeEfDyRCYIbrgGFGyJcMxio9ZdAwX2JrI4UbWDEgAt0ltOdFzBwyGVBVyIplxtIs-exXDfxtA1czxFzH5F-M-AFJOi_K_d3b9H2h3Xu72WwyL_qmcLw_u74Eb--9-3DHxi3U3q6wRkokJbTSEOpElpO-OkwOu_CS1Ta0lwgzzvAwM5y-74aPaeZa-313fh4O5lDWFJWHZ-h4Cu5-M7AjsblJjiMsec4GIt4.7zNmuNlG7NKcOeciyaSDKg/df_val_preds.pickle","fileType":".pickle","contentLength":274044,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"df_val_preds.pickle","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169086,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard3of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard3of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XjaZ3Mm2d920HGjZT1RgwA.SQlSKi2pA62l_Mw8Btgbz9fSafNcA0wn_hPwjTrK5AZ_NCJMb1JLpLMBLeOcg4Fca0-qmPCc27nGbiMQHuGWscJ1m2e-4jGTZ_2K42R2y3JSdC8UsfQ_8GD5q-M0VbSQ5BbuZdFkRFcRt0h9q91KKAMauLF0CBoI1BpYM0aVVQ8g3h6FRmvH0eWtIjtE95mmCa8-111Gua20a78GfYoZ6_XE4khpmW_Xcm-I5Qf2wuHuC1tNZ1wg88MXE6_Bn_5Tec83DDXtC-733VOPQAQ3b8I5YJqIQSs61xi8TO0yhloPleAGeb-NFHjGg2wxI8-Y1FxmB_rq7Z8vHFFn-ClFTq2i69OjkBoVXX6wcfqkWmhXEGnt3VouMWUNYnVruUyxMTYDD0DgOaNCpSZ3_JWNTSmNxjg1GVFcM5qFznyE7ofUKaI4oXvgXoGThReuugksghwyvwAR7UtX4HJUhXTGPefjNfPx-UurS2zOj4NRBTGs-wbP2nXxBc4yPdMBI4Y0xCQ9qOc6Wu-KZgCGP6h4CRdLqcUEG0I_JV6kINqYMwSaUij_3h8qOiyl7NAVhY3GsmDiowFoO6E3rFbPB8B08IxtcDXoRlqD0qNuMcgWsKWUiC-gxYFLq15_WMyDoi4xPD_466eqxU11valVIYGTKCKTDeQK0I8DKVdlVZlvDaA.ZHLnauLEIU6K-HhBRejiBA/tfjs/model/group1-shard3of3.bin","fileType":".bin","contentLength":4154376,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard3of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169087,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/model.json","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/model.json","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..cYUaeaPl_7RVxbb-6J-8LQ.vaaomLJk1ILg217mggLesaoGkBUc0w1iIk9RjkpgNCrWhJuP8VHgCtnBXqn6h2G792y3tnr1OAcO-2uhRs9VG8l6Q-wSSOLepq6m6D8_1GPrQwBVmV2s10OE4OCYp_GjAVaDPV89ldiZDEt0llWcDbV7KmmLEm4lWNKlhnb2tAlS5vNF1gOCWjwaNQDxAHAHvQsUI_6Q2C50nJo5J7eua0xQ2f68l1GE7jOeKhypuJNvLPaAMVz7ghCTkqZogiHD1fMYVWcY6VMU_N5COAEG_RNoHqhmCPjfqgkjNL46mgOkJe4S4uSE6Ro8ZPTc36GWxFa8a3ohjgWd44vnCvHAOJA-TihBXE3CwDyh_dkJ4g1inmtAjLg3vtA2IXkBM3tnSWCYskctq1c9fUimE-fdcIse6R5u733j27qYoDYiGmPCF8fPcF8xQhJKxD_9QJ_QOBfOEyrdtNzhlG8MDDsWyryinb5gI1yHxHFlepZKhPaKiegIXLM2viXGcCZiOQ7jTR__v4GiBxCasqFbRAv9c2UO1V5aaEGIl-vg4Vbwa5VNbGKjHFvRK7-Ai-NugvgMQdo9dZD733AeL4UI0vPuFfBUn7yRD_knUB9TBofXi6PQSbGZeyVjJoH9Wq1kzjmudBo-NEIKlUmpjObS7WRmVwnUsioNFkwzH-II5mwieAw.IJNpmdcyI73jK7k95cWULQ/tfjs/model/model.json","fileType":".json","contentLength":10029,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"model.json","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":4009178,"kernelVersionId":14798337,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..DDtJV_v34D-CAsLmBVtRBQ.bSbDZ99PreI7pBgaqxr5dRjpLfZp0vI71eRpyotj2rysdD8hJoML_9OxHKd-D3ttzPW0h3jlX3OFZgipcCZag08Ufl1pbWNZNRWKMrkvrkXiWR5Cb944mLA2vTcaqvyjx5JBg4v2Mo0wfHPxTuetxv10-hxkoPjGGrJ10l43CbW0SJfB4J26Tlgm0uGUY1ocMJcLNsADubtSuDLIrt2Nz2xOg0GEYqDLQ4NSLJyvQDL3fWsrYf9PVbRoCCaDcqS88-CJwMlkfewWZkhUElhhrBNthI4XCRsqNajJP2iv_Sf54XNHr49lemEyS9M-AJYBGUlbPmc0AAA4eTCbw0AxchgzYZHrFrPIQt5eZ75br_OkxMktwZ7gZAajINwAZzXbC5gDRcXBYI7dLWVYkH-BzbEfudeabL_0eufKS1V7mYi7AqqNMkcfzxxTX4SmHZ_g3kHw-r2G3nqS3-RSRNFov-8lYsY-k1g-hZVMB-I9cq-bN9_kBOopkIpwtYGbnh3H.j3wvSGDU7_fEQAejzbXkFA","scope":"vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":""}],"versions":[{"id":14798337,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-28T04:38:48.057Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":2,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":11065.347707848,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14798337","versionNumber":3,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"},{"id":14798128,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-28T04:32:14.59Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":9,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":10431.400544859,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14798128","versionNumber":2,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"},{"id":14615343,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-24T06:23:47.033Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":2143,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":10124.893342878,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14615343","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"}],"categories":{"type":"notebook","tags":[{"displayName":"GPU","fontAwesomeIcon":null,"id":16580,"name":"gpu","fullPath":"admin \u003e accelerators \u003e gpu","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27gpu%27","description":"This is for content that uses or relates to GPUs","isInherited":false,"datasetCount":31,"competitionCount":0,"scriptCount":1694707,"totalCount":1694738,"tagUrl":"/tags/gpu"},{"displayName":"CNN","fontAwesomeIcon":null,"id":13415,"name":"cnn","fullPath":"technique \u003e neural networks \u003e cnn","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27cnn%27","description":null,"isInherited":false,"datasetCount":170,"competitionCount":0,"scriptCount":1821,"totalCount":1991,"tagUrl":"/tags/cnn"}]},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/14798337","submission":null,"menuLinks":[{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/notebook","text":"Notebook","showOnMobile":true,"title":"Notebook","tab":"notebook","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/code","text":"Code","showOnMobile":true,"title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/data","text":"Input","showOnMobile":true,"title":"Input","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/output","text":"Output","showOnMobile":false,"title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/execution","text":"Execution Info","showOnMobile":false,"title":"Execution Info","tab":"execution","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/log","text":"Log","showOnMobile":false,"title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/comments","text":"Comments","showOnMobile":true,"title":"Comments","tab":"comments","count":3,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/14798337","text":"Fork Notebook","showOnMobile":true,"title":"Fork Notebook","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":8,"medalVotes":6,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=4009178","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/734966-kg.jpg","displayName":"Sayantan Das","profileUrl":"/sayantandas30011998","tier":"Contributor","tierInt":1,"userId":734966,"userName":"sayantandas30011998"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/793761-kg.jpg","displayName":"Larxel","profileUrl":"/andrewmvd","tier":"Grandmaster","tierInt":4,"userId":793761,"userName":"andrewmvd"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1056223-kg.jpg","displayName":"Ekrem Bayar","profileUrl":"/ekrembayar","tier":"Master","tierInt":3,"userId":1056223,"userName":"ekrembayar"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1552901-kg.jpg","displayName":"xrvf","profileUrl":"/aquibjkhan","tier":"Contributor","tierInt":1,"userId":1552901,"userName":"aquibjkhan"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2106122-fb.jpg","displayName":"sjb17","profileUrl":"/jainsarika04","tier":"Novice","tierInt":0,"userId":2106122,"userName":"jainsarika04"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2554700-kg.jpg","displayName":"Vipul Gandhi","profileUrl":"/vipulgandhi","tier":"Expert","tierInt":2,"userId":2554700,"userName":"vipulgandhi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"arif","profileUrl":"/arifzizi","tier":"Novice","tierInt":0,"userId":3586185,"userName":"arifzizi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/5138170-kg.JPG","displayName":"Ramon M. Ferreira","profileUrl":"/ramonmf","tier":"Contributor","tierInt":1,"userId":5138170,"userName":"ramonmf"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Malaria Cell Images Dataset","parentUrl":"/iarunava/cell-images-for-detecting-malaria","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-datasets-images/87153/200743/31c387765e937986306b32afe5b7148c/dataset-thumbnail.jpg?t=2018-12-05-05-57-20","canWrite":false,"canAdminister":false,"currentUserForkParentSessionId":null,"currentUserHasForked":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeInnerTableOfContents":true,"simplifiedViewer":false,"kernelOutputDataset":null,"disableComments":false,"taskSubmissionInfo":null,"learnSeriesNavigationData":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL-B4q8D_q6fu9C_LYNMSW1bYgcEPrerIZa4lO4t6ZewXZ6_6Y3JCWdOo58BU5o7ekgX8aozyw_ETIpMvQtGqS-AsO-HnMiRqC8t4Y8_p18JokTI9qczXXCFnLPvb9aCpfg"></form>
<script nonce="" type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
    "HTML-CSS": {
    preferredFont: "TeX",
    availableFonts: ["STIX", "TeX"],
    linebreaks: {
    automatic: true
    },
    EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
    inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
    displayMath: [["$$", "$$"], ["\\[", "\\]"]],
    processEscapes: true,
    ignoreClass: "tex2jax_ignore|dno"
    },
    TeX: {
    noUndefined: {
    attributes: {
    mathcolor: "red",
    mathbackground: "#FFEEEE",
    mathsize: "90%"
    }
    }
    },
    Macros: {
    href: "{}"
    },
    skipStartupTypeset: true,
    messageStyle: "none"
    });
</script>
<script nonce="" type="text/javascript" async="" crossorigin="anonymous" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/MathJax.js"></script>


</div></div><div class="sc-pQrCd hifEBa"><div class="sc-AxjAm dVDPh"><div class="sc-AxirZ ezTeoc">We use cookies on Kaggle to deliver our services, analyze web traffic, and improve your experience on the site. By using Kaggle, you agree to our use of cookies.</div><div class="sc-AxiKw kOAUSS"><div class="sc-AxhCb gsXzyw">Got it</div><a href="https://www.kaggle.com/cookies" class="sc-AxhUy fxWvvr"><div class="sc-AxhCb gsXzyw">Learn more</div></a></div></div></div></div></div>
<div data-component-name="NavigationContainer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script class="kaggle-component" nonce="">var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"navigationType":"BOTH_NAV"});performance && performance.mark && performance.mark("NavigationContainer.componentCouldBootstrap");</script>
<div id="site-body" class="hide">
    



<div data-component-name="KernelViewer" style="display: flex; flex-direction: column; flex: 1 0 auto;"></div><script class="kaggle-component" nonce="">var Kaggle=window.Kaggle||{};Kaggle.State=Kaggle.State||[];Kaggle.State.push({"kernel":{"id":4009178,"title":"Malaria Cell Analyzer + Tensorflow.js Web App","forkParent":null,"currentRunId":14798337,"mostRecentRunId":40892475,"url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app","tags":[{"name":"cnn","slug":"cnn","url":"/tags/cnn"},{"name":"gpu","slug":"gpu","url":"/tags/gpu"}],"commentCount":0,"upvoteCount":8,"viewCount":974,"forkCount":7,"bestPublicScore":null,"author":{"id":1086574,"displayName":"Marsh","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","profileUrl":"/vbookshelf","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":2,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isKaggleBot":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"isPhoneVerified":false},"isPrivate":false,"updatedTime":"2020-08-17T07:50:16.94Z","selfLink":"/kernels/4009178","pinnedDockerImageVersionId":null,"dockerImagePinningType":"original","originalDockerImageId":29507,"isLanguageTemplate":false,"medal":"bronze","topicId":93251,"readGroupId":null,"writeGroupId":null,"slug":"malaria-cell-analyzer-tensorflow-js-web-app","hasUsedAccelerator":false,"pinnedSessionId":null,"disableComments":false,"hasLinkedSubmission":false},"kernelBlob":{"id":331614577,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","accelerator":"gpu","isInternetEnabled":true},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Malaria Cell Analyzer\\nby Marsh [ @vbookshelf ] \u003cbr\u003e\\n24 May 2019\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/children.jpg\\\u0022 width=\\\u0022600\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eMalaria killed more than 261,000 children in 2017\u003c/h5\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003e *Malaria exacts a massive toll on human health and imposes a heavy social\\nand economic burden in low and middle income countries, particularly in Sub-Saharan Africa and South Asia. An estimated 219 million people suffered from the disease in 2017 and about 435,000 died. More than 90 percent of the deaths were in Africa, and over 60 percent were among children under 5.\u003cbr\u003e*\\n ~ [gatesfoundation.org](https://www.gatesfoundation.org/What-We-Do/Global-Health/Malaria)\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Introduction\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The goal of this kernel is to build a model that can detect malaria parasites in a cell image. The model will analyze a segmented red blood cell image and classify it as either \\\u0022uninfected\\\u0022 or \\\u0022parasitized\\\u0022. Once trained the model will be deployed online as a Tensorflow.js web app. \\n\\nMalaria diagnosis is currently a manual process. This prototype app is capable of automatically batch analyzing cell images. Therefore, it can help speed up a doctor\u0027s diagnostic workflow and reduce diagnostic errors. It can also help medical staff to triage patients by allowing them to quickly assess the severity of each patient\u0027s infection.\\n\\nWe will do basic EDA, build a Keras cnn model, do 5 fold cross validation, train the model using all the data and finally evaluate the model on a holdout set.\\n\\n\\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message saying that the model is loading but the app may actually be frozen. To see the app in action simply feed it some images from this dataset.\\n\\n\u003e Web App\u003cbr\u003e\\n\u003e http://malaria.test.woza.work/\u003cbr\u003e\\n\u003e \\n\u003e Github\u003cbr\u003e\\n\u003e https://github.com/vbookshelf/Malaria-Cell-Analyzer\\n\\n\\nBecause this app relies on segmented images (Fig. 3 below) this isn\u0027t a true end to end solution. In practice a glass slide image has many cells (see Fig. 2 below). A segmentation algorithm will need to be used to isolate (segment out) each cell before it can be fed into this app.\\n\\nThe app is based on Tensorflow.js, a technology that Google developed that allows machine learning models to run in a web browser. You\u0027ll notice that the app can process multiple images in about one second. It\u0027s fast because the analysis is done locally - on the user\u0027s pc or mobile phone. There\u0027s no need to upload images to an external server thus ensuring patient privacy and data security.\\n\\nI hope that this project will help you see how easy it is to build and deploy an Ai based solution.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Contents\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ca href=\u0027#Domain_Knowledge\u0027\u003e1. Domain Knowledge\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#EDA\u0027\u003e2. EDA\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Create_a_Holdout-Set\u0027\u003e3. Create a Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train-Test-Split_Model\u0027\u003e4. Train-Test-Split Model\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Error_Analysis\u0027\u003e5. Error Analysis\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Cross_Validation\u0027\u003e6. 5 Fold Cross Validation\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train\u0027\u003e7. Train the Final Model on all data\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Evaluate\u0027\u003e8. Evaluate the Final Model on the Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Convert\u0027\u003e9. Convert the Final Model to Tensorflow.js\u003c/a\u003e\u003cbr\u003e\\n\\n\u003ca href=\u0027#Citations\u0027\u003eCitations\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Reference_Kernels\u0027\u003eReference Kernels\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Helpful_Resources\u0027\u003eHelpful Resources\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Conclusion\u0027\u003eConclusion\u003c/a\u003e\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# set seeds to ensure repeatability of results\\nfrom numpy.random import seed\\nseed(101)\\nfrom tensorflow import set_random_seed\\nset_random_seed(101)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport os\\n\\nimport cv2\\nimport tensorflow\\n\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nimport itertools\\nimport shutil\\nimport pickle\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nfrom sklearn.model_selection import KFold, StratifiedKFold\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import classification_report\\n\\n\\nimport tensorflow\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.metrics import categorical_crossentropy\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\\nfrom tensorflow.keras.metrics import binary_accuracy\\n\\n\\n# Don\u0027t Show Warning Messages\\nimport warnings\\nwarnings.filterwarnings(\u0027ignore\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\nIMAGE_HEIGHT = 96\\nIMAGE_WIDTH = 96\\n\\nNUM_HOLDOUT_IMAGES = 200\\n\\nNUM_EPOCHS = 10\\nNUM_FOLDS = 5\\n\\nPADDING = 10\\nBATCH_SIZE = 10\\n\\n\\nNUM_FINAL_MODEL_EPOCHS = 10\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Domain_Knowledge\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 1. Domain Knowledge\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ciframe width=\\\u0022560\\\u0022 height=\\\u0022315\\\u0022 src=\\\u0022https://www.youtube.com/embed/2O3YrdUZQ5U?rel=0\\\u0022 frameborder=\\\u00220\\\u0022 allow=\\\u0022accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\u0022 allowfullscreen\u003e\u003c/iframe\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n\\n### Quick Facts\\n\\n- Malaria is a disease that infects and destroys red blood cells. \\n- Doctors analyze thick and thin blood smears to diagnose malaria and determine how severe it is.\\n- The parasite that causes malaria is called Plasmodium.\\n- The female Anopheles mosquito is the only mosquito that transmits malaria.\\n\\n\\n### Background\\n\\nMicroscopic examination of thick and thin blood smears is the easiest and most reliable test for malaria. Blood smears are often taken from a finger prick.\\n\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/glassslide.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 1. Microscope and glass slide\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/raw_image.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 2. Cells seen under a microscope. \u003cbr\u003e\\nThin blood smear.\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/parasitized.png\\\u0022 width=\\\u0022200\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 3. Segmented parasitized cell\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n*Malaria diagnosis involves the following*:\u003cbr\u003e\\n\u003e - Determine if the malaria parasite is present.\\n\u003e - Determine the parasite species.\\n\u003e - Determine parasite density by counting the number of cells that are infected.\\n\\n\u003cbr\u003e\\n\\n*Thick Blood Smear*\u003cbr\u003e\\n\\nA thick blood smear is a drop of blood on a glass slide. It\u0027s used to determine if there are any parasites in the blood. It\u0027s a larger blood sample because there could only be a few parasites present so a larger sample is needed to detect them. If no parasites are found the patient will have repeated blood smears every 8 hours for a few of days to confirm that there\u0027s no malaria infection.\\n\\n\u003cbr\u003e\\n\\n*Thin Blood Smear*\u003cbr\u003e\\n\\nA thin blood smear is a drop of blood that\u0027s spread over a large area of the glass slide. This blood smear helps doctor\u0027s discover what species of parasite is causing the infection.\\n\\n\u003cbr\u003e\\n\\n*Plasmodium*\u003cbr\u003e\\n\\nThis is the parasite that causes malaria. There are many species of Plasmodium. Five species cause malaria - P. vivax, P. ovale, P. malariae, P. knowlesi and P. falciparum. Most deaths are caused by P. falciparum. The other species usually cause a milder form of malaria.\\n\\n\u003cbr\u003e\\n\\n*Parasitemia*\u003cbr\u003e\\n\\nThis is the percentage of red blood cells that are infected by malaria (parasite density). This is computed by counting the number of infected cells visible under a microscope. This number helps doctors determine how severe the disease is. They then prescribe a treatment based on the severity. For example, if a large percentage of blood cells is infected medicine may be given directly into a vein instead of by mouth.\\n\\n\u003cbr\u003e\\n**Sources**\\n\\n- uofmhealth.org\u003cbr\u003e\\nhttps://www.uofmhealth.org/health-library/hw118744\\n\\n- Wikipedia Malaria\u003cbr\u003e\\nhttps://en.wikipedia.org/wiki/Malaria\\n\\n- Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027EDA\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 2. EDA\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### How many files are in each folder?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nuninfected_list = os.listdir(path_uninfected)\\nparasitized_list = os.listdir(path_parasitized)\\n\\nprint(\u0027Uninfected: \u0027, len(uninfected_list))\\nprint(\u0027Parasitized: \u0027, len(parasitized_list))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The data description says there should be 27,558 cell images i.e. 13779 files per folder. We see that each folder has an extra file. Later we\u0027ll check what kind of file this is.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Let\u0027s take a quick look at some images from each class\\n\\nThere are two classes\\n- uninfected\\n- parasitized.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = uninfected_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = parasitized_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Check if any non image files are in the folders\\n\\nWe see that there are actually 13780 files in each folder and not 13779 as expected. Folders can sometimes contain files that are automatically created during processing. These files can cause errors in the code if we don\u0027t know they exist - like the code we wrote above to display the images. Let\u0027s check if there are any non image files in the folders. \u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\n# sample image name: C140P101ThinF_IMG_20151005_211735_cell_159.png\\n\\nfor item in uninfected_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Uninfected folder: \u0027, item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\nfor item in parasitized_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Parasitized folder: \u0027,item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We see that each folder has one non image file called Thumbs.db. Now that we know that these non image files exist we will be sure exclude them later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Put the image names into dataframes\\n\\nHere we will create a dataframe called df_combined that includes both uninfected and parasitized images. This new dataframe will have a column showing the target class of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# create the dataframe\\ndf_uninfected = pd.DataFrame(uninfected_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_uninfected = df_uninfected[df_uninfected[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_uninfected[\u0027target\u0027] = 0\\n\\n\\n# create the dataframe\\ndf_parasitized = pd.DataFrame(parasitized_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_parasitized = df_parasitized[df_parasitized[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_parasitized[\u0027target\u0027] = 1\\n\\n#print(df_uninfected.shape)\\n#print(df_parasitized.shape)\\n\\n# Combine the two dataframes\\n\\ndf_combined = pd.concat([df_uninfected, df_parasitized], axis=0).reset_index(drop=True)\\n\\n#df_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_combined.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shape.\\n# There should be 27558 rows.\\n\\ndf_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if the image names are unique.\\n# The output should be 27558\\n\\ndf_combined[\u0027image_id\u0027].nunique()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The output above matches the number of rows. This confirms that each image has a unique name. This is important to verify because duplicate image file names will cause errors in the processing code that we will write later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### What are the image sizes and how many channels does each have?\\n\\nHere we will add the following info on each image to the df_combined dataframe:\\n\\nw = width\u003cbr\u003e\\nh = height\u003cbr\u003e\\nc = number of channels\u003cbr\u003e\\nmax_pixel_value\u003cbr\u003e\\nmin_pixel_value\u003cbr\u003e\\nimage_format\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndef read_image_sizes(file_name):\\n    \\\u0022\\\u0022\\\u0022\\n    1. Get the shape of the image\\n    2. Get the min and max pixel values in the image.\\n    Getting pixel values will tell if any pre-processing has been done.\\n    3. This info will be added to the original dataframe.\\n    \\\u0022\\\u0022\\\u0022\\n    \\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n     \\n    if file_name in uninfected_list:\\n        \\n        path = path_uninfected\\n        \\n    else:\\n        path = path_parasitized\\n    \\n    \\n    image = cv2.imread(path + file_name)\\n    max_pixel_val = image.max()\\n    min_pixel_val = image.min()\\n    img_format = file_name.split(\u0027.\u0027)[1]\\n    output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val, img_format]\\n    return output\\n\\nm = np.stack(df_combined[\u0027image_id\u0027].apply(read_image_sizes))\\ndf = pd.DataFrame(m,columns=[\u0027w\u0027,\u0027h\u0027,\u0027c\u0027,\u0027max_pixel_val\u0027,\u0027min_pixel_val\u0027, \u0027image_format\u0027])\\n\\ndf_combined = pd.concat([df_combined,df],axis=1, sort=False)\\n\\ndf_combined.head(10)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images have 3 channels\\ndf_combined[\u0027c\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images are in png format\\ndf_combined[\u0027image_format\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all black images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 0) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 0)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all white images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 255) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 255)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Display random images from each of the target classes\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The images are randomly selected. Therefore, different images will be dispayed each time the code is run. Also you\u0027ll see that parasitized images seem to have a blue-ish area that\u0027s not as common in uninfected images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_uninfected[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_parasitized[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### EDA Summary\\n\\n- There are 27,558 segmented cell images.\\n- Each folder contains 13,779 images.\\n- The images are of various sizes.\\n- All images are in png format.\\n- All images have 3 channels i.e. all are colour images.\\n- There are no all black or all white images.\\n- The target distribution is balanced i.e. each target class has the same number of images.\\n- All cells were segmented from thin blood smear slides.\\n- The parasite specie on all parasitized images is P. falciparum.\\n- Each folder contains a non image file called Thumbs.db.\\n- The presence of a blue-ish area inside the cell appears to be a common (but not definitive) indicator that a cell is parasitized.\\n\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Create_a_Holdout-Set\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 3. Create a Holdout Set\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As we continue to train, validate and modify a model we could over time start to overfit the validation data without realizing it. This means that the model will perform well during training but it will perform poorly on unseen data i.e. the app will perform badly in production.\\n\\nHere we will create a holdout set containing 200 images. We will keep this holdout set aside and only use it at the end to check how the final model performs on unseen data.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# shuffle\\ndf_combined = shuffle(df_combined, random_state=101)\\n\\n# create a holdout set with 200 samples\\ndf_holdout = df_combined.sample(NUM_HOLDOUT_IMAGES, random_state=101)\\n\\n# create a list of holdout images\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_holdout.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shapes.\\n# The ouput should be:\\n# (200, 8)\\n# (27358, 8)\\n\\nprint(df_holdout.shape)\\nprint(df_data.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution in the holdout set.\\n# 0 = uninfected\\n# 1 = parasitized\\n\\ndf_holdout[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a holdout set containing 200 images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train-Test-Split_Model\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 4. Train-Test-Split Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Train-test-split is not the ideal way to assess model perfromance. However, it\u0027s a good starting point because it\u0027s simple to set up and runs 5 times faster than 5 fold cross validation. This helps us to:\\n\\n- Quickly get a feel for what kind of performance we can expect from this data.\\n- Quickly test different architectures and parameters.\\n- Establish the workflow that will later be used in cross validation.\\n- Check the training curves to see if the model is overfitting.\\n- Check the training curves to establish the number of epochs we will use when training the final model on all data.\\n- Perform error analysis.\\n\\nA train-test-split model is a rough guide. When deciding if a particular change actually improved model performance we will base that decision on cross validation results. This is important. If we don\u0027t use cross validation we may think we are improving but in reality we may be getting nowhere.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Directory Structure\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022To reduce RAM use and prevent this kernel from crashing we will batch feed the images when training the model. We will use generators to do this. In order to use this approach Keras requires that a particular directory structure be set up. Keras uses this structure to automatically infer the class (target) of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check if base_dir has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# see what\u0027s inside base_dir\\nos.listdir(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Train and Val Sets\\n\\nWe will create a stratified val set. This means that the val set will have the same target distribution as the train set. Actually doing stratification isn\u0027t essential here because the target is balanced. Because of this there\u0027s a good chance that randomly selecting rows will still result in the val set having a fairly balanced target distribution. Stratification is more applicable for data where the target is unbalanced.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# select the column that we will use for stratification\\ny = df_data[\u0027target\u0027]\\n\\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\\n\\nprint(df_train.shape)\\nprint(df_val.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution of the val set.\\n# The target should be approx balanced.\\n\\ndf_val[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Transfer the images into the folders\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n# Get a list of images in each of the two folders\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n        \\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n        \\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the number of images in each folder\\n\\n# train\\nprint(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\nprint(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Set Up the Generators\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Note that here we are normalizing the images inside the generator.\\n# If you wanted to add some data augmentation you could do it here.\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\nval_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Model Architecture\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022I found this cnn architecture a while ago in a kernel created by @fmarazzi. I\u0027ve since used this on several projects. It really is a good multi-purpose architecture. Here I\u0027ve modified it slightly by adding a ZeroPadding layer. Later, in the error analysis section, I\u0027ll explain why I added this layer.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\nmodel.summary()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Train the Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Please note that we\u0027ve set the learning rate to reduce (decay) at each epoch.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022filepath = \\\u0022model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n                              \\n                              \\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            validation_data=val_gen,\\n                            validation_steps=val_steps,\\n                            epochs=NUM_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Evaluate the model using the val set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# get the metric names so we can use evaulate_generator\\nmodel.metrics_names\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Here the best epoch will be used.\\n\\nmodel.load_weights(\u0027model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\nprint(\u0027val_loss:\u0027, val_loss)\\nprint(\u0027val_acc:\u0027, val_acc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Plot the Training Curves\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# display the loss and accuracy curves\\n\\nimport matplotlib.pyplot as plt\\n\\nacc = history.history[\u0027acc\u0027]\\nval_acc = history.history[\u0027val_acc\u0027]\\nloss = history.history[\u0027loss\u0027]\\nval_loss = history.history[\u0027val_loss\u0027]\\n\\nepochs = range(1, len(acc) + 1)\\n\\nplt.plot(epochs, loss, \u0027bo\u0027, label=\u0027Training loss\u0027)\\nplt.plot(epochs, val_loss, \u0027b\u0027, label=\u0027Validation loss\u0027)\\nplt.title(\u0027Training and validation loss\u0027)\\nplt.legend()\\nplt.figure()\\n\\nplt.plot(epochs, acc, \u0027bo\u0027, label=\u0027Training acc\u0027)\\nplt.plot(epochs, val_acc, \u0027b\u0027, label=\u0027Validation acc\u0027)\\nplt.title(\u0027Training and validation accuracy\u0027)\\nplt.legend()\\nplt.figure()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the labels of the test images.\\n\\ntest_labels = test_gen.classes\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# We need these to plot the confusion matrix.\\ntest_labels\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the label associated with each class\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# If you wanted to get the image_id\u0027s to match them to predictions this\\n# is how to do it.\\n\\n# test_gen.filenames\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check the number of predictions\\npredictions.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Source: Scikit Learn website\\n# http://scikit-learn.org/stable/auto_examples/\\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\\n# selection-plot-confusion-matrix-py\\n\\n\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\u0027Confusion matrix\u0027,\\n                          cmap=plt.cm.Blues):\\n    \\\u0022\\\u0022\\\u0022\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\u0022\\\u0022\\\u0022\\n    if normalize:\\n        cm = cm.astype(\u0027float\u0027) / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\u0022Normalized confusion matrix\\\u0022)\\n    else:\\n        print(\u0027Confusion matrix, without normalization\u0027)\\n\\n    print(cm)\\n\\n    plt.imshow(cm, interpolation=\u0027nearest\u0027, cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n\\n    fmt = \u0027.2f\u0027 if normalize else \u0027d\u0027\\n    thresh = cm.max() / 2.\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\n        plt.text(j, i, format(cm[i, j], fmt),\\n                 horizontalalignment=\\\u0022center\\\u0022,\\n                 color=\\\u0022white\\\u0022 if cm[i, j] \u003e thresh else \\\u0022black\\\u0022)\\n\\n    plt.ylabel(\u0027True label\u0027)\\n    plt.xlabel(\u0027Predicted label\u0027)\\n    plt.tight_layout()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_labels.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Error_Analysis\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5. Error Analysis\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022In this section we\u0027re not going to use any fancy statistical gymnastics. We\u0027re simply going to look at the images that the model predicted correctly and those it got wrong. We want to see if there are any patterns or issues that could\u0027ve caused the model to make mistakes.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Put the val image_id, labels and predictions into a dataframe.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# put the val image_id, labels and predictions into a dataframe\\n\\nval_pred_dict = {\\n    \u0027image_id\u0027: test_gen.filenames,\\n    \u0027val_labels\u0027: test_gen.classes,\\n    \u0027val_preds\u0027: predictions.argmax(axis=1)\\n}\\n\\ndf_val_preds = pd.DataFrame(val_pred_dict)\\n\\n\\n# Adjust the file names\\n\\n# sample image name: a_uninfected/C100P61ThinF_IMG_20150918_144104_...\\n# we want just this part: C100P61ThinF_IMG_20150918_144104_...\\n\\ndef adjust_file_names(x):\\n    # split into a list based on \u0027/\u0027\\n    fname = x.split(\u0027/\u0027)\\n    # chose the second item in the list which is the image name\\n    fname = fname[1]\\n    \\n    return fname\\n\\ndf_val_preds[\u0027image_id\u0027] = df_val_preds[\u0027image_id\u0027].apply(adjust_file_names)\\n\\n\\n# savedf_val_preds so we can analyze the results later\\npickle.dump(df_val_preds,open(\u0027df_val_preds.pickle\u0027,\u0027wb\u0027))\\n\\n# code to load the dataframe\\n# df_val_preds = pickle.load(open(\u0027df_val_preds\u0027,\u0027rb\u0027))\\n\\n\\n#df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# filter out those rows where the model made correct predictions\\ndf_correct = df_val_preds[df_val_preds[\u0027val_labels\u0027] == df_val_preds[\u0027val_preds\u0027]]\\n\\n# filter out those rows where the model made wrong predictions\\ndf_wrong = df_val_preds[df_val_preds[\u0027val_labels\u0027] != df_val_preds[\u0027val_preds\u0027]]\\n\\nprint(df_correct.shape)\\nprint(df_wrong.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the correct predictions\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_correct.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_correct[df_correct[\u0027val_labels\u0027] == 1]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 1\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_correct[df_correct[\u0027val_labels\u0027] == 0]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 0\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the wrong predictions\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized but the model predicted uninfected\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_wrong[df_wrong[\u0027val_labels\u0027] == 1]\\n\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 0\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected but the model predicted parasitized\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_wrong[df_wrong[\u0027val_labels\u0027] == 0]\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 1\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Observations and Comments\\n\\nIn cases where the label was parasitized but the model predicted uninfected I noticed that the blue-ish area was located close to the edge of the image. Could it be that the model was not detecting important patterns that are located at the edge of images? To address this issue I decided to add a padding layer to the model arcitecture. This layer added a 10 pixel zero padding around each image. I found that the cross validation scores improved after this change.\\n\\n\\nWhen doing this error analysis our premise has been that if there is a mismatch between the label and the prediction then the model has made a mistake. However, there\u0027s another possibility - the model prediction is correct. There are 27,558 images in this dataset. All images were examined and labeled by the same expert. This must\u0027ve been a tedious and time consuming process so it\u0027s possible that some images were incorrectly labeled. \\n\\nAt this point it would be helpful to collaborate with a domain expert in order to discuss possible reasons for the mistakes this model is making. Are they the result of model weakness, incorrect labels or maybe damaged images? Is the correct label easy for a human to classify or would a human struggle to make a correct diagnosis? What are the main indicators that an expert looks for when examining cell images? Is the model seeing things that an expert didn\u0027t notice?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Cross_Validation\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5 Fold Cross Validation\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Here we will simply apply the same workflow that we used for the train-test-split model to 5 folds. For each fold we\u0027ll get the loss, accuracy and auc. Then we\u0027ll average the results of the 5 folds to get the final scores.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# ==============================\\n# Create the 5 Folds\\n# ==============================\\n\\n# shuffle df_combined and change the name to df_data\\ndf_data = shuffle(df_combined.copy())\\n\\n# train_test_split\\ny = df_data[\u0027target\u0027]\\n\\n# initialize kfold\\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=101)\\n\\n# define y for stratification\\ny = df_data[\u0027target\u0027]\\n\\n# Note:\\n# Each fold is a tuple ([train_index_values], [val_index_values])\\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df_train, y)\\n\\n# Put the folds into a list. This is a list of tuples.\\n# y was set above.\\nfold_list = list(kf.split(df_data, y))\\n\\n\\n# ==============================\\n# Loop Through the Folds\\n# ==============================\\n\\n# create a list to store the predictions\\nval_pred_list = []\\n\\n# create a list to store the scores\\nval_acc_list = []\\nval_loss_list = []\\nval_auc_list = []\\n\\n\\nfor i, fold in enumerate(fold_list):\\n\\n    # Delete the image data directory we created to prevent a Kaggle error.\\n    # Kaggle allows a max of 500 files to be saved.\\n    \\n    if os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n        \\n        \\n    \\n    # set df_data\\n    df_data = df_combined.copy()\\n    \\n    print(\u0027=== Fold_\u0027 + str(i) + \u0027 ===\u0027)\\n    print(\u0027\\\\n\u0027)\\n\\n    # map the train and val index values to dataframe rows\\n    df_train = df_data[df_data.index.isin(fold[0])]\\n    df_val = df_data[df_data.index.isin(fold[1])]\\n    \\n\\n\\n\\n    # ==============================\\n    # Create a Directory Structure\\n    # ==============================\\n\\n    # Create a new directory\\n    base_dir = \u0027base_dir\u0027\\n    os.mkdir(base_dir)\\n\\n\\n    #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n    # now we create 2 folders inside \u0027base_dir\u0027:\\n\\n    # train\\n        # a_uninfected\\n        # b_parasitized\\n\\n    # val\\n        # a_uninfected\\n        # b_parasitized\\n\\n\\n    # create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n    # train_dir\\n    train_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\n    os.mkdir(train_dir)\\n\\n    # val_dir\\n    val_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\n    os.mkdir(val_dir)\\n\\n\\n    # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n    # Inside each folder we create seperate folders for each class\\n\\n    # create new folders inside train_dir\\n    a_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n    # create new folders inside val_dir\\n    a_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n\\n    # =================================\\n    # Transfer the Images into Folders\\n    # =================================\\n\\n    # Set the image_id as the index in df_data\\n    df_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n    # Get a list of images in each of the two folders\\n\\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n    folder_1 = os.listdir(path_uninfected)\\n    folder_2 = os.listdir(path_parasitized)\\n\\n    # Get a list of train and val images\\n    train_list = list(df_train[\u0027image_id\u0027])\\n    val_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n    # Transfer the train images\\n\\n    for image in train_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n\\n\\n    # Transfer the val images\\n\\n    for image in val_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n    # Print the number of images in each folder\\n\\n    # train\\n    #print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n    # val\\n    #print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n    #print(\u0027\\\\n\u0027)\\n\\n\\n    # ==============================\\n    # Set Up the Generators\\n    # ==============================\\n\\n    train_path = \u0027base_dir/train_dir\u0027\\n    valid_path = \u0027base_dir/val_dir\u0027\\n\\n    num_train_samples = len(df_train)\\n    num_val_samples = len(df_val)\\n    train_batch_size = BATCH_SIZE\\n    val_batch_size = BATCH_SIZE\\n\\n\\n    train_steps = np.ceil(num_train_samples / train_batch_size)\\n    val_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\n    datagen = ImageDataGenerator(rescale=1.0/255)\\n\\n    train_gen = datagen.flow_from_directory(train_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=train_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    val_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    # Note: shuffle=False causes the test dataset to not be shuffled\\n    test_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027,\\n                                            shuffle=False)\\n    \\n    print(\u0027\\\\n\u0027)\\n\\n    # ==============================\\n    # Set Up the Model Architecture\\n    # ==============================\\n\\n\\n\\n    kernel_size = (3,3)\\n    pool_size= (2,2)\\n    first_filters = 32\\n    second_filters = 64\\n    third_filters = 128\\n\\n    dropout_conv = 0.3\\n    dropout_dense = 0.3\\n\\n\\n    model = Sequential()\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                     input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n    \\n    model.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n    \\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size)) \\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Flatten())\\n    model.add(Dense(256, activation = \\\u0022relu\\\u0022))\\n    model.add(Dropout(dropout_dense))\\n    model.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n    #model.summary()\\n\\n\\n\\n    # ==============================\\n    # Train the Model\\n    # ==============================\\n\\n\\n    model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n                  metrics=[\u0027accuracy\u0027])\\n\\n    filepath = \\\u0022model.h5\\\u0022\\n    checkpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                                 save_best_only=True, mode=\u0027max\u0027)\\n\\n    reduce_lr = ReduceLROnPlateau(monitor=\u0027val_acc\u0027, factor=0.5, patience=2, \\n                                       verbose=1, mode=\u0027max\u0027, min_lr=0.00001)\\n\\n\\n    callbacks_list = [checkpoint, reduce_lr]\\n\\n    history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                                validation_data=val_gen,\\n                                validation_steps=val_steps,\\n                                epochs=NUM_EPOCHS, verbose=1,\\n                               callbacks=callbacks_list)\\n\\n\\n\\n    # ==================================\\n    # Evaluate the Model on the Val Set\\n    # ==================================\\n\\n    model.load_weights(\u0027model.h5\u0027)\\n\\n    val_loss, val_acc = \\\\\\n    model.evaluate_generator(test_gen, \\n                            steps=val_steps)\\n    \\n    # append the acc score val_scores_list\\n    val_acc_list.append(val_acc)\\n    val_loss_list.append(val_loss)\\n    \\n    \\n    # ==================================\\n    # Calculate the AUC Score\\n    # ==================================\\n\\n    test_labels = test_gen.classes\\n\\n    # make a prediction\\n    predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n    \\n    # append the predictions to a list\\n    val_pred_list.append(predictions)\\n    \\n    val_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n    \\n    val_auc_list.append(val_auc)\\n    \\n    \\n    \\n    # ==================================\\n    # Print the Fold Scores\\n    # ==================================\\n    \\n    \\n    #print(\u0027\\\\n\u0027)\\n    #print(\u0027Fold_\u0027 + str(i) + \u0027 scores:\\\\n\u0027)\\n    #print(\u0027val_loss:\u0027, val_loss)\\n    #print(\u0027val_acc:\u0027, val_acc)\\n    #print(\u0027val_auc:\u0027, val_auc)\\n    #print(\u0027\\\\n\u0027)\\n\\n    \\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Average for of all 5 folds:\\\\n\u0027)\\n#print(\u0027Average Accuracy: \u0027, avg_acc)\\n#print(\u0027Average Loss: \u0027, avg_loss)\\n#print(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Print the scores for each fold and the average scores\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Print the scores\\n\\nprint(\u0027Val Acc\u0027)\\nfor item in val_acc_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val Loss\u0027)\\nfor item in val_loss_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val AUC\u0027)\\nfor item in val_auc_list:\\n    print(item)\\n\\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Average for of all 5 folds:\\\\n\u0027)\\nprint(\u0027Average Accuracy: \u0027, avg_acc)\\nprint(\u0027Average Loss: \u0027, avg_loss)\\nprint(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 7. Train the Final Model using all the data\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We\u0027re not using validation data in this final model. Therefore, we won\u0027t be able to set up the model to save the best epoch. We\u0027ll need to determine the number of training epochs before starting.\\n\\nTo determine this number I would normally look at the training curves (see train-test-split model) and determine the epoch at which the model started to overfit i.e. the training and validation accuracy curves start to diverge. However, with this data and architecture it appears that overfitting is not a huge problem because the training and validation curves do not diverge significantly. Therefore choosing 10 epochs seems to give a good balance between training time and model quality.\\n\\nIt\u0027s also important to keep in mind that we can train on all data because we set the learning rate to decay at each epoch i.e. the learning rate was scheduled. If we had used a dynamic learning rate (e.g. ReduceLROnPlateau) then we\u0027ll need to have some way of replicating the learning rate changes that happened automatically during 5 fold cross validation. Therefore, using a scheduled learning rate keeps things simple.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n\\n\\n\\n# ==============================\\n# Set df_train\\n# ==============================\\n\\n# This variable was set above. Just setting it here again for clarity.\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\\ndf_train = df_data.copy()\\ndf_val = df_holdout.copy()\\n\\n\\n\\n# ==============================\\n# Create a Directory Structure\\n# ==============================\\n\\n# Create a new directory\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n\\n# =================================\\n# Transfer the Images into Folders\\n# =================================\\n\\n# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n# Set the image_id as the index in df_data\\ndf_holdout.set_index(\u0027image_id\u0027, inplace=True)\\n\\n\\n\\n# Get a list of images in each of the two folders\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n\\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n\\n    fname = image\\n    target = df_holdout.loc[image,\u0027target\u0027]\\n\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n# Print the number of images in each folder\\n\\n# train\\n#print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\n#print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n#print(\u0027\\\\n\u0027)\\n\\n\\n# ==============================\\n# Set Up the Generators\\n# ==============================\\n\\ntrain_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\\n\\nprint(\u0027\\\\n\u0027)\\n\\n# ==============================\\n# Set Up the Model Architecture\\n# ==============================\\n\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n#model.summary()\\n\\n\\n\\n# ==============================\\n# Train the Model\\n# ==============================\\n\\n\\nmodel.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\\n\\n# we are saving the model based on training accuracy\\nfilepath = \\\u0022final_model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n\\n\\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            epochs=NUM_FINAL_MODEL_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\\n\\n\\n# ==================================\\n# Evaluate the Model on the Val Set\\n# ==================================\\n\\nmodel.load_weights(\u0027final_model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\n\\n# ==================================\\n# Calculate the AUC Score\\n# ==================================\\n\\ntest_labels = test_gen.classes\\n\\n# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n\\n\\nval_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n\\n\\n\\n\\n# ==================================\\n# Print the Scores\\n# ==================================\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Accuracy: \u0027, val_acc)\\n#print(\u0027Loss: \u0027, val_loss)\\n#print(\u0027AUC: \u0027, val_auc)\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Evaluate\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 8. Evaluate the Final Model on the Holdout Set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# ==================================\\n# Print the Scores\\n# ==================================\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Accuracy: \u0027, val_acc)\\nprint(\u0027Loss: \u0027, val_loss)\\nprint(\u0027AUC: \u0027, val_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\\n\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\\n\\n\\n# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a trained model that can be incorporated into the web app. The metrics we calculated above all look very good meaning that the model should perform well on unseen data.\\n\\nAlso, because we\u0027ve used a simple architecture the model size will be less than 10 MB. This means that it will download quickly. Therefore, the web page will load fast and the overall user experience will be good.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Convert\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 9. Convert the final model from Keras to Tensorflow.js\\n\\nThis conversion needs to be done so that the model can be loaded into the web app.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# --ignore-installed is added to fix an error.\\n\\n# https://stackoverflow.com/questions/49932759/pip-10-and-apt-how-to-avoid-cannot-uninstall\\n# -x-errors-for-distutils-packages\\n\\n!pip install tensorflowjs --ignore-installed\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Use the command line conversion tool to convert the model\\n\\n!tensorflowjs_converter --input_format keras final_model.h5 tfjs/model\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the folder containing the tfjs model files has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the tfjs files exist\\nos.listdir(\u0027tfjs/model\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Delete the images that were moved around\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Citations\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Citations\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n- Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images. PeerJ6:e4568\u003cbr\u003e\\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude, RJ, Jaeger S, Thoma GR. (2018)\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Reference_Kernels\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Reference Kernels\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Gabriel Preda, Honey Bee Subspecies Classification\u003cbr\u003e\\nhttps://www.kaggle.com/gpreda/honey-bee-subspecies-classification\\n\\n- Francesco Marazzi, Baseline Keras CNN\u003cbr\u003e\\nhttps://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\\n\\n- Kimmo Sskilahti, Image processing with scikit-image\u003cbr\u003e\\nhttps://www.kaggle.com/ksaaskil/image-processing-with-scikit-image\\n\\n- Marsh, Skin Lesion Analyzer\u003cbr\u003e\\nhttps://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Helpful_Resources\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Helpful Resources\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Excellent tutorial series by deeplizard on how to use Tensorflow.js to build a web app.\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\\n\\n- Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\\n\\n- Tutorial by Data School on how to evaluate a classifier\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\\n\\n- Tensorflow.js gallery of projects\u003cbr\u003e\\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Conclusion\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Conclusion\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This is the Foldscope Origami Microscope.\u003cbr\u003e\\nhttps://youtu.be/ky-cqSI5mwE\\n\\nThe microscope image could perhaps be photographed using phone camera and then processed using a web app.\\n\\n\\nThis was a software solution. However, there could be some who are reading this who may be interested in combining the power of Ai with electronics - by building robots, analyzers and other devices. You probably don\u0027t know where to start. The good news is that if you can code then you can also build electronic devices. The principles are the same. It\u0027s just that one uses software and the other uses physical components. \\n\\nThese two practical Udemy courses are a great place to start your journey towards world domination. They are geared towards young students and, most importantly, the instructor is an excellent teacher. The courses were developed with deaf students in mind.\\n\\nElectronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/analog-electronics-robotics-learn-by-building/\\n\\nDigital Electronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/digital-electronics-robotics-learn-by-building-module-ii/\\n\\n\\nMany thanks to Arunava for making this interesting dataset available on Kaggle.\\n\\nThank you for reading.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022Python 3\u0022,\u0022language\u0022:\u0022python\u0022,\u0022name\u0022:\u0022python3\u0022},\u0022language_info\u0022:{\u0022codemirror_mode\u0022:{\u0022name\u0022:\u0022ipython\u0022,\u0022version\u0022:3},\u0022file_extension\u0022:\u0022.py\u0022,\u0022mimetype\u0022:\u0022text/x-python\u0022,\u0022name\u0022:\u0022python\u0022,\u0022nbconvert_exporter\u0022:\u0022python\u0022,\u0022pygments_lexer\u0022:\u0022ipython3\u0022,\u0022version\u0022:\u00223.6.4\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2019-05-28T04:38:47.9095886Z"},"kernelRun":{"id":14798337,"kernelId":4009178,"status":"complete","type":"batch","sourceType":"notebook","language":"python","title":"Malaria Cell Analyzer + Tensorflow.js Web App","dateCreated":"2019-05-28T04:38:48.057Z","dateEvaluated":"2019-05-28T04:38:48.57Z","dateCancelled":null,"workerContainerPort":null,"workerUptimeSeconds":79407,"workerIPAddress":"172.21.129.78  ","workerIPAddressExternal":null,"scriptLanguageId":9,"scriptLanguageName":"IPython Notebook HTML","renderedOutputUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..TgZ-UlZOZpZxMgd6TNI8-g.n9OqO8yFIi_BtBqpqyAbcKDSddpgU0jsW42KOm3R-ccCWUs5Lhd-6fRgZHLfyOhtGplPcXuyaEPbdHaQYRyNVgDoC9ml4j2y2O3PiNvXJcgdWwptldsHwKBA416ycyZj07R8poTCf1ekx8Nk15-Ze80EndoHnKslyvyo4cXnrYEocaXikjuBVVfmtdk11ykxyDAhkx2HWXpo7rX5GJBdGGqMaaODtK2SBsZsRNcpclixq85z2qnM_PYMIaodsRsqa6cQ3gbBHVZyi4itN8bYv4NLafVjtijiTaInOGot6sxB-Q9QtQZugpkw84FHvbFm7JfPmxjGOR4K8x4hi0qHTNVgX_7iWxizk5z7eVEsieShp-GvNWgDtqCIcBuTmLPcWJF0m8-nGOPDYH3KdCbTm0a8zGObQC3X1I011JDRY5Q6DjejmmKGXut8BwysVMe0S2GOXjBRT2YGomfjRMR0wHpidXBZyzV8OpI5I2SoNN5nHgmadz1lzDlgzYoBXKBv9AUuSA2RBvn_unjnXtiwG-jiZQ9AzkthDXJuSZ1W-fV4j6a1XJjBfIfm6PKzaz8NX8T-dR8WlFf_oeRnhvZAOxsz0VQJxuKaE1ZvEjtaHDCQG-MQuh2kSJj3qE-WCAjZnd4HUXNJ3gAMQf2Bz7qjKkKF4pqXX8iRZGXYeH1GtMU.7aYWiJATsRDNrRJBtGX4KA/__results__.html","commit":{"id":331614577,"settings":{"dockerImageVersionId":null,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":null}],"sourceType":"notebook","language":"python","accelerator":"gpu","isInternetEnabled":true},"source":"{\u0022cells\u0022:[{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Malaria Cell Analyzer\\nby Marsh [ @vbookshelf ] \u003cbr\u003e\\n24 May 2019\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/children.jpg\\\u0022 width=\\\u0022600\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eMalaria killed more than 261,000 children in 2017\u003c/h5\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003e *Malaria exacts a massive toll on human health and imposes a heavy social\\nand economic burden in low and middle income countries, particularly in Sub-Saharan Africa and South Asia. An estimated 219 million people suffered from the disease in 2017 and about 435,000 died. More than 90 percent of the deaths were in Africa, and over 60 percent were among children under 5.\u003cbr\u003e*\\n ~ [gatesfoundation.org](https://www.gatesfoundation.org/What-We-Do/Global-Health/Malaria)\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Introduction\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The goal of this kernel is to build a model that can detect malaria parasites in a cell image. The model will analyze a segmented red blood cell image and classify it as either \\\u0022uninfected\\\u0022 or \\\u0022parasitized\\\u0022. Once trained the model will be deployed online as a Tensorflow.js web app. \\n\\nMalaria diagnosis is currently a manual process. This prototype app is capable of automatically batch analyzing cell images. Therefore, it can help speed up a doctor\u0027s diagnostic workflow and reduce diagnostic errors. It can also help medical staff to triage patients by allowing them to quickly assess the severity of each patient\u0027s infection.\\n\\nWe will do basic EDA, build a Keras cnn model, do 5 fold cross validation, train the model using all the data and finally evaluate the model on a holdout set.\\n\\n\\nThis is the link to the live app. The html, css, and javascript code is available on Github. I recommend using the latest version of the Chrome browser. When using Safari for example, you may see a message saying that the model is loading but the app may actually be frozen. To see the app in action simply feed it some images from this dataset.\\n\\n\u003e Web App\u003cbr\u003e\\n\u003e http://malaria.test.woza.work/\u003cbr\u003e\\n\u003e \\n\u003e Github\u003cbr\u003e\\n\u003e https://github.com/vbookshelf/Malaria-Cell-Analyzer\\n\\n\\nBecause this app relies on segmented images (Fig. 3 below) this isn\u0027t a true end to end solution. In practice a glass slide image has many cells (see Fig. 2 below). A segmentation algorithm will need to be used to isolate (segment out) each cell before it can be fed into this app.\\n\\nThe app is based on Tensorflow.js, a technology that Google developed that allows machine learning models to run in a web browser. You\u0027ll notice that the app can process multiple images in about one second. It\u0027s fast because the analysis is done locally - on the user\u0027s pc or mobile phone. There\u0027s no need to upload images to an external server thus ensuring patient privacy and data security.\\n\\nI hope that this project will help you see how easy it is to build and deploy an Ai based solution.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## Contents\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ca href=\u0027#Domain_Knowledge\u0027\u003e1. Domain Knowledge\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#EDA\u0027\u003e2. EDA\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Create_a_Holdout-Set\u0027\u003e3. Create a Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train-Test-Split_Model\u0027\u003e4. Train-Test-Split Model\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Error_Analysis\u0027\u003e5. Error Analysis\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Cross_Validation\u0027\u003e6. 5 Fold Cross Validation\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Train\u0027\u003e7. Train the Final Model on all data\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Evaluate\u0027\u003e8. Evaluate the Final Model on the Holdout Set\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Convert\u0027\u003e9. Convert the Final Model to Tensorflow.js\u003c/a\u003e\u003cbr\u003e\\n\\n\u003ca href=\u0027#Citations\u0027\u003eCitations\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Reference_Kernels\u0027\u003eReference Kernels\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Helpful_Resources\u0027\u003eHelpful Resources\u003c/a\u003e\u003cbr\u003e\\n\u003ca href=\u0027#Conclusion\u0027\u003eConclusion\u003c/a\u003e\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# set seeds to ensure repeatability of results\\nfrom numpy.random import seed\\nseed(101)\\nfrom tensorflow import set_random_seed\\nset_random_seed(101)\\n\\nimport pandas as pd\\nimport numpy as np\\nimport os\\n\\nimport cv2\\nimport tensorflow\\n\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.utils import shuffle\\nimport itertools\\nimport shutil\\nimport pickle\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\nfrom sklearn.model_selection import KFold, StratifiedKFold\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import classification_report\\n\\n\\nimport tensorflow\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, ZeroPadding2D\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.metrics import categorical_crossentropy\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.models import Model\\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\\nfrom tensorflow.keras.metrics import binary_accuracy\\n\\n\\n# Don\u0027t Show Warning Messages\\nimport warnings\\nwarnings.filterwarnings(\u0027ignore\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\nIMAGE_HEIGHT = 96\\nIMAGE_WIDTH = 96\\n\\nNUM_HOLDOUT_IMAGES = 200\\n\\nNUM_EPOCHS = 10\\nNUM_FOLDS = 5\\n\\nPADDING = 10\\nBATCH_SIZE = 10\\n\\n\\nNUM_FINAL_MODEL_EPOCHS = 10\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Domain_Knowledge\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 1. Domain Knowledge\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003ciframe width=\\\u0022560\\\u0022 height=\\\u0022315\\\u0022 src=\\\u0022https://www.youtube.com/embed/2O3YrdUZQ5U?rel=0\\\u0022 frameborder=\\\u00220\\\u0022 allow=\\\u0022accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\u0022 allowfullscreen\u003e\u003c/iframe\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n\\n### Quick Facts\\n\\n- Malaria is a disease that infects and destroys red blood cells. \\n- Doctors analyze thick and thin blood smears to diagnose malaria and determine how severe it is.\\n- The parasite that causes malaria is called Plasmodium.\\n- The female Anopheles mosquito is the only mosquito that transmits malaria.\\n\\n\\n### Background\\n\\nMicroscopic examination of thick and thin blood smears is the easiest and most reliable test for malaria. Blood smears are often taken from a finger prick.\\n\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/glassslide.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 1. Microscope and glass slide\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/raw_image.jpg\\\u0022 width=\\\u0022400\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 2. Cells seen under a microscope. \u003cbr\u003e\\nThin blood smear.\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n\u003cimg src=\\\u0022http://malaria.test.woza.work/assets/parasitized.png\\\u0022 width=\\\u0022200\\\u0022\u003e\u003c/img\u003e\\n\\n\u003ch5 align=\\\u0022center\\\u0022\u003eFig 3. Segmented parasitized cell\u003c/h5\u003e\\n\\n\u003cbr\u003e\\n\u003cbr\u003e\\n\\n*Malaria diagnosis involves the following*:\u003cbr\u003e\\n\u003e - Determine if the malaria parasite is present.\\n\u003e - Determine the parasite species.\\n\u003e - Determine parasite density by counting the number of cells that are infected.\\n\\n\u003cbr\u003e\\n\\n*Thick Blood Smear*\u003cbr\u003e\\n\\nA thick blood smear is a drop of blood on a glass slide. It\u0027s used to determine if there are any parasites in the blood. It\u0027s a larger blood sample because there could only be a few parasites present so a larger sample is needed to detect them. If no parasites are found the patient will have repeated blood smears every 8 hours for a few of days to confirm that there\u0027s no malaria infection.\\n\\n\u003cbr\u003e\\n\\n*Thin Blood Smear*\u003cbr\u003e\\n\\nA thin blood smear is a drop of blood that\u0027s spread over a large area of the glass slide. This blood smear helps doctor\u0027s discover what species of parasite is causing the infection.\\n\\n\u003cbr\u003e\\n\\n*Plasmodium*\u003cbr\u003e\\n\\nThis is the parasite that causes malaria. There are many species of Plasmodium. Five species cause malaria - P. vivax, P. ovale, P. malariae, P. knowlesi and P. falciparum. Most deaths are caused by P. falciparum. The other species usually cause a milder form of malaria.\\n\\n\u003cbr\u003e\\n\\n*Parasitemia*\u003cbr\u003e\\n\\nThis is the percentage of red blood cells that are infected by malaria (parasite density). This is computed by counting the number of infected cells visible under a microscope. This number helps doctors determine how severe the disease is. They then prescribe a treatment based on the severity. For example, if a large percentage of blood cells is infected medicine may be given directly into a vein instead of by mouth.\\n\\n\u003cbr\u003e\\n**Sources**\\n\\n- uofmhealth.org\u003cbr\u003e\\nhttps://www.uofmhealth.org/health-library/hw118744\\n\\n- Wikipedia Malaria\u003cbr\u003e\\nhttps://en.wikipedia.org/wiki/Malaria\\n\\n- Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027EDA\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 2. EDA\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### How many files are in each folder?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nuninfected_list = os.listdir(path_uninfected)\\nparasitized_list = os.listdir(path_parasitized)\\n\\nprint(\u0027Uninfected: \u0027, len(uninfected_list))\\nprint(\u0027Parasitized: \u0027, len(parasitized_list))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The data description says there should be 27,558 cell images i.e. 13779 files per folder. We see that each folder has an extra file. Later we\u0027ll check what kind of file this is.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Let\u0027s take a quick look at some images from each class\\n\\nThere are two classes\\n- uninfected\\n- parasitized.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = uninfected_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image = parasitized_list[i]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Check if any non image files are in the folders\\n\\nWe see that there are actually 13780 files in each folder and not 13779 as expected. Folders can sometimes contain files that are automatically created during processing. These files can cause errors in the code if we don\u0027t know they exist - like the code we wrote above to display the images. Let\u0027s check if there are any non image files in the folders. \u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\n# sample image name: C140P101ThinF_IMG_20151005_211735_cell_159.png\\n\\nfor item in uninfected_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Uninfected folder: \u0027, item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Check if any non image files are present in the folder\\n\\nfor item in parasitized_list:\\n    # split the filename into a list\\n    file_list = item.split(\u0027.\u0027)\\n    \\n    # check if the file extension is not png\\n    if file_list[1] != \u0027png\u0027:\\n        print(\u0027Parasitized folder: \u0027,item)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We see that each folder has one non image file called Thumbs.db. Now that we know that these non image files exist we will be sure exclude them later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Put the image names into dataframes\\n\\nHere we will create a dataframe called df_combined that includes both uninfected and parasitized images. This new dataframe will have a column showing the target class of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# create the dataframe\\ndf_uninfected = pd.DataFrame(uninfected_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_uninfected = df_uninfected[df_uninfected[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_uninfected[\u0027target\u0027] = 0\\n\\n\\n# create the dataframe\\ndf_parasitized = pd.DataFrame(parasitized_list, columns=[\u0027image_id\u0027])\\n\\n# remove the non image file\\ndf_parasitized = df_parasitized[df_parasitized[\u0027image_id\u0027] != \u0027Thumbs.db\u0027]\\n\\n# add a target column\\ndf_parasitized[\u0027target\u0027] = 1\\n\\n#print(df_uninfected.shape)\\n#print(df_parasitized.shape)\\n\\n# Combine the two dataframes\\n\\ndf_combined = pd.concat([df_uninfected, df_parasitized], axis=0).reset_index(drop=True)\\n\\n#df_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_combined.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shape.\\n# There should be 27558 rows.\\n\\ndf_combined.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if the image names are unique.\\n# The output should be 27558\\n\\ndf_combined[\u0027image_id\u0027].nunique()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The output above matches the number of rows. This confirms that each image has a unique name. This is important to verify because duplicate image file names will cause errors in the processing code that we will write later.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### What are the image sizes and how many channels does each have?\\n\\nHere we will add the following info on each image to the df_combined dataframe:\\n\\nw = width\u003cbr\u003e\\nh = height\u003cbr\u003e\\nc = number of channels\u003cbr\u003e\\nmax_pixel_value\u003cbr\u003e\\nmin_pixel_value\u003cbr\u003e\\nimage_format\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndef read_image_sizes(file_name):\\n    \\\u0022\\\u0022\\\u0022\\n    1. Get the shape of the image\\n    2. Get the min and max pixel values in the image.\\n    Getting pixel values will tell if any pre-processing has been done.\\n    3. This info will be added to the original dataframe.\\n    \\\u0022\\\u0022\\\u0022\\n    \\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n     \\n    if file_name in uninfected_list:\\n        \\n        path = path_uninfected\\n        \\n    else:\\n        path = path_parasitized\\n    \\n    \\n    image = cv2.imread(path + file_name)\\n    max_pixel_val = image.max()\\n    min_pixel_val = image.min()\\n    img_format = file_name.split(\u0027.\u0027)[1]\\n    output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val, img_format]\\n    return output\\n\\nm = np.stack(df_combined[\u0027image_id\u0027].apply(read_image_sizes))\\ndf = pd.DataFrame(m,columns=[\u0027w\u0027,\u0027h\u0027,\u0027c\u0027,\u0027max_pixel_val\u0027,\u0027min_pixel_val\u0027, \u0027image_format\u0027])\\n\\ndf_combined = pd.concat([df_combined,df],axis=1, sort=False)\\n\\ndf_combined.head(10)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images have 3 channels\\ndf_combined[\u0027c\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check if all images are in png format\\ndf_combined[\u0027image_format\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all black images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 0) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 0)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check for all white images\\nlen(df_combined[(df_combined[\u0027max_pixel_val\u0027] == 255) \u0026 (df_combined[\u0027max_pixel_val\u0027] == 255)])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Display random images from each of the target classes\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022The images are randomly selected. Therefore, different images will be dispayed each time the code is run. Also you\u0027ll see that parasitized images seem to have a blue-ish area that\u0027s not as common in uninfected images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_uninfected[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027uninfected\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_parasitized[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.xlabel(\u0027parasitized\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### EDA Summary\\n\\n- There are 27,558 segmented cell images.\\n- Each folder contains 13,779 images.\\n- The images are of various sizes.\\n- All images are in png format.\\n- All images have 3 channels i.e. all are colour images.\\n- There are no all black or all white images.\\n- The target distribution is balanced i.e. each target class has the same number of images.\\n- All cells were segmented from thin blood smear slides.\\n- The parasite specie on all parasitized images is P. falciparum.\\n- Each folder contains a non image file called Thumbs.db.\\n- The presence of a blue-ish area inside the cell appears to be a common (but not definitive) indicator that a cell is parasitized.\\n\\n\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Create_a_Holdout-Set\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 3. Create a Holdout Set\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022As we continue to train, validate and modify a model we could over time start to overfit the validation data without realizing it. This means that the model will perform well during training but it will perform poorly on unseen data i.e. the app will perform badly in production.\\n\\nHere we will create a holdout set containing 200 images. We will keep this holdout set aside and only use it at the end to check how the final model performs on unseen data.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# shuffle\\ndf_combined = shuffle(df_combined, random_state=101)\\n\\n# create a holdout set with 200 samples\\ndf_holdout = df_combined.sample(NUM_HOLDOUT_IMAGES, random_state=101)\\n\\n# create a list of holdout images\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_holdout.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the shapes.\\n# The ouput should be:\\n# (200, 8)\\n# (27358, 8)\\n\\nprint(df_holdout.shape)\\nprint(df_data.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution in the holdout set.\\n# 0 = uninfected\\n# 1 = parasitized\\n\\ndf_holdout[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a holdout set containing 200 images.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train-Test-Split_Model\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 4. Train-Test-Split Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Train-test-split is not the ideal way to assess model perfromance. However, it\u0027s a good starting point because it\u0027s simple to set up and runs 5 times faster than 5 fold cross validation. This helps us to:\\n\\n- Quickly get a feel for what kind of performance we can expect from this data.\\n- Quickly test different architectures and parameters.\\n- Establish the workflow that will later be used in cross validation.\\n- Check the training curves to see if the model is overfitting.\\n- Check the training curves to establish the number of epochs we will use when training the final model on all data.\\n- Perform error analysis.\\n\\nA train-test-split model is a rough guide. When deciding if a particular change actually improved model performance we will base that decision on cross validation results. This is important. If we don\u0027t use cross validation we may think we are improving but in reality we may be getting nowhere.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Directory Structure\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022To reduce RAM use and prevent this kernel from crashing we will batch feed the images when training the model. We will use generators to do this. In order to use this approach Keras requires that a particular directory structure be set up. Keras uses this structure to automatically infer the class (target) of each image.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check if base_dir has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# see what\u0027s inside base_dir\\nos.listdir(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Train and Val Sets\\n\\nWe will create a stratified val set. This means that the val set will have the same target distribution as the train set. Actually doing stratification isn\u0027t essential here because the target is balanced. Because of this there\u0027s a good chance that randomly selecting rows will still result in the val set having a fairly balanced target distribution. Stratification is more applicable for data where the target is unbalanced.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# select the column that we will use for stratification\\ny = df_data[\u0027target\u0027]\\n\\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\\n\\nprint(df_train.shape)\\nprint(df_val.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Check the target distribution of the val set.\\n# The target should be approx balanced.\\n\\ndf_val[\u0027target\u0027].value_counts()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Transfer the images into the folders\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_data.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n# Get a list of images in each of the two folders\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n        \\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n    \\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n    \\n    \\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n        \\n    \\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n        \\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n        \\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        \\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the number of images in each folder\\n\\n# train\\nprint(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\nprint(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\nprint(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Set Up the Generators\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022train_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Note that here we are normalizing the images inside the generator.\\n# If you wanted to add some data augmentation you could do it here.\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\nval_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create the Model Architecture\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022I found this cnn architecture a while ago in a kernel created by @fmarazzi. I\u0027ve since used this on several projects. It really is a good multi-purpose architecture. Here I\u0027ve modified it slightly by adding a ZeroPadding layer. Later, in the error analysis section, I\u0027ll explain why I added this layer.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-10min-0-925-lb\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\nmodel.summary()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Train the Model\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Please note that we\u0027ve set the learning rate to reduce (decay) at each epoch.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022filepath = \\\u0022model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n                              \\n                              \\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            validation_data=val_gen,\\n                            validation_steps=val_steps,\\n                            epochs=NUM_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Evaluate the model using the val set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# get the metric names so we can use evaulate_generator\\nmodel.metrics_names\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Here the best epoch will be used.\\n\\nmodel.load_weights(\u0027model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\nprint(\u0027val_loss:\u0027, val_loss)\\nprint(\u0027val_acc:\u0027, val_acc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Plot the Training Curves\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# display the loss and accuracy curves\\n\\nimport matplotlib.pyplot as plt\\n\\nacc = history.history[\u0027acc\u0027]\\nval_acc = history.history[\u0027val_acc\u0027]\\nloss = history.history[\u0027loss\u0027]\\nval_loss = history.history[\u0027val_loss\u0027]\\n\\nepochs = range(1, len(acc) + 1)\\n\\nplt.plot(epochs, loss, \u0027bo\u0027, label=\u0027Training loss\u0027)\\nplt.plot(epochs, val_loss, \u0027b\u0027, label=\u0027Validation loss\u0027)\\nplt.title(\u0027Training and validation loss\u0027)\\nplt.legend()\\nplt.figure()\\n\\nplt.plot(epochs, acc, \u0027bo\u0027, label=\u0027Training acc\u0027)\\nplt.plot(epochs, val_acc, \u0027b\u0027, label=\u0027Validation acc\u0027)\\nplt.title(\u0027Training and validation accuracy\u0027)\\nplt.legend()\\nplt.figure()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the labels of the test images.\\n\\ntest_labels = test_gen.classes\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# We need these to plot the confusion matrix.\\ntest_labels\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Print the label associated with each class\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# If you wanted to get the image_id\u0027s to match them to predictions this\\n# is how to do it.\\n\\n# test_gen.filenames\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check the number of predictions\\npredictions.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Source: Scikit Learn website\\n# http://scikit-learn.org/stable/auto_examples/\\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\\n# selection-plot-confusion-matrix-py\\n\\n\\ndef plot_confusion_matrix(cm, classes,\\n                          normalize=False,\\n                          title=\u0027Confusion matrix\u0027,\\n                          cmap=plt.cm.Blues):\\n    \\\u0022\\\u0022\\\u0022\\n    This function prints and plots the confusion matrix.\\n    Normalization can be applied by setting `normalize=True`.\\n    \\\u0022\\\u0022\\\u0022\\n    if normalize:\\n        cm = cm.astype(\u0027float\u0027) / cm.sum(axis=1)[:, np.newaxis]\\n        print(\\\u0022Normalized confusion matrix\\\u0022)\\n    else:\\n        print(\u0027Confusion matrix, without normalization\u0027)\\n\\n    print(cm)\\n\\n    plt.imshow(cm, interpolation=\u0027nearest\u0027, cmap=cmap)\\n    plt.title(title)\\n    plt.colorbar()\\n    tick_marks = np.arange(len(classes))\\n    plt.xticks(tick_marks, classes, rotation=45)\\n    plt.yticks(tick_marks, classes)\\n\\n    fmt = \u0027.2f\u0027 if normalize else \u0027d\u0027\\n    thresh = cm.max() / 2.\\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\\n        plt.text(j, i, format(cm[i, j], fmt),\\n                 horizontalalignment=\\\u0022center\\\u0022,\\n                 color=\\\u0022white\\\u0022 if cm[i, j] \u003e thresh else \\\u0022black\\\u0022)\\n\\n    plt.ylabel(\u0027True label\u0027)\\n    plt.xlabel(\u0027Predicted label\u0027)\\n    plt.tight_layout()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_labels.shape\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022test_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Error_Analysis\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5. Error Analysis\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022In this section we\u0027re not going to use any fancy statistical gymnastics. We\u0027re simply going to look at the images that the model predicted correctly and those it got wrong. We want to see if there are any patterns or issues that could\u0027ve caused the model to make mistakes.\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Put the val image_id, labels and predictions into a dataframe.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# put the val image_id, labels and predictions into a dataframe\\n\\nval_pred_dict = {\\n    \u0027image_id\u0027: test_gen.filenames,\\n    \u0027val_labels\u0027: test_gen.classes,\\n    \u0027val_preds\u0027: predictions.argmax(axis=1)\\n}\\n\\ndf_val_preds = pd.DataFrame(val_pred_dict)\\n\\n\\n# Adjust the file names\\n\\n# sample image name: a_uninfected/C100P61ThinF_IMG_20150918_144104_...\\n# we want just this part: C100P61ThinF_IMG_20150918_144104_...\\n\\ndef adjust_file_names(x):\\n    # split into a list based on \u0027/\u0027\\n    fname = x.split(\u0027/\u0027)\\n    # chose the second item in the list which is the image name\\n    fname = fname[1]\\n    \\n    return fname\\n\\ndf_val_preds[\u0027image_id\u0027] = df_val_preds[\u0027image_id\u0027].apply(adjust_file_names)\\n\\n\\n# savedf_val_preds so we can analyze the results later\\npickle.dump(df_val_preds,open(\u0027df_val_preds.pickle\u0027,\u0027wb\u0027))\\n\\n# code to load the dataframe\\n# df_val_preds = pickle.load(open(\u0027df_val_preds\u0027,\u0027rb\u0027))\\n\\n\\n#df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_val_preds.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# filter out those rows where the model made correct predictions\\ndf_correct = df_val_preds[df_val_preds[\u0027val_labels\u0027] == df_val_preds[\u0027val_preds\u0027]]\\n\\n# filter out those rows where the model made wrong predictions\\ndf_wrong = df_val_preds[df_val_preds[\u0027val_labels\u0027] != df_val_preds[\u0027val_preds\u0027]]\\n\\nprint(df_correct.shape)\\nprint(df_wrong.shape)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the correct predictions\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022df_correct.head()\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_correct[df_correct[\u0027val_labels\u0027] == 1]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 1\u0027, fontsize=20)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected that the model predicted correctly\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_correct[df_correct[\u0027val_labels\u0027] == 0]\\n\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 0\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Analyze the wrong predictions\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are parasitized but the model predicted uninfected\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_1 = df_wrong[df_wrong[\u0027val_labels\u0027] == 1]\\n\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_1[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_parasitized + image))\\n    \\n    plt.tight_layout()\\n    \\n    plt.xlabel(\u0027true: 1, pred: 0\u0027, fontsize=20)\\n    \\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022#### Display images that are uninfected but the model predicted parasitized\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\ndf_0 = df_wrong[df_wrong[\u0027val_labels\u0027] == 0]\\n\\n# Note that this chooses random images. Therefore,\\n# we see diffrent images each time the code is run.\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected/\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized/\u0027\\n\\n\\n# set up the canvas for the subplots\\nplt.figure(figsize=(20,10))\\n\\n# Our subplot will contain 2 rows and 4 columns\\n# plt.subplot(nrows, ncols, plot_number)\\nplt.subplot(2,4,1)\\n\\n# plt.imread reads an image from a path and converts it into an array\\n\\n# starting from 1 makes the code easier to write\\nfor i in range(1,9):\\n    \\n    plt.subplot(2,4,i)\\n    \\n    # get a random image\\n    image_list = list(df_0[\u0027image_id\u0027].sample(1))\\n    image = image_list[0]\\n    \\n    # display the image\\n    plt.imshow(plt.imread(path_uninfected + image))\\n    \\n    plt.xlabel(\u0027true: 0, pred: 1\u0027, fontsize=20)\\n    \u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Observations and Comments\\n\\nIn cases where the label was parasitized but the model predicted uninfected I noticed that the blue-ish area was located close to the edge of the image. Could it be that the model was not detecting important patterns that are located at the edge of images? To address this issue I decided to add a padding layer to the model arcitecture. This layer added a 10 pixel zero padding around each image. I found that the cross validation scores improved after this change.\\n\\n\\nWhen doing this error analysis our premise has been that if there is a mismatch between the label and the prediction then the model has made a mistake. However, there\u0027s another possibility - the model prediction is correct. There are 27,558 images in this dataset. All images were examined and labeled by the same expert. This must\u0027ve been a tedious and time consuming process so it\u0027s possible that some images were incorrectly labeled. \\n\\nAt this point it would be helpful to collaborate with a domain expert in order to discuss possible reasons for the mistakes this model is making. Are they the result of model weakness, incorrect labels or maybe damaged images? Is the correct label easy for a human to classify or would a human struggle to make a correct diagnosis? What are the main indicators that an expert looks for when examining cell images? Is the model seeing things that an expert didn\u0027t notice?\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Cross_Validation\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 5 Fold Cross Validation\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022Here we will simply apply the same workflow that we used for the train-test-split model to 5 folds. For each fold we\u0027ll get the loss, accuracy and auc. Then we\u0027ll average the results of the 5 folds to get the final scores.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true,\u0022_kg_hide-output\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# ==============================\\n# Create the 5 Folds\\n# ==============================\\n\\n# shuffle df_combined and change the name to df_data\\ndf_data = shuffle(df_combined.copy())\\n\\n# train_test_split\\ny = df_data[\u0027target\u0027]\\n\\n# initialize kfold\\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=101)\\n\\n# define y for stratification\\ny = df_data[\u0027target\u0027]\\n\\n# Note:\\n# Each fold is a tuple ([train_index_values], [val_index_values])\\n# fold_0, fold_1, fold_2, fold_3, fold_5 = kf.split(df_train, y)\\n\\n# Put the folds into a list. This is a list of tuples.\\n# y was set above.\\nfold_list = list(kf.split(df_data, y))\\n\\n\\n# ==============================\\n# Loop Through the Folds\\n# ==============================\\n\\n# create a list to store the predictions\\nval_pred_list = []\\n\\n# create a list to store the scores\\nval_acc_list = []\\nval_loss_list = []\\nval_auc_list = []\\n\\n\\nfor i, fold in enumerate(fold_list):\\n\\n    # Delete the image data directory we created to prevent a Kaggle error.\\n    # Kaggle allows a max of 500 files to be saved.\\n    \\n    if os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n        \\n        \\n    \\n    # set df_data\\n    df_data = df_combined.copy()\\n    \\n    print(\u0027=== Fold_\u0027 + str(i) + \u0027 ===\u0027)\\n    print(\u0027\\\\n\u0027)\\n\\n    # map the train and val index values to dataframe rows\\n    df_train = df_data[df_data.index.isin(fold[0])]\\n    df_val = df_data[df_data.index.isin(fold[1])]\\n    \\n\\n\\n\\n    # ==============================\\n    # Create a Directory Structure\\n    # ==============================\\n\\n    # Create a new directory\\n    base_dir = \u0027base_dir\u0027\\n    os.mkdir(base_dir)\\n\\n\\n    #[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n    # now we create 2 folders inside \u0027base_dir\u0027:\\n\\n    # train\\n        # a_uninfected\\n        # b_parasitized\\n\\n    # val\\n        # a_uninfected\\n        # b_parasitized\\n\\n\\n    # create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n    # train_dir\\n    train_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\n    os.mkdir(train_dir)\\n\\n    # val_dir\\n    val_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\n    os.mkdir(val_dir)\\n\\n\\n    # [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n    # Inside each folder we create seperate folders for each class\\n\\n    # create new folders inside train_dir\\n    a_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n    # create new folders inside val_dir\\n    a_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\n    os.mkdir(a_uninfected)\\n    b_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\n    os.mkdir(b_parasitized)\\n\\n\\n\\n    # =================================\\n    # Transfer the Images into Folders\\n    # =================================\\n\\n    # Set the image_id as the index in df_data\\n    df_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n    # Get a list of images in each of the two folders\\n\\n    path_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\n    path_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\n    folder_1 = os.listdir(path_uninfected)\\n    folder_2 = os.listdir(path_parasitized)\\n\\n    # Get a list of train and val images\\n    train_list = list(df_train[\u0027image_id\u0027])\\n    val_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n    # Transfer the train images\\n\\n    for image in train_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(train_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n\\n\\n    # Transfer the val images\\n\\n    for image in val_list:\\n\\n        fname = image\\n        target = df_data.loc[image,\u0027target\u0027]\\n\\n\\n        if target == 0:\\n            label = \u0027a_uninfected\u0027\\n        else:\\n            label = \u0027b_parasitized\u0027\\n\\n\\n        if fname in folder_1:\\n            # source path to image\\n            src = os.path.join(path_uninfected, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n        if fname in folder_2:\\n            # source path to image\\n            src = os.path.join(path_parasitized, fname)\\n            # destination path to image\\n            dst = os.path.join(val_dir, label, fname)\\n\\n            image = cv2.imread(src)\\n            image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n            # save the image at the destination\\n            cv2.imwrite(dst, image)\\n\\n            # copy the image from the source to the destination\\n            #shutil.copyfile(src, dst)\\n\\n    # Print the number of images in each folder\\n\\n    # train\\n    #print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n    # val\\n    #print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n    #print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n    #print(\u0027\\\\n\u0027)\\n\\n\\n    # ==============================\\n    # Set Up the Generators\\n    # ==============================\\n\\n    train_path = \u0027base_dir/train_dir\u0027\\n    valid_path = \u0027base_dir/val_dir\u0027\\n\\n    num_train_samples = len(df_train)\\n    num_val_samples = len(df_val)\\n    train_batch_size = BATCH_SIZE\\n    val_batch_size = BATCH_SIZE\\n\\n\\n    train_steps = np.ceil(num_train_samples / train_batch_size)\\n    val_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\n    datagen = ImageDataGenerator(rescale=1.0/255)\\n\\n    train_gen = datagen.flow_from_directory(train_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=train_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    val_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027)\\n\\n    # Note: shuffle=False causes the test dataset to not be shuffled\\n    test_gen = datagen.flow_from_directory(valid_path,\\n                                            target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                            batch_size=val_batch_size,\\n                                            class_mode=\u0027categorical\u0027,\\n                                            shuffle=False)\\n    \\n    print(\u0027\\\\n\u0027)\\n\\n    # ==============================\\n    # Set Up the Model Architecture\\n    # ==============================\\n\\n\\n\\n    kernel_size = (3,3)\\n    pool_size= (2,2)\\n    first_filters = 32\\n    second_filters = 64\\n    third_filters = 128\\n\\n    dropout_conv = 0.3\\n    dropout_dense = 0.3\\n\\n\\n    model = Sequential()\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                     input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n    \\n    model.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n    \\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size)) \\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\n    model.add(MaxPooling2D(pool_size = pool_size))\\n    model.add(Dropout(dropout_conv))\\n\\n    model.add(Flatten())\\n    model.add(Dense(256, activation = \\\u0022relu\\\u0022))\\n    model.add(Dropout(dropout_dense))\\n    model.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n    #model.summary()\\n\\n\\n\\n    # ==============================\\n    # Train the Model\\n    # ==============================\\n\\n\\n    model.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n                  metrics=[\u0027accuracy\u0027])\\n\\n    filepath = \\\u0022model.h5\\\u0022\\n    checkpoint = ModelCheckpoint(filepath, monitor=\u0027val_acc\u0027, verbose=1, \\n                                 save_best_only=True, mode=\u0027max\u0027)\\n\\n    reduce_lr = ReduceLROnPlateau(monitor=\u0027val_acc\u0027, factor=0.5, patience=2, \\n                                       verbose=1, mode=\u0027max\u0027, min_lr=0.00001)\\n\\n\\n    callbacks_list = [checkpoint, reduce_lr]\\n\\n    history = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                                validation_data=val_gen,\\n                                validation_steps=val_steps,\\n                                epochs=NUM_EPOCHS, verbose=1,\\n                               callbacks=callbacks_list)\\n\\n\\n\\n    # ==================================\\n    # Evaluate the Model on the Val Set\\n    # ==================================\\n\\n    model.load_weights(\u0027model.h5\u0027)\\n\\n    val_loss, val_acc = \\\\\\n    model.evaluate_generator(test_gen, \\n                            steps=val_steps)\\n    \\n    # append the acc score val_scores_list\\n    val_acc_list.append(val_acc)\\n    val_loss_list.append(val_loss)\\n    \\n    \\n    # ==================================\\n    # Calculate the AUC Score\\n    # ==================================\\n\\n    test_labels = test_gen.classes\\n\\n    # make a prediction\\n    predictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n    \\n    # append the predictions to a list\\n    val_pred_list.append(predictions)\\n    \\n    val_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n    \\n    val_auc_list.append(val_auc)\\n    \\n    \\n    \\n    # ==================================\\n    # Print the Fold Scores\\n    # ==================================\\n    \\n    \\n    #print(\u0027\\\\n\u0027)\\n    #print(\u0027Fold_\u0027 + str(i) + \u0027 scores:\\\\n\u0027)\\n    #print(\u0027val_loss:\u0027, val_loss)\\n    #print(\u0027val_acc:\u0027, val_acc)\\n    #print(\u0027val_auc:\u0027, val_auc)\\n    #print(\u0027\\\\n\u0027)\\n\\n    \\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Average for of all 5 folds:\\\\n\u0027)\\n#print(\u0027Average Accuracy: \u0027, avg_acc)\\n#print(\u0027Average Loss: \u0027, avg_loss)\\n#print(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Print the scores for each fold and the average scores\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Print the scores\\n\\nprint(\u0027Val Acc\u0027)\\nfor item in val_acc_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val Loss\u0027)\\nfor item in val_loss_list:\\n    print(item)\\n    \\nprint(\u0027\\\\n\u0027)\\n\\nprint(\u0027Val AUC\u0027)\\nfor item in val_auc_list:\\n    print(item)\\n\\n    \\n    \\n# Calc the average score over the 5 folds\\navg_acc = sum(val_acc_list)/len(val_acc_list)\\navg_loss = sum(val_loss_list)/len(val_loss_list)\\navg_auc = sum(val_auc_list)/len(val_auc_list)\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Average for of all 5 folds:\\\\n\u0027)\\nprint(\u0027Average Accuracy: \u0027, avg_acc)\\nprint(\u0027Average Loss: \u0027, avg_loss)\\nprint(\u0027Average AUC: \u0027, avg_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Train\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 7. Train the Final Model using all the data\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We\u0027re not using validation data in this final model. Therefore, we won\u0027t be able to set up the model to save the best epoch. We\u0027ll need to determine the number of training epochs before starting.\\n\\nTo determine this number I would normally look at the training curves (see train-test-split model) and determine the epoch at which the model started to overfit i.e. the training and validation accuracy curves start to diverge. However, with this data and architecture it appears that overfitting is not a huge problem because the training and validation curves do not diverge significantly. Therefore choosing 10 epochs seems to give a good balance between training time and model quality.\\n\\nIt\u0027s also important to keep in mind that we can train on all data because we set the learning rate to decay at each epoch i.e. the learning rate was scheduled. If we had used a dynamic learning rate (e.g. ReduceLROnPlateau) then we\u0027ll need to have some way of replicating the learning rate changes that happened automatically during 5 fold cross validation. Therefore, using a scheduled learning rate keeps things simple.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true,\u0022_kg_hide-input\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022#\\n# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\\n\\n\\n\\n# ==============================\\n# Set df_train\\n# ==============================\\n\\n# This variable was set above. Just setting it here again for clarity.\\nholdout_images_list = list(df_holdout[\u0027image_id\u0027])\\n\\n# Select only rows that are not part of the holdout set.\\n# Note the use of ~ to execute \u0027not in\u0027.\\ndf_data = df_combined[~df_combined[\u0027image_id\u0027].isin(holdout_images_list)]\\n\\ndf_train = df_data.copy()\\ndf_val = df_holdout.copy()\\n\\n\\n\\n# ==============================\\n# Create a Directory Structure\\n# ==============================\\n\\n# Create a new directory\\nbase_dir = \u0027base_dir\u0027\\nos.mkdir(base_dir)\\n\\n\\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\\n\\n# now we create 2 folders inside \u0027base_dir\u0027:\\n\\n# train\\n    # a_uninfected\\n    # b_parasitized\\n\\n# val\\n    # a_uninfected\\n    # b_parasitized\\n\\n\\n# create a path to \u0027base_dir\u0027 to which we will join the names of the new folders\\n# train_dir\\ntrain_dir = os.path.join(base_dir, \u0027train_dir\u0027)\\nos.mkdir(train_dir)\\n\\n# val_dir\\nval_dir = os.path.join(base_dir, \u0027val_dir\u0027)\\nos.mkdir(val_dir)\\n\\n\\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\\n# Inside each folder we create seperate folders for each class\\n\\n# create new folders inside train_dir\\na_uninfected = os.path.join(train_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(train_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n# create new folders inside val_dir\\na_uninfected = os.path.join(val_dir, \u0027a_uninfected\u0027)\\nos.mkdir(a_uninfected)\\nb_parasitized = os.path.join(val_dir, \u0027b_parasitized\u0027)\\nos.mkdir(b_parasitized)\\n\\n\\n\\n# =================================\\n# Transfer the Images into Folders\\n# =================================\\n\\n# Set the image_id as the index in df_data\\ndf_data.set_index(\u0027image_id\u0027, inplace=True)\\n\\n# Set the image_id as the index in df_data\\ndf_holdout.set_index(\u0027image_id\u0027, inplace=True)\\n\\n\\n\\n# Get a list of images in each of the two folders\\n\\npath_uninfected = \u0027../input/cell_images/cell_images/Uninfected\u0027\\npath_parasitized = \u0027../input/cell_images/cell_images/Parasitized\u0027\\n\\nfolder_1 = os.listdir(path_uninfected)\\nfolder_2 = os.listdir(path_parasitized)\\n\\n# Get a list of train and val images\\ntrain_list = list(df_train[\u0027image_id\u0027])\\nval_list = list(df_val[\u0027image_id\u0027])\\n\\n\\n\\n# Transfer the train images\\n\\nfor image in train_list:\\n\\n    fname = image\\n    target = df_data.loc[image,\u0027target\u0027]\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(train_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n\\n\\n# Transfer the val images\\n\\nfor image in val_list:\\n\\n    fname = image\\n    target = df_holdout.loc[image,\u0027target\u0027]\\n\\n\\n    if target == 0:\\n        label = \u0027a_uninfected\u0027\\n    else:\\n        label = \u0027b_parasitized\u0027\\n\\n\\n    if fname in folder_1:\\n        # source path to image\\n        src = os.path.join(path_uninfected, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n    if fname in folder_2:\\n        # source path to image\\n        src = os.path.join(path_parasitized, fname)\\n        # destination path to image\\n        dst = os.path.join(val_dir, label, fname)\\n\\n        image = cv2.imread(src)\\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\\n        # save the image at the destination\\n        cv2.imwrite(dst, image)\\n\\n        # copy the image from the source to the destination\\n        #shutil.copyfile(src, dst)\\n\\n# Print the number of images in each folder\\n\\n# train\\n#print(len(os.listdir(\u0027base_dir/train_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/train_dir/b_parasitized/\u0027)))\\n\\n# val\\n#print(len(os.listdir(\u0027base_dir/val_dir/a_uninfected\u0027)))\\n#print(len(os.listdir(\u0027base_dir/val_dir/b_parasitized/\u0027)))\\n#print(\u0027\\\\n\u0027)\\n\\n\\n# ==============================\\n# Set Up the Generators\\n# ==============================\\n\\ntrain_path = \u0027base_dir/train_dir\u0027\\nvalid_path = \u0027base_dir/val_dir\u0027\\n\\nnum_train_samples = len(df_train)\\nnum_val_samples = len(df_val)\\ntrain_batch_size = BATCH_SIZE\\nval_batch_size = BATCH_SIZE\\n\\n\\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\\nval_steps = np.ceil(num_val_samples / val_batch_size)\\n\\n\\ndatagen = ImageDataGenerator(rescale=1.0/255)\\n\\ntrain_gen = datagen.flow_from_directory(train_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=train_batch_size,\\n                                        class_mode=\u0027categorical\u0027)\\n\\n\\n# Note: shuffle=False causes the test dataset to not be shuffled\\ntest_gen = datagen.flow_from_directory(valid_path,\\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\\n                                        batch_size=val_batch_size,\\n                                        class_mode=\u0027categorical\u0027,\\n                                        shuffle=False)\\n\\nprint(\u0027\\\\n\u0027)\\n\\n# ==============================\\n# Set Up the Model Architecture\\n# ==============================\\n\\n\\nkernel_size = (3,3)\\npool_size= (2,2)\\nfirst_filters = 32\\nsecond_filters = 64\\nthird_filters = 128\\n\\ndropout_conv = 0.3\\ndropout_dense = 0.3\\n\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027, \\n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\\n\\nmodel.add(ZeroPadding2D(padding=(PADDING, PADDING), data_format=None))\\n\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(Conv2D(first_filters, kernel_size, activation = \u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size)) \\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(second_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(Conv2D(third_filters, kernel_size, activation =\u0027relu\u0027))\\nmodel.add(MaxPooling2D(pool_size = pool_size))\\nmodel.add(Dropout(dropout_conv))\\n\\nmodel.add(Flatten())\\nmodel.add(Dense(256, activation = \\\u0022relu\\\u0022))\\nmodel.add(Dropout(dropout_dense))\\nmodel.add(Dense(2, activation = \\\u0022softmax\\\u0022))\\n\\n#model.summary()\\n\\n\\n\\n# ==============================\\n# Train the Model\\n# ==============================\\n\\n\\nmodel.compile(Adam(lr=0.0001, decay=1e-6), loss=\u0027binary_crossentropy\u0027, \\n              metrics=[\u0027accuracy\u0027])\\n\\n# we are saving the model based on training accuracy\\nfilepath = \\\u0022final_model.h5\\\u0022\\ncheckpoint = ModelCheckpoint(filepath, monitor=\u0027acc\u0027, verbose=1, \\n                             save_best_only=True, mode=\u0027max\u0027)\\n\\n\\ncallbacks_list = [checkpoint]\\n\\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \\n                            epochs=NUM_FINAL_MODEL_EPOCHS, verbose=1,\\n                           callbacks=callbacks_list)\\n\\n\\n# ==================================\\n# Evaluate the Model on the Val Set\\n# ==================================\\n\\nmodel.load_weights(\u0027final_model.h5\u0027)\\n\\nval_loss, val_acc = \\\\\\nmodel.evaluate_generator(test_gen, \\n                        steps=val_steps)\\n\\n\\n# ==================================\\n# Calculate the AUC Score\\n# ==================================\\n\\ntest_labels = test_gen.classes\\n\\n# make a prediction\\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)\\n\\n\\nval_auc = roc_auc_score(test_labels, predictions.argmax(axis=1))\\n\\n\\n\\n\\n# ==================================\\n# Print the Scores\\n# ==================================\\n\\n#print(\u0027\\\\n\u0027)\\n#print(\u0027Accuracy: \u0027, val_acc)\\n#print(\u0027Loss: \u0027, val_loss)\\n#print(\u0027AUC: \u0027, val_auc)\\n\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Evaluate\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 8. Evaluate the Final Model on the Holdout Set\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# ==================================\\n# Print the Scores\\n# ==================================\\n\\nprint(\u0027\\\\n\u0027)\\nprint(\u0027Accuracy: \u0027, val_acc)\\nprint(\u0027Loss: \u0027, val_loss)\\nprint(\u0027AUC: \u0027, val_auc)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Confusion Matrix\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# argmax returns the index of the max value in a row\\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))\\n\\ntest_gen.class_indices\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Define the labels of the class indices. These need to match the \\n# order shown above.\\ncm_plot_labels = [\u0027uninfected\u0027, \u0027parasitized\u0027]\\n\\nplot_confusion_matrix(cm, cm_plot_labels, title=\u0027Confusion Matrix\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Create a Classification Report\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Get the filenames, labels and associated predictions\\n\\n# This outputs the sequence in which the generator processed the test images\\ntest_filenames = test_gen.filenames\\n\\n# Get the true labels\\ny_true = test_gen.classes\\n\\n# Get the predicted labels\\ny_pred = predictions.argmax(axis=1)\\n\\n\\n# Generate a classification report\\n\\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\\n\\nprint(report)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022**Recall** = Given a class, will the classifier be able to detect it?\u003cbr\u003e\\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?\u003cbr\u003e\\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\u003cbr\u003e\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\u003chr\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022We now have a trained model that can be incorporated into the web app. The metrics we calculated above all look very good meaning that the model should perform well on unseen data.\\n\\nAlso, because we\u0027ve used a simple architecture the model size will be less than 10 MB. This means that it will download quickly. Therefore, the web page will load fast and the overall user experience will be good.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Convert\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022## 9. Convert the final model from Keras to Tensorflow.js\\n\\nThis conversion needs to be done so that the model can be loaded into the web app.\u0022},{\u0022metadata\u0022:{\u0022_kg_hide-output\u0022:true,\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# --ignore-installed is added to fix an error.\\n\\n# https://stackoverflow.com/questions/49932759/pip-10-and-apt-how-to-avoid-cannot-uninstall\\n# -x-errors-for-distutils-packages\\n\\n!pip install tensorflowjs --ignore-installed\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Use the command line conversion tool to convert the model\\n\\n!tensorflowjs_converter --input_format keras final_model.h5 tfjs/model\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the folder containing the tfjs model files has been created\\n!ls\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# check that the tfjs files exist\\nos.listdir(\u0027tfjs/model\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Delete the images that were moved around\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022# Delete the image data directory we created to prevent a Kaggle error.\\n# Kaggle allows a max of 500 files to be saved.\\n\\nif os.path.isdir(\u0027base_dir\u0027) == True: # return true if the directory exists\\n    \\n        shutil.rmtree(\u0027base_dir\u0027)\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Citations\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Citations\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022\\n- Pre-trained convolutional neural networks as feature extractors toward improved Malaria parasite detection in thin blood smear images. PeerJ6:e4568\u003cbr\u003e\\nRajaraman S, Antani SK, Poostchi M, Silamut K, Hossain MA, Maude, RJ, Jaeger S, Thoma GR. (2018)\u003cbr\u003e\\nhttps://peerj.com/articles/4568/\\n\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Reference_Kernels\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Reference Kernels\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Gabriel Preda, Honey Bee Subspecies Classification\u003cbr\u003e\\nhttps://www.kaggle.com/gpreda/honey-bee-subspecies-classification\\n\\n- Francesco Marazzi, Baseline Keras CNN\u003cbr\u003e\\nhttps://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\\n\\n- Kimmo Sskilahti, Image processing with scikit-image\u003cbr\u003e\\nhttps://www.kaggle.com/ksaaskil/image-processing-with-scikit-image\\n\\n- Marsh, Skin Lesion Analyzer\u003cbr\u003e\\nhttps://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Helpful_Resources\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Helpful Resources\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022- Excellent tutorial series by deeplizard on how to use Tensorflow.js to build a web app.\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HEQDRWMK6yY\\n\\n- Tutorial by Minsuk Heo on Accuracy, Precision and F1 Score\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=HBi-P5j0Kec\\n\\n- Tutorial by Data School on how to evaluate a classifier\u003cbr\u003e\\nhttps://www.youtube.com/watch?v=85dtiMz9tSo\\n\\n- Tensorflow.js gallery of projects\u003cbr\u003e\\nhttps://github.com/tensorflow/tfjs/blob/master/GALLERY.md\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022| \u003ca id=\u0027Conclusion\u0027\u003e\u003c/a\u003e\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022### Conclusion\u0022},{\u0022metadata\u0022:{},\u0022cell_type\u0022:\u0022markdown\u0022,\u0022source\u0022:\u0022This is the Foldscope Origami Microscope.\u003cbr\u003e\\nhttps://youtu.be/ky-cqSI5mwE\\n\\nThe microscope image could perhaps be photographed using phone camera and then processed using a web app.\\n\\n\\nThis was a software solution. However, there could be some who are reading this who may be interested in combining the power of Ai with electronics - by building robots, analyzers and other devices. You probably don\u0027t know where to start. The good news is that if you can code then you can also build electronic devices. The principles are the same. It\u0027s just that one uses software and the other uses physical components. \\n\\nThese two practical Udemy courses are a great place to start your journey towards world domination. They are geared towards young students and, most importantly, the instructor is an excellent teacher. The courses were developed with deaf students in mind.\\n\\nElectronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/analog-electronics-robotics-learn-by-building/\\n\\nDigital Electronics and Robotics\u003cbr\u003e\\nhttps://www.udemy.com/digital-electronics-robotics-learn-by-building-module-ii/\\n\\n\\nMany thanks to Arunava for making this interesting dataset available on Kaggle.\\n\\nThank you for reading.\u0022},{\u0022metadata\u0022:{\u0022trusted\u0022:true},\u0022cell_type\u0022:\u0022code\u0022,\u0022source\u0022:\u0022\u0022,\u0022execution_count\u0022:null,\u0022outputs\u0022:[]}],\u0022metadata\u0022:{\u0022kernelspec\u0022:{\u0022display_name\u0022:\u0022Python 3\u0022,\u0022language\u0022:\u0022python\u0022,\u0022name\u0022:\u0022python3\u0022},\u0022language_info\u0022:{\u0022codemirror_mode\u0022:{\u0022name\u0022:\u0022ipython\u0022,\u0022version\u0022:3},\u0022file_extension\u0022:\u0022.py\u0022,\u0022mimetype\u0022:\u0022text/x-python\u0022,\u0022name\u0022:\u0022python\u0022,\u0022nbconvert_exporter\u0022:\u0022python\u0022,\u0022pygments_lexer\u0022:\u0022ipython3\u0022,\u0022version\u0022:\u00223.6.4\u0022}},\u0022nbformat\u0022:4,\u0022nbformat_minor\u0022:1}","dateCreated":"2019-05-28T04:38:47.9095886Z"},"resources":null,"isolatorResults":"\u003cresults\u003e\u003cdisk_kb_free\u003e4594240\u003c/disk_kb_free\u003e\u003cdocker_image_digest\u003e742fa26e1d0b8e816011492dfa78fe0213a1f3cb60b57c8b6ef07909925a70b7\u003c/docker_image_digest\u003e\u003cdocker_image_id\u003esha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241\u003c/docker_image_id\u003e\u003cdocker_image_name\u003egcr.io/kaggle-private-byod/python\u003c/docker_image_name\u003e\u003cexit_code\u003e0\u003c/exit_code\u003e\u003cfailure_message /\u003e\u003cinvalid_path_errors\u003eFalse\u003c/invalid_path_errors\u003e\u003cout_of_memory\u003eFalse\u003c/out_of_memory\u003e\u003crun_time_seconds\u003e11065.347707848\u003c/run_time_seconds\u003e\u003csucceeded\u003eTrue\u003c/succeeded\u003e\u003ctimeout_exceeded\u003eFalse\u003c/timeout_exceeded\u003e\u003cused_all_space\u003eFalse\u003c/used_all_space\u003e\u003cwas_killed\u003eFalse\u003c/was_killed\u003e\u003c/results\u003e","runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/gpu.Dockerfile","dockerHubUrl":"https://gcr.io/kaggle-gpu-images/python","dockerImageDigest":"742fa26e1d0b8e816011492dfa78fe0213a1f3cb60b57c8b6ef07909925a70b7","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-gpu-images/python","diskKbFree":4594240,"failureMessage":"","exitCode":0,"queuedSeconds":0,"outputSizeBytes":0,"runTimeSeconds":11065.347707848,"usedAllSpace":false,"timeoutExceeded":false,"isValidStatus":false,"wasGpuEnabled":false,"wasInternetEnabled":false,"outOfMemory":false,"invalidPathErrors":false,"succeeded":true,"wasKilled":false},"dockerImageVersionId":25160,"usedCustomDockerImage":false,"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":""}],"useNewKernelsBackend":null,"isGpuEnabled":true,"isTpuEnabled":false,"isInternetEnabled":true},"author":{"id":1086574,"displayName":"Marsh","email":null,"editedEmail":null,"editedEmailCode":null,"userName":"vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","profileUrl":"/vbookshelf","registerDate":"0001-01-01T00:00:00Z","lastVisitDate":"0001-01-01T00:00:00Z","statusId":0,"performanceTier":2,"userRoles":null,"userLogins":null,"groupIds":null,"duplicateUsers":null,"hasPhoneVerifications":false,"failedNerdchas":0,"hasPendingNerdcha":false,"deleteRequests":null,"userAttributes":null,"isAdmin":false,"isKaggleBot":false,"isAnonymous":false,"canAct":false,"canBeSeen":false,"thumbnailName":null,"isPhoneVerified":false},"baseUrl":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app","collaborators":{"owner":{"userId":1086574,"groupId":null,"groupMemberCount":null,"profileUrl":"/vbookshelf","thumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1086574-kg.jpg","name":"Marsh","slug":"vbookshelf","userTier":2,"joinDate":null,"type":"owner","isUser":true,"isGroup":false},"collaborators":[]},"initialTab":null,"log":"[{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook__.ipynb to notebook\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.6876314699911745\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Executing notebook with kernel: python3\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 5.90076449199114\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.301794: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 200.85943224398943\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.304988: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc427a7020 executing computations on platform Host. Devices:\\n2019-05-28 04:42:10.305032: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): \u003cundefined\u003e, \u003cundefined\u003e\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 200.8657584189932\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.476988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.0341933269956\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.478124: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bc4285b760 executing computations on platform CUDA. Devices:\\n2019-05-28 04:42:10.478440: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.03845160298806\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:10.485884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \\nname: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\\npciBusID: 0000:00:04.0\\ntotalMemory: 15.90GiB freeMemory: 15.61GiB\\n2019-05-28 04:42:10.485930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.0499695509934\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:11.211375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\\n2019-05-28 04:42:11.211458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \\n2019-05-28 04:42:11.211476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.76856156899885\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:11.218924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15121 MB memory) -\u003e physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 201.77562138000212\n},{\n  \u0022data\u0022: \u00222019-05-28 04:42:12.999853: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 203.55696834999253\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 1821.7109596949886\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 9013.67850043399\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] WARNING | Timeout waiting for IOPub output\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 10980.866831804\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Writing 1968811 bytes to __notebook__.ipynb\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11060.63353391699\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Converting notebook __notebook__.ipynb to html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11063.305635205994\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Support files will be in __results___files/\\n[NbConvertApp] Making directory __results___files\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.712728618993\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.717234862997\n},{\n  \u0022data\u0022: \u0022[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Making directory __results___files\\n[NbConvertApp] Writing 555278 bytes to __results__.html\\n\u0022,\n  \u0022stream_name\u0022: \u0022stderr\u0022,\n  \u0022time\u0022: 11064.729104304992\n}]","outputFiles":[{"ownerInfo":null,"kernelVersionOutputFileId":32169077,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard2of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard2of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..lVWW8fJDfDhP_QH84jKWSQ.TRrs1hy06z2HLvHh7NM0JqrKyGaDooL2-PuCY693Q193hKtNhNxg9TyzreybB39bP6gdranC8xQN649iRXv09L51GgUtOWGzUlmOCUFZuU_LyM0oqZDuojLpOuTDLaEd8dRUQXq57sa1MFrSHzpGlDPlS7v4o9vtgwL7ibe1O-ILCSoC7C-ANSo8jEAMU_ky-Tpobc2Utl7J5SWaGMF2Nogxf07rLoDk1uGp2lHWon_c6s5t2FX-MQdOWEqoUG4b11yO5aIaDTVbpPOQzafdh08uEgoHSnTv0FUYVzD_hkecRQHKkG7IMM7ZKZ_mrWaZTebMleOcKLpEnDD5oWXevGZizqdURfGrwtZOQzj1sanTGDEHq05DAid50g_R43TdyxpwqVmnlama4wjR1Z5FRvpSaqGh_RG5xsnIw8lK_NUcjy9zIsIopMIl-mgvRfXj92t2sdqUPnq9W52gfgNd0NOpMZaeOhvmLa12SNRJ9KsrhBUQBRoAsRjImM46OnzGgnh_rLeO0crYRAJ3qxjUYEizSOpx6KZ3ZhwsvUuUzDO6sbTjQzMSJSsLXAM5nf1Rd-J6Ip5ErMLI08u9d1b02iJPuf0oJYvW6eHtMLlij0CIyLtDskTCmVJyzih3kObeLsBw4j7r7A3Fhc5awMnSP_EL_nnRO4F88bXJbKvVx8g.YBRUfgoYYwRSDkfdSk9BVg/tfjs/model/group1-shard2of3.bin","fileType":".bin","contentLength":4194304,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard2of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169078,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard1of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard1of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..WiD7ljU4zJJzCXZmIYID4g.g2GIiZuBUPU7o-xvzarv3qnHNqrixlBP55f99zF_7Y1wWwatx51W-9pxGpjh49XDjwibfiYuLwsMWitwyxeVlYSdwcjrZpXktfJZ3WeTpyZzm1ebMEwkcxjB4Qqe1wdSmlUjvgETkGu-Gwsu9B7Z569OmFBIN87OA4A4oKs11wyb9NakU2FEELpJlFHghwqo9OizC8ogP-qxMHeiQOCJpYobXQKBVAZazcN-sPCmm-X99nMQefdCPztACWHhbsSzXm1r4pnwhKNUsJ6zzL4IhvPP8DOcIeb4jkSA_Wmw-g5z_kTqpVicH6lRlYmtPZdtyGNA2TdtHSgeGH7NjtJToWgbsdtZweru3Ku-x2odNm3eVusytVjH8_RrGFf3bB2FCiZNWv4aYvGswAet5SQG42BUqiJIz95VwOgZMrY9_7koPALSwQyGRfn66RiN_ytIaldRlY2qrQV7Yfjt69W4qtKNJxOqr79OINWVGSk-JxmMRwHV-aG7pxqIs9XWKrj_zR2q1zDOshtfD3KTbIMVW1ExFSfYA_2p9PP9ApaEe86MzutZ8hpnN1oMinYKNDxrSNa2zqKzCfr_nlC7UTQObskL2S0PrxZ6cx5WVOq5bXBRIOdBf2aaps05BntpA6zoQ5n3oM7HnsDtv85MLNjaly8X9pg96V5C2ul0P0dd59I.6ly4S4FsvOjCQpGvrlEM4Q/tfjs/model/group1-shard1of3.bin","fileType":".bin","contentLength":4194304,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard1of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169079,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"model.h5","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/model.h5","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LRVrI4PINN63zY3wCl0PlA.lcxfMM503itGlJCPPw5CRoFLFIbWjINWwElVuu7g0Ga5G-lmywueFpyzYFSC3HE4ax19p2qmVZb8BaFMZziWwhvgITl7EVnCnVflNGmAj61ymj2xudifo5RSNvOp7n5s0-b4UjUB_v1PSLb23KWxiLi4Z0rzzuqGjWeqn86uBWfi35J8DNKmKOoTXaUZ42bzkL6cA24UTLNcvsFptljihhV7pg-taGUv_O5lkuNqM9eyGJTX4xhAI3EwlJluTHHY4q2owc8Ah8XCmWJqNoM6x_E3N5UENEq_Z1ldrcxXLeU-hy9l3wRp5pxU_XpYdQbwyG27Q9lvczeT2QdCzJ2VTxNPmcTUDREj9hDZLyAsbdMhpPg7ym6fbz53l3nCEYbryZshbnM7oYChowZjReYZy0ePm6NurLlYIZW--rMX5bJF8L2wnma4yg0Fjc5ZaeHNnogMvZP_mxqQx67M8ZjXayMRZzvxU0TkmisxWQXh3USFHO62Zx-WLwx41CZCH42aX-hGG_3HRdbsvws69Bxczicd5cURCz8R-X9yZ6yxZmbHa7lr3oA-_cCUFoFSJRUl6hnAB4pTOmq4apYfsgfZMu0AIu3S6bMKTkrAQFgNA9owTqUHjvi34FS7VsnwDeP3-Q1I3VZxoaxdQCRO4AsnmNbszhRA4zCxuUM_G497VNE.3yUfgPWFjIwXnkrC_qlVUQ/model.h5","fileType":".h5","contentLength":37715208,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"model.h5","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169080,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"final_model.h5","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/final_model.h5","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..dACm-iYru0dwus9_VgjV3A.MUmYStVpEgKjPo7iccka0YFL8CEzjtxn03JS6B2XBLsg28W04XXalRZdGWIefJF2Oh20JrGDyMuGanVHQqanRR7lUNMJM4zUMHxoBAdwK8rHtUV6dyrcsAcgmb2th4Vne1KQHX3oZRdEdGboKX1c3vIJe94X4bO_5IJDJ6N0TAN09jphXWhXGJMx6kjWGcMkDUYmKnRDxU4T61go5L-ub4bAUsedohygc_O5VnyIbPypVjwJPQ8Fze16suworvUEGEbkJq9pV6t2xZru80RqLVv7TwLsc66ZIVGwC8Re5ExIsd43Pq7RSkeB1KgLlz3BQ1kJDFbXq3togGq0oHbJcYYwJsjbLET1xvCbr-sZzp3AvGNhnqoHMy0Eas45f9WnLqWdasGmUkjYSoummnbbseK5puZ3uPilHVH5xaMeFHDwpit0FZ-m0lNIJdS3K0AeIUl34sJURjv0npgPvkYHNrTL1KShkvFkjPVvIf2wPROrrTSNCbLtvXqgxBeQanbdNkUjbp73Usfqc0QBCVnEU1kNJsOnRMSV0ybp0WuZwYGz-ij8psVkQuHf8SXfi8O0GxGRcz3XrkMTI9StJxOJbT4EYU3CIse5XTXLDwG1e4h8PAeBgqZENKX-O7pR3vqxDFopi1TNHWiLnSpU-lDBIE278IwbZBZG2XNlaTLSR1c.114t6UIYooOpweIgr1IKvA/final_model.h5","fileType":".h5","contentLength":37715208,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"final_model.h5","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169081,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"df_val_preds.pickle","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/df_val_preds.pickle","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..AqqX0iJHMyn3qHyUZ8WIqg.EbTVxjoImv7BzrgJMcgvKRySlxVF4uthis9atDkUOAi8XgVtNJf005R_S17bpuReiXLtoJpU1aJVUpqOltA5BpjdC2uLjOegUBLx4Fq7N2_r4etFenBpAbYmAdPvuLDK93Y3aRuQMVQ5Hotd1kMFZG7fYRBozMZoMbWP4sNdu_X6_Q8e_GL9VaccJU3pkuLhE43_N7J1DevO0w0OUj525_YlHiKTtqsTbSngqDv5vdMYYGcTqqy28DcoGuEaj0WYye98JbUqyTGGcySitQdWVcAS0oKdy-27FDGfofos0qp0warEA7KGJv55lazMaskiXjSyFTGRzk2v59D7-BGM1Z_0-PzN9rXabAKV1Q-7OOL2N1So6hktMiOse0n1cwwHTrzIK6Xh7UQtE5c0S8lZ9pd1f42494KI1G4PwUh85vHbPdW1RsX8VrKgfw3eVZRwYbK_2ZgIHeEfDyRCYIbrgGFGyJcMxio9ZdAwX2JrI4UbWDEgAt0ltOdFzBwyGVBVyIplxtIs-exXDfxtA1czxFzH5F-M-AFJOi_K_d3b9H2h3Xu72WwyL_qmcLw_u74Eb--9-3DHxi3U3q6wRkokJbTSEOpElpO-OkwOu_CS1Ta0lwgzzvAwM5y-74aPaeZa-313fh4O5lDWFJWHZ-h4Cu5-M7AjsblJjiMsec4GIt4.7zNmuNlG7NKcOeciyaSDKg/df_val_preds.pickle","fileType":".pickle","contentLength":274044,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"df_val_preds.pickle","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169086,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/group1-shard3of3.bin","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/group1-shard3of3.bin","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XjaZ3Mm2d920HGjZT1RgwA.SQlSKi2pA62l_Mw8Btgbz9fSafNcA0wn_hPwjTrK5AZ_NCJMb1JLpLMBLeOcg4Fca0-qmPCc27nGbiMQHuGWscJ1m2e-4jGTZ_2K42R2y3JSdC8UsfQ_8GD5q-M0VbSQ5BbuZdFkRFcRt0h9q91KKAMauLF0CBoI1BpYM0aVVQ8g3h6FRmvH0eWtIjtE95mmCa8-111Gua20a78GfYoZ6_XE4khpmW_Xcm-I5Qf2wuHuC1tNZ1wg88MXE6_Bn_5Tec83DDXtC-733VOPQAQ3b8I5YJqIQSs61xi8TO0yhloPleAGeb-NFHjGg2wxI8-Y1FxmB_rq7Z8vHFFn-ClFTq2i69OjkBoVXX6wcfqkWmhXEGnt3VouMWUNYnVruUyxMTYDD0DgOaNCpSZ3_JWNTSmNxjg1GVFcM5qFznyE7ofUKaI4oXvgXoGThReuugksghwyvwAR7UtX4HJUhXTGPefjNfPx-UurS2zOj4NRBTGs-wbP2nXxBc4yPdMBI4Y0xCQ9qOc6Wu-KZgCGP6h4CRdLqcUEG0I_JV6kINqYMwSaUij_3h8qOiyl7NAVhY3GsmDiowFoO6E3rFbPB8B08IxtcDXoRlqD0qNuMcgWsKWUiC-gxYFLq15_WMyDoi4xPD_466eqxU11valVIYGTKCKTDeQK0I8DKVdlVZlvDaA.ZHLnauLEIU6K-HhBRejiBA/tfjs/model/group1-shard3of3.bin","fileType":".bin","contentLength":4154376,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"group1-shard3of3.bin","description":null},{"ownerInfo":null,"kernelVersionOutputFileId":32169087,"kernelVersionId":14798337,"kernelId":4009178,"size":0,"fullPath":"tfjs/model/model.json","previewUrl":"/notebooks/preview.json/14798337/12cf1fd6-667c-2d86-4228-ef338bd83b15/tfjs/model/model.json","downloadUrl":"https://www.kaggleusercontent.com/kf/14798337/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..cYUaeaPl_7RVxbb-6J-8LQ.vaaomLJk1ILg217mggLesaoGkBUc0w1iIk9RjkpgNCrWhJuP8VHgCtnBXqn6h2G792y3tnr1OAcO-2uhRs9VG8l6Q-wSSOLepq6m6D8_1GPrQwBVmV2s10OE4OCYp_GjAVaDPV89ldiZDEt0llWcDbV7KmmLEm4lWNKlhnb2tAlS5vNF1gOCWjwaNQDxAHAHvQsUI_6Q2C50nJo5J7eua0xQ2f68l1GE7jOeKhypuJNvLPaAMVz7ghCTkqZogiHD1fMYVWcY6VMU_N5COAEG_RNoHqhmCPjfqgkjNL46mgOkJe4S4uSE6Ro8ZPTc36GWxFa8a3ohjgWd44vnCvHAOJA-TihBXE3CwDyh_dkJ4g1inmtAjLg3vtA2IXkBM3tnSWCYskctq1c9fUimE-fdcIse6R5u733j27qYoDYiGmPCF8fPcF8xQhJKxD_9QJ_QOBfOEyrdtNzhlG8MDDsWyryinb5gI1yHxHFlepZKhPaKiegIXLM2viXGcCZiOQ7jTR__v4GiBxCasqFbRAv9c2UO1V5aaEGIl-vg4Vbwa5VNbGKjHFvRK7-Ai-NugvgMQdo9dZD733AeL4UI0vPuFfBUn7yRD_knUB9TBofXi6PQSbGZeyVjJoH9Wq1kzjmudBo-NEIKlUmpjObS7WRmVwnUsioNFkwzH-II5mwieAw.IJNpmdcyI73jK7k95cWULQ/tfjs/model/model.json","fileType":".json","contentLength":10029,"contentType":null,"contentMD5":null,"validationErrors":null,"type":"kernelOutputFile","collapsed":false,"info":{"metrics":null,"archiveInfo":null,"archiveInfoV2":null,"blobFileInfo":null,"convertCsvInfo":null,"kernelReference":null},"settings":{"csvSettings":null,"bigQuerySettings":null,"bigQueryMirrorSettings":null,"storageSettings":null,"remoteUrlSettings":null,"remoteGithubSettings":null},"render":null,"totalChildren":0,"children":[],"firestorePath":null,"firestoreKey":null,"name":"model.json","description":null}],"outputFilesCropped":false,"ouputFilesOwnerInfo":{"databundleVersionId":0,"dataset":null,"competition":null,"kernel":{"kernelId":4009178,"kernelVersionId":14798337,"dataviewToken":"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..DDtJV_v34D-CAsLmBVtRBQ.bSbDZ99PreI7pBgaqxr5dRjpLfZp0vI71eRpyotj2rysdD8hJoML_9OxHKd-D3ttzPW0h3jlX3OFZgipcCZag08Ufl1pbWNZNRWKMrkvrkXiWR5Cb944mLA2vTcaqvyjx5JBg4v2Mo0wfHPxTuetxv10-hxkoPjGGrJ10l43CbW0SJfB4J26Tlgm0uGUY1ocMJcLNsADubtSuDLIrt2Nz2xOg0GEYqDLQ4NSLJyvQDL3fWsrYf9PVbRoCCaDcqS88-CJwMlkfewWZkhUElhhrBNthI4XCRsqNajJP2iv_Sf54XNHr49lemEyS9M-AJYBGUlbPmc0AAA4eTCbw0AxchgzYZHrFrPIQt5eZ75br_OkxMktwZ7gZAajINwAZzXbC5gDRcXBYI7dLWVYkH-BzbEfudeabL_0eufKS1V7mYi7AqqNMkcfzxxTX4SmHZ_g3kHw-r2G3nqS3-RSRNFov-8lYsY-k1g-hZVMB-I9cq-bN9_kBOopkIpwtYGbnh3H.j3wvSGDU7_fEQAejzbXkFA","scope":"vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app"},"previewsDisabled":false},"pageMessages":[],"dataSources":[{"sourceType":"DatasetVersion","sourceId":200743,"databundleVersionId":null,"mountSlug":""}],"versions":[{"id":14798337,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-28T04:38:48.057Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":2,"linesInsertedFromPrevious":1,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":11065.347707848,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14798337","versionNumber":3,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"},{"id":14798128,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-28T04:32:14.59Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":9,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":10431.400544859,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14798128","versionNumber":2,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"},{"id":14615343,"kernelVersionId":null,"isForkParent":false,"isNotebook":true,"languageName":"Python","lastRunTime":"2019-05-24T06:23:47.033Z","linesChangedFromPrevious":0,"linesDeletedFromPrevious":0,"linesInsertedFromPrevious":2143,"outputFilesTotalSizeBytes":0,"runInfo":{"dockerfileUrl":"https://github.com/Kaggle/docker-python/blob/master/Dockerfile","dockerHubUrl":"https://registry.hub.docker.com/u/kaggle/python/","dockerImageId":"sha256:9e5f85caa66f12ab18b2ae4d1154618b06409b9db764e5760bfa9d9c74bc3241","dockerImageName":"gcr.io/kaggle-private-byod/python","exitCode":0,"failureMessage":"","isValidStatus":true,"runTimeSeconds":10124.893342878,"succeeded":true,"timeoutExceeded":false,"usedAllSpace":false},"status":"complete","title":"Malaria Cell Analyzer + Tensorflow.js Web App","url":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app?scriptVersionId=14615343","versionNumber":1,"hasVersionNumber":true,"isRedacted":false,"versionAuthor":null,"versionType":"batch"}],"categories":{"type":"notebook","tags":[{"displayName":"GPU","fontAwesomeIcon":null,"id":16580,"name":"gpu","fullPath":"admin \u003e accelerators \u003e gpu","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27gpu%27","description":"This is for content that uses or relates to GPUs","isInherited":false,"datasetCount":31,"competitionCount":0,"scriptCount":1694707,"totalCount":1694738,"tagUrl":"/tags/gpu"},{"displayName":"CNN","fontAwesomeIcon":null,"id":13415,"name":"cnn","fullPath":"technique \u003e neural networks \u003e cnn","listingUrl":"/kernels?sortBy=relevance\u0026group=all\u0026search=tag%3A%27cnn%27","description":null,"isInherited":false,"datasetCount":170,"competitionCount":0,"scriptCount":1821,"totalCount":1991,"tagUrl":"/tags/cnn"}]},"submitToCompetitionInfo":null,"downloadAllFilesUrl":"/kernels/svzip/14798337","submission":null,"menuLinks":[{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/notebook","text":"Notebook","showOnMobile":true,"title":"Notebook","tab":"notebook","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/code","text":"Code","showOnMobile":true,"title":"Code","tab":"code","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/data","text":"Input","showOnMobile":true,"title":"Input","tab":"data","count":1,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/output","text":"Output","showOnMobile":false,"title":"Output","tab":"output","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/execution","text":"Execution Info","showOnMobile":false,"title":"Execution Info","tab":"execution","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/log","text":"Log","showOnMobile":false,"title":"Log","tab":"log","count":null,"showZeroCountExplicitly":false,"reportEventCategory":null,"reportEventType":null},{"href":"/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app/comments","text":"Comments","showOnMobile":true,"title":"Comments","tab":"comments","count":3,"showZeroCountExplicitly":true,"reportEventCategory":null,"reportEventType":null}],"rightMenuLinks":[],"callToAction":{"href":"/kernels/fork-version/14798337","text":"Fork Notebook","showOnMobile":true,"title":"Fork Notebook","tab":null,"count":null,"showZeroCountExplicitly":false,"reportEventCategory":"kernels","reportEventType":"anonymousKernelForkCreation"},"voteButton":{"totalVotes":8,"medalVotes":6,"hasAlreadyVotedUp":false,"hasAlreadyVotedDown":false,"canUpvote":true,"canDownvote":false,"voteUpUrl":"/kernels/vote?id=4009178","voteDownUrl":null,"voters":[{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/734966-kg.jpg","displayName":"Sayantan Das","profileUrl":"/sayantandas30011998","tier":"Contributor","tierInt":1,"userId":734966,"userName":"sayantandas30011998"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/793761-kg.jpg","displayName":"Larxel","profileUrl":"/andrewmvd","tier":"Grandmaster","tierInt":4,"userId":793761,"userName":"andrewmvd"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1056223-kg.jpg","displayName":"Ekrem Bayar","profileUrl":"/ekrembayar","tier":"Master","tierInt":3,"userId":1056223,"userName":"ekrembayar"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/1552901-kg.jpg","displayName":"xrvf","profileUrl":"/aquibjkhan","tier":"Contributor","tierInt":1,"userId":1552901,"userName":"aquibjkhan"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2106122-fb.jpg","displayName":"sjb17","profileUrl":"/jainsarika04","tier":"Novice","tierInt":0,"userId":2106122,"userName":"jainsarika04"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/2554700-kg.jpg","displayName":"Vipul Gandhi","profileUrl":"/vipulgandhi","tier":"Expert","tierInt":2,"userId":2554700,"userName":"vipulgandhi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/default-thumb.png","displayName":"arif","profileUrl":"/arifzizi","tier":"Novice","tierInt":0,"userId":3586185,"userName":"arifzizi"},{"avatarThumbnailUrl":"https://storage.googleapis.com/kaggle-avatars/thumbnails/5138170-kg.JPG","displayName":"Ramon M. Ferreira","profileUrl":"/ramonmf","tier":"Contributor","tierInt":1,"userId":5138170,"userName":"ramonmf"}],"currentUserInfo":null,"showVoters":true,"alwaysShowVoters":true},"parentDataSource":null,"parentName":"Malaria Cell Images Dataset","parentUrl":"/iarunava/cell-images-for-detecting-malaria","thumbnailImageUrl":"https://storage.googleapis.com/kaggle-datasets-images/87153/200743/31c387765e937986306b32afe5b7148c/dataset-thumbnail.jpg?t=2018-12-05-05-57-20","canWrite":false,"canAdminister":false,"currentUserForkParentSessionId":null,"currentUserHasForked":false,"forkParentIsRedacted":false,"forkDiffLinesChanged":0,"forkDiffLinesDeleted":0,"forkDiffLinesInserted":0,"forkDiffUrl":null,"forkParentAuthorDisplayName":null,"forkParentAuthorUrl":null,"forkParentTitle":null,"forkParentUrl":null,"canSeeInnerTableOfContents":true,"simplifiedViewer":false,"kernelOutputDataset":null,"disableComments":false,"taskSubmissionInfo":null,"learnSeriesNavigationData":null});performance && performance.mark && performance.mark("KernelViewer.componentCouldBootstrap");</script>

<form action="https://www.kaggle.com/vbookshelf/malaria-cell-analyzer-tensorflow-js-web-app" id="__AjaxAntiForgeryForm" method="post"><input name="X-XSRF-TOKEN" type="hidden" value="CfDJ8LdUzqlsSWBPr4Ce3rb9VL-B4q8D_q6fu9C_LYNMSW1bYgcEPrerIZa4lO4t6ZewXZ6_6Y3JCWdOo58BU5o7ekgX8aozyw_ETIpMvQtGqS-AsO-HnMiRqC8t4Y8_p18JokTI9qczXXCFnLPvb9aCpfg"></form>
<script nonce="" type="text/x-mathjax-config;executed=true">
    MathJax.Hub.Config({
    "HTML-CSS": {
    preferredFont: "TeX",
    availableFonts: ["STIX", "TeX"],
    linebreaks: {
    automatic: true
    },
    EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
    inlineMath: [["\\(", "\\)"], ["\\\\(", "\\\\)"]],
    displayMath: [["$$", "$$"], ["\\[", "\\]"]],
    processEscapes: true,
    ignoreClass: "tex2jax_ignore|dno"
    },
    TeX: {
    noUndefined: {
    attributes: {
    mathcolor: "red",
    mathbackground: "#FFEEEE",
    mathsize: "90%"
    }
    }
    },
    Macros: {
    href: "{}"
    },
    skipStartupTypeset: true,
    messageStyle: "none"
    });
</script>
<script nonce="" type="text/javascript" async="" crossorigin="anonymous" src="./Malaria Cell Analyzer + Tensorflow.js Web App _ Kaggle_files/MathJax.js"></script>


</div>




    </main>


<div class="ReactModalPortal"></div></body></html>